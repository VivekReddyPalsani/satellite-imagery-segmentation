{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-05-01T05:30:13.783Z",
     "iopub.execute_input": "2025-04-30T05:58:44.835793Z",
     "iopub.status.busy": "2025-04-30T05:58:44.835480Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Separate Models for Temporal Image Segmentation and Captioning (Improved)\n",
    "# This script implements improved models and training procedures\n",
    "# for the LEVIR-MCI dataset, leveraging pre-trained backbones, U-Net,\n",
    "# and Transformer architectures.\n",
    "#\n",
    "# Improvements:\n",
    "# - Replaced simple CNN encoder with a pre-trained ResNet backbone.\n",
    "# - Implemented a U-Net decoder for segmentation.\n",
    "# - Implemented a Transformer decoder for captioning.\n",
    "# - Added Learning Rate Scheduling (ReduceLROnPlateau).\n",
    "# - Implemented Beam Search decoding for captioning validation.\n",
    "# - Adjusted for Kaggle file paths and uses torch.nn.DataParallel if available.\n",
    "# - Automatically runs preprocessing if output files are not found.\n",
    "# - Includes weighted loss for segmentation.\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from imageio.v2 import imread # Make sure imageio is installed\n",
    "from random import randint\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "import time # Import time for timing epochs\n",
    "import torchvision.models as models # For pre-trained models\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau # For LR scheduling\n",
    "import heapq # For Beam Search\n",
    "\n",
    "# =============================================================================\n",
    "# --- Hardcoded Paths for Kaggle ---\\\n",
    "# IMPORTANT: Update DATASET_ROOT if necessary.\n",
    "# =============================================================================\n",
    "DATASET_ROOT = '/kaggle/input/levir-mci-dataset/LEVIR-MCI-dataset' # <-- *** VERIFY THIS PATH ***\n",
    "SAVE_OUTPUT_DIR = '/kaggle/working/' # Standard Kaggle writable output directory\n",
    "\n",
    "# =============================================================================\n",
    "# --- Utility Functions ---\\\n",
    "# =============================================================================\n",
    "\n",
    "# Define the mapping from RGB colors to class IDs\n",
    "# Background: 0 (black), Road: 1 (grey), Building: 2 (white)\n",
    "COLOR_TO_ID_MAPPING = {\n",
    "    (0, 0, 0): 0,          # Background (Black)\n",
    "    (128, 128, 128): 1,    # Road (Grey)\n",
    "    (255, 255, 255): 2,    # Building (White)\n",
    "}\n",
    "NUM_CLASSES = 3 # Background, Road, Building\n",
    "\n",
    "def rgb_to_class_id_mask(rgb_mask_np):\n",
    "    \"\"\"Converts an RGB mask (H, W, 3) to a class ID mask (H, W).\"\"\"\n",
    "    h, w, c = rgb_mask_np.shape\n",
    "    if c != 3:\n",
    "        if c == 1 or rgb_mask_np.ndim == 2:\n",
    "             class_id_mask = np.full((h, w), 0, dtype=np.int64)\n",
    "             if rgb_mask_np.ndim == 2:\n",
    "                grey_mask = rgb_mask_np\n",
    "             else:\n",
    "                grey_mask = rgb_mask_np.squeeze(-1)\n",
    "             class_id_mask[grey_mask == 0] = 0\n",
    "             class_id_mask[grey_mask == 128] = 1\n",
    "             class_id_mask[grey_mask == 255] = 2\n",
    "             return class_id_mask\n",
    "        else:\n",
    "            raise ValueError(f\"Input mask must have 3 channels (RGB), but got {c} with shape {rgb_mask_np.shape}\")\n",
    "\n",
    "    class_id_mask = np.full((h, w), 0, dtype=np.int64)\n",
    "    for color, class_id in COLOR_TO_ID_MAPPING.items():\n",
    "        matches = np.all(rgb_mask_np == np.array(color, dtype=rgb_mask_np.dtype), axis=-1)\n",
    "        class_id_mask[matches] = class_id\n",
    "    return class_id_mask\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count if self.count > 0 else 0\n",
    "\n",
    "def calculate_iou(predicted_mask, true_mask, model_or_num_classes):\n",
    "    \"\"\"\n",
    "    Calculates Intersection over Union (IoU) for a batch of masks.\n",
    "    Handles both direct num_classes or a model object (DataParallel or not).\n",
    "    \"\"\"\n",
    "    if isinstance(model_or_num_classes, nn.Module):\n",
    "        model_ref = model_or_num_classes.module if isinstance(model_or_num_classes, nn.DataParallel) else model_or_num_classes\n",
    "        # Try accessing num_classes directly or via a specific attribute if needed\n",
    "        if hasattr(model_ref, 'num_classes'):\n",
    "             num_classes = model_ref.num_classes\n",
    "        elif hasattr(model_ref, 'decoder') and hasattr(model_ref.decoder, 'num_classes'): # Check if it's nested in decoder\n",
    "             num_classes = model_ref.decoder.num_classes\n",
    "        else:\n",
    "             # Fallback or raise error if num_classes cannot be determined\n",
    "             print(\"Warning: Could not automatically determine num_classes from model. Falling back to NUM_CLASSES global.\")\n",
    "             num_classes = NUM_CLASSES # Use global as fallback\n",
    "    else:\n",
    "        num_classes = model_or_num_classes # Assume it's the integer num_classes\n",
    "\n",
    "    iou_per_class = torch.zeros(num_classes, device=predicted_mask.device)\n",
    "    for class_id in range(num_classes):\n",
    "        predicted_class_mask = (predicted_mask == class_id)\n",
    "        true_class_mask = (true_mask == class_id)\n",
    "\n",
    "        intersection = (predicted_class_mask & true_class_mask).sum().float()\n",
    "        union = (predicted_class_mask | true_class_mask).sum().float()\n",
    "\n",
    "        if union == 0:\n",
    "            iou_per_class[class_id] = float('nan') # Avoid division by zero, handle later with nanmean\n",
    "        else:\n",
    "            iou_per_class[class_id] = intersection / union\n",
    "\n",
    "    return iou_per_class\n",
    "\n",
    "# =============================================================================\n",
    "# --- Dataset Class ---\\\n",
    "# (Mostly reused, ensure paths and normalization are correct)\n",
    "# =============================================================================\n",
    "\n",
    "class LEVIRCCDataset(Dataset):\n",
    "    def __init__(self, data_folder, processed_data_dir, split,\n",
    "                 load_segmentation=True,\n",
    "                 max_length=41,\n",
    "                 vocab_file='vocab.json',\n",
    "                 allow_unk=True,\n",
    "                 max_iters=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_folder (str): Path to the root LEVIR-MCI dataset folder.\n",
    "            processed_data_dir (str): Path to the folder with preprocessed data.\n",
    "            split (str): 'train', 'val', or 'test'.\n",
    "            load_segmentation (bool): Load segmentation maps.\n",
    "            max_length (int): Max caption sequence length.\n",
    "            vocab_file (str): Vocabulary JSON file name.\n",
    "            allow_unk (bool): Allow unknown tokens.\n",
    "            max_iters (int, optional): Repeat dataset for this many items per epoch.\n",
    "        \"\"\"\n",
    "        self.data_folder = data_folder\n",
    "        self.processed_data_dir = processed_data_dir\n",
    "        self.split = split\n",
    "        self.load_segmentation = load_segmentation\n",
    "        self.max_length = max_length\n",
    "        self.allow_unk = allow_unk\n",
    "\n",
    "        # Image normalization parameters (ImageNet defaults)\n",
    "        self.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "        self.std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "        assert self.split in {'train', 'val', 'test'}\n",
    "\n",
    "        # ---- Load Vocabulary ----\n",
    "        vocab_path = os.path.join(self.processed_data_dir, vocab_file)\n",
    "        try:\n",
    "            with open(vocab_path, 'r') as f:\n",
    "                self.word_vocab = json.load(f)\n",
    "            self.idx_to_word = {v: k for k, v in self.word_vocab.items()}\n",
    "            self.vocab_size = len(self.word_vocab)\n",
    "            self.pad_idx = self.word_vocab.get('<NULL>', 0)\n",
    "            self.start_idx = self.word_vocab.get('<START>', 2)\n",
    "            self.end_idx = self.word_vocab.get('<END>', 3)\n",
    "            self.unk_idx = self.word_vocab.get('<UNK>', 1)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Vocabulary file not found at {vocab_path}. Run preprocessing.\")\n",
    "\n",
    "        # --- Load Image Filenames/Base Names ---\n",
    "        split_file_path = os.path.join(self.processed_data_dir, f'{split}.txt')\n",
    "        try:\n",
    "            with open(split_file_path, 'r') as f:\n",
    "                self.img_ids = [line.strip() for line in f if line.strip()]\n",
    "        except FileNotFoundError:\n",
    "             raise FileNotFoundError(f\"Split file not found at {split_file_path}. Run preprocessing.\")\n",
    "\n",
    "        if not self.img_ids:\n",
    "            raise ValueError(f\"No image IDs found in split file: {split_file_path}\")\n",
    "\n",
    "        # ---- Prepare file paths ----\n",
    "        self.files = []\n",
    "        image_base_path = os.path.join(self.data_folder, 'images', self.split)\n",
    "        token_base_path = os.path.join(self.processed_data_dir, 'tokens')\n",
    "        label_folder_name = 'label' # Assuming 'label' contains the segmentation masks\n",
    "\n",
    "        missing_files_count = 0\n",
    "        for img_base_name in self.img_ids:\n",
    "            img_file_name = f\"{img_base_name}.png\"\n",
    "            token_file_name = f\"{img_base_name}.json\"\n",
    "\n",
    "            file_paths = {\n",
    "                \"name\": img_base_name,\n",
    "                \"imgA\": os.path.join(image_base_path, 'A', img_file_name),\n",
    "                \"imgB\": os.path.join(image_base_path, 'B', img_file_name),\n",
    "                \"token\": os.path.join(token_base_path, token_file_name)\n",
    "            }\n",
    "            seg_path = None\n",
    "            if self.load_segmentation:\n",
    "                seg_path = os.path.join(image_base_path, label_folder_name, img_file_name)\n",
    "                file_paths[\"seg_label\"] = seg_path\n",
    "\n",
    "            paths_to_check = [file_paths[\"imgA\"], file_paths[\"imgB\"], file_paths[\"token\"]]\n",
    "            if self.load_segmentation:\n",
    "                 paths_to_check.append(seg_path)\n",
    "\n",
    "            files_exist = all(os.path.exists(p) for p in paths_to_check if p is not None)\n",
    "\n",
    "            if not files_exist:\n",
    "                # Optionally print which file is missing\n",
    "                # for p in paths_to_check:\n",
    "                #     if p is not None and not os.path.exists(p):\n",
    "                #         print(f\"Missing file: {p}\")\n",
    "                missing_files_count += 1\n",
    "                continue\n",
    "\n",
    "            self.files.append(file_paths)\n",
    "\n",
    "        if missing_files_count > 0:\n",
    "            print(f\"Warning: Skipped {missing_files_count} entries due to missing files in split '{self.split}'.\")\n",
    "\n",
    "        if not self.files:\n",
    "             raise ValueError(f\"No valid file sets found for split '{self.split}'. Check paths and preprocessing.\")\n",
    "\n",
    "        # --- Handle max_iters ---\n",
    "        self.max_iters = max_iters\n",
    "        if max_iters is not None and max_iters > 0:\n",
    "            if not self.files:\n",
    "                 raise ValueError(f\"Cannot use max_iters > 0 when no valid files were loaded for split '{self.split}'.\")\n",
    "            n_repeat = int(np.ceil(max_iters / len(self.files)))\n",
    "            self.files = self.files * n_repeat\n",
    "\n",
    "        print(f\"Initialized LEVIRCCDataset for split '{self.split}' with {len(self.files)} items (after potential repeat).\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.max_iters is not None and self.max_iters > 0:\n",
    "            return self.max_iters\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        actual_index = index % len(self.files) if self.files else 0\n",
    "        if not self.files:\n",
    "             raise IndexError(\"Dataset is empty.\")\n",
    "        datafiles = self.files[actual_index]\n",
    "\n",
    "        # --- Load Images ---\n",
    "        try:\n",
    "            imgA_np = np.array(imread(datafiles[\"imgA\"]), dtype=np.uint8)\n",
    "            imgB_np = np.array(imread(datafiles[\"imgB\"]), dtype=np.uint8)\n",
    "            if imgA_np.ndim != 3 or imgA_np.shape[-1] != 3 or imgB_np.ndim != 3 or imgB_np.shape[-1] != 3:\n",
    "                 raise ValueError(f\"Image dimensions incorrect for {datafiles['name']}. Expected (H, W, 3), got {imgA_np.shape} and {imgB_np.shape}\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error loading images for {datafiles['name']}: {e}\")\n",
    "             raise\n",
    "\n",
    "        # Convert to float32, normalize, and HWC -> CHW\n",
    "        imgA = (imgA_np.astype(np.float32) / 255.0 - self.mean) / self.std\n",
    "        imgB = (imgB_np.astype(np.float32) / 255.0 - self.mean) / self.std\n",
    "        imgA = torch.from_numpy(imgA.transpose(2, 0, 1))\n",
    "        imgB = torch.from_numpy(imgB.transpose(2, 0, 1))\n",
    "\n",
    "\n",
    "        # --- Load and Process Segmentation ---\n",
    "        seg_mask_class_ids = torch.zeros(imgA.shape[1:], dtype=torch.long) # Default empty mask\n",
    "        if self.load_segmentation:\n",
    "            try:\n",
    "                seg_label_np = np.array(imread(datafiles[\"seg_label\"]), dtype=np.uint8)\n",
    "                seg_mask_class_ids_np = rgb_to_class_id_mask(seg_label_np)\n",
    "                seg_mask_class_ids = torch.from_numpy(seg_mask_class_ids_np).long()\n",
    "            except FileNotFoundError:\n",
    "                 print(f\"Error loading segmentation label for {datafiles['name']}. Path: {datafiles['seg_label']}\")\n",
    "                 # Keep the default empty mask or raise error depending on strictness\n",
    "                 # raise\n",
    "            except Exception as e:\n",
    "                 print(f\"Error processing segmentation label for {datafiles['name']}: {e}\")\n",
    "                 # Keep the default empty mask or raise error\n",
    "                 # raise\n",
    "\n",
    "        # --- Load Captions ---\n",
    "        token_sequence = None\n",
    "        all_caption_tokens = [] # To store all tokenized captions for evaluation\n",
    "\n",
    "        try:\n",
    "            with open(datafiles[\"token\"], 'r') as f:\n",
    "                caption_list = json.load(f) # List of lists of token indices\n",
    "\n",
    "            if not caption_list:\n",
    "                print(f\"Warning: Empty caption list found for {datafiles['name']}. Using default padding.\")\n",
    "                token_sequence = torch.full((self.max_length,), self.pad_idx, dtype=torch.long)\n",
    "                all_caption_tokens = [[self.start_idx] + [self.end_idx] * (self.max_length - 1)] # Dummy list\n",
    "            else:\n",
    "                # Ensure all loaded captions have the expected max_length\n",
    "                processed_caption_list = []\n",
    "                for cap in caption_list:\n",
    "                    if len(cap) == self.max_length:\n",
    "                        processed_caption_list.append(cap)\n",
    "                    else:\n",
    "                        # Pad or truncate if necessary (should ideally be handled in preprocessing)\n",
    "                        print(f\"Warning: Caption length mismatch for {datafiles['name']}. Expected {self.max_length}, got {len(cap)}. Adjusting.\")\n",
    "                        cap = cap[:self.max_length] # Truncate\n",
    "                        while len(cap) < self.max_length:\n",
    "                            cap.append(self.pad_idx) # Pad\n",
    "                        processed_caption_list.append(cap)\n",
    "                caption_list = processed_caption_list\n",
    "\n",
    "                if not caption_list: # If all captions were invalid length\n",
    "                     print(f\"Error: No valid length captions found for {datafiles['name']} after length check.\")\n",
    "                     token_sequence = torch.full((self.max_length,), self.pad_idx, dtype=torch.long)\n",
    "                     all_caption_tokens = [[self.start_idx] + [self.end_idx] * (self.max_length - 1)]\n",
    "                else:\n",
    "                    if self.split == 'train':\n",
    "                        # Select one random caption for training\n",
    "                        selected_caption = caption_list[randint(0, len(caption_list) - 1)]\n",
    "                        token_sequence = torch.tensor(selected_caption, dtype=torch.long)\n",
    "                        all_caption_tokens = [selected_caption] # Store the selected one\n",
    "                    else: # For validation/test, load all captions\n",
    "                        all_caption_tokens = caption_list\n",
    "                        token_sequence = None # Not needed directly for eval\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error loading token file for {datafiles['name']}. Path: {datafiles['token']}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing token file {datafiles['token']}: {e}\")\n",
    "            raise\n",
    "\n",
    "        # --- Prepare output dictionary ---\n",
    "        item = {\n",
    "            'imgA': imgA,\n",
    "            'imgB': imgB,\n",
    "            'name': datafiles['name'],\n",
    "            'all_caption_tokens': all_caption_tokens # Always include for eval\n",
    "        }\n",
    "\n",
    "        if self.load_segmentation:\n",
    "            item['seg_label'] = seg_mask_class_ids\n",
    "\n",
    "        if self.split == 'train' and token_sequence is not None:\n",
    "             item['caption_tokens'] = token_sequence # Include single sequence for training\n",
    "\n",
    "        return item\n",
    "\n",
    "# =============================================================================\n",
    "# --- Preprocessing Function ---\\\n",
    "# (Reused - ensure it runs correctly before training)\n",
    "# =============================================================================\n",
    "def run_preprocessing_direct(data_folder, processed_data_dir, caption_file, max_length, word_count_threshold):\n",
    "    \"\"\"\n",
    "    Runs the preprocessing steps: tokenization, vocabulary creation, and splitting.\n",
    "    \"\"\"\n",
    "    print(\"Starting preprocessing...\")\n",
    "    output_dir = processed_data_dir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'tokens'), exist_ok=True)\n",
    "\n",
    "    # --- Load and process captions ---\n",
    "    caption_file_path = os.path.join(data_folder, caption_file)\n",
    "    try:\n",
    "        with open(caption_file_path, 'r') as f:\n",
    "            caption_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Caption file not found at {caption_file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(f\"Could not decode JSON from caption file at {caption_file_path}\")\n",
    "\n",
    "    word_counts = defaultdict(int)\n",
    "    image_captions = defaultdict(list) # Stores {img_base_name: [ [token1, token2,...], [token1, token2,...] ]}\n",
    "\n",
    "    print(\"Tokenizing captions and counting words...\")\n",
    "    max_actual_len = 0\n",
    "    for img_info in tqdm(caption_data['images']):\n",
    "        img_base_name = os.path.splitext(img_info['filename'])[0]\n",
    "        for sentence in img_info['sentences']:\n",
    "            tokens = sentence['tokens']\n",
    "            max_actual_len = max(max_actual_len, len(tokens))\n",
    "            # Add <START> and <END> tokens\n",
    "            processed_tokens = ['<START>'] + tokens + ['<END>']\n",
    "            # Pad or truncate tokens to max_length\n",
    "            if len(processed_tokens) > max_length:\n",
    "                processed_tokens = processed_tokens[:max_length-1] + ['<END>'] # Ensure END is last if truncated\n",
    "            while len(processed_tokens) < max_length:\n",
    "                processed_tokens.append('<NULL>') # Use <NULL> for padding\n",
    "\n",
    "            image_captions[img_base_name].append(processed_tokens)\n",
    "\n",
    "            # Count words (only original tokens)\n",
    "            for token in tokens:\n",
    "                word_counts[token] += 1\n",
    "\n",
    "    print(f\"Max actual caption length (before special tokens/padding): {max_actual_len}\")\n",
    "    print(f\"Target max_length (with special tokens/padding): {max_length}\")\n",
    "    if max_actual_len + 2 > max_length:\n",
    "        print(f\"Warning: MAX_LENGTH ({max_length}) might be too short for some captions (max actual length + 2 = {max_actual_len + 2}). Consider increasing MAX_LENGTH.\")\n",
    "\n",
    "    # --- Build Vocabulary ---\n",
    "    print(\"Building vocabulary...\")\n",
    "    words = [word for word, count in word_counts.items() if count >= word_count_threshold]\n",
    "    vocab = {'<NULL>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}\n",
    "    for i, word in enumerate(words):\n",
    "        vocab[word] = i + 4\n",
    "    vocab_size = len(vocab)\n",
    "    print(f\"Vocabulary size: {vocab_size} (using threshold {word_count_threshold})\")\n",
    "\n",
    "    vocab_file_path = os.path.join(output_dir, 'vocab.json')\n",
    "    with open(vocab_file_path, 'w') as f:\n",
    "        json.dump(vocab, f)\n",
    "    print(f\"Vocabulary saved to {vocab_file_path}\")\n",
    "\n",
    "    # --- Convert tokens to indices and save ---\n",
    "    print(\"Converting tokens to indices and saving...\")\n",
    "    token_output_dir = os.path.join(output_dir, 'tokens')\n",
    "    for img_base_name, captions in tqdm(image_captions.items()):\n",
    "        indexed_captions = []\n",
    "        for caption_tokens in captions:\n",
    "            # Use vocab.get(token, vocab['<UNK>']) to handle unknown words\n",
    "            indexed_caption = [vocab.get(token, vocab['<UNK>']) for token in caption_tokens]\n",
    "            # Verify length after indexing\n",
    "            if len(indexed_caption) != max_length:\n",
    "                 print(f\"Error: Indexed caption length mismatch for {img_base_name}. Expected {max_length}, got {len(indexed_caption)}. Skipping this caption.\")\n",
    "                 # Optionally pad/truncate again here, but it indicates a logic error earlier\n",
    "                 continue\n",
    "            indexed_captions.append(indexed_caption)\n",
    "\n",
    "        if indexed_captions: # Only save if there are valid captions\n",
    "            token_file_path = os.path.join(token_output_dir, f'{img_base_name}.json')\n",
    "            with open(token_file_path, 'w') as f:\n",
    "                json.dump(indexed_captions, f)\n",
    "        else:\n",
    "            print(f\"Warning: No valid captions saved for {img_base_name}.\")\n",
    "\n",
    "\n",
    "    print(\"Tokenization and saving complete.\")\n",
    "\n",
    "    # --- Create Train/Val/Test Splits ---\n",
    "    print(\"Creating train/val/test split files...\")\n",
    "    splits = ['train', 'val', 'test']\n",
    "    for split in splits:\n",
    "        split_image_folder_A = os.path.join(data_folder, 'images', split, 'A')\n",
    "        split_image_folder_B = os.path.join(data_folder, 'images', split, 'B')\n",
    "        split_label_folder = os.path.join(data_folder, 'images', split, 'label') # Assuming 'label' folder exists\n",
    "        token_folder = os.path.join(output_dir, 'tokens')\n",
    "\n",
    "        if not os.path.exists(split_image_folder_A):\n",
    "            print(f\"Warning: Image folder 'A' for split '{split}' not found at {split_image_folder_A}. Skipping split file creation.\")\n",
    "            continue\n",
    "\n",
    "        # List image base names from folder A\n",
    "        img_base_names_in_folder = sorted([os.path.splitext(f)[0] for f in os.listdir(split_image_folder_A) if f.endswith('.png')])\n",
    "\n",
    "        # Filter names to only include those with corresponding B image, label (if exists), and token file\n",
    "        valid_img_base_names = []\n",
    "        for name in img_base_names_in_folder:\n",
    "             img_b_exists = os.path.exists(os.path.join(split_image_folder_B, f\"{name}.png\"))\n",
    "             label_exists = os.path.exists(os.path.join(split_label_folder, f\"{name}.png\"))\n",
    "             token_exists = os.path.exists(os.path.join(token_folder, f\"{name}.json\"))\n",
    "\n",
    "             # Require A, B, token. Require label only if the folder exists.\n",
    "             if img_b_exists and token_exists and (label_exists or not os.path.exists(split_label_folder)):\n",
    "                 valid_img_base_names.append(name)\n",
    "             # else:\n",
    "                 # print(f\"Skipping {name} in split {split} due to missing files (B:{img_b_exists}, Label:{label_exists}, Token:{token_exists})\")\n",
    "\n",
    "\n",
    "        split_file_path = os.path.join(output_dir, f'{split}.txt')\n",
    "        with open(split_file_path, 'w') as f:\n",
    "            for name in valid_img_base_names:\n",
    "                f.write(f\"{name}\\n\")\n",
    "\n",
    "        print(f\"Split file for '{split}' created with {len(valid_img_base_names)} images at {split_file_path}\")\n",
    "\n",
    "    print(\"Preprocessing finished.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- Class Weight Calculation Function ---\\\n",
    "# (Reused - calculates weights for segmentation loss)\n",
    "# =============================================================================\n",
    "def calculate_class_weights(data_folder, processed_data_dir, split='train', num_classes=NUM_CLASSES):\n",
    "    \"\"\"Calculates class weights using inverse frequency.\"\"\"\n",
    "    print(f\"Calculating class weights for '{split}' split...\")\n",
    "    split_file_path = os.path.join(processed_data_dir, f'{split}.txt')\n",
    "    if not os.path.exists(split_file_path):\n",
    "         raise FileNotFoundError(f\"Split file not found at {split_file_path}. Cannot calculate weights.\")\n",
    "\n",
    "    with open(split_file_path, 'r') as f:\n",
    "        img_ids = [line.strip() for line in f if line.strip()]\n",
    "    if not img_ids:\n",
    "        raise ValueError(f\"No image IDs found in split file: {split_file_path}.\")\n",
    "\n",
    "    label_folder_name = 'label'\n",
    "    image_base_path = os.path.join(data_folder, 'images', split)\n",
    "    class_pixel_counts = np.zeros(num_classes, dtype=np.int64)\n",
    "    total_pixels = 0\n",
    "\n",
    "    for img_base_name in tqdm(img_ids, desc=\"Counting pixels for class weights\"):\n",
    "        seg_path = os.path.join(image_base_path, label_folder_name, f\"{img_base_name}.png\")\n",
    "        if not os.path.exists(seg_path):\n",
    "            # print(f\"Warning: Seg mask not found for {img_base_name} at {seg_path}. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            seg_label_np = np.array(imread(seg_path), dtype=np.uint8)\n",
    "            seg_mask_class_ids_np = rgb_to_class_id_mask(seg_label_np)\n",
    "            unique_classes, counts = np.unique(seg_mask_class_ids_np, return_counts=True)\n",
    "            for class_id, count in zip(unique_classes, counts):\n",
    "                if 0 <= class_id < num_classes:\n",
    "                    class_pixel_counts[class_id] += count\n",
    "            total_pixels += seg_mask_class_ids_np.size\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing mask {img_base_name} for pixel count: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Class pixel counts: {class_pixel_counts}\")\n",
    "    print(f\"Total pixels counted: {total_pixels}\")\n",
    "\n",
    "    if total_pixels == 0:\n",
    "        print(\"Warning: No pixels counted. Returning equal weights.\")\n",
    "        return torch.ones(num_classes, dtype=torch.float)\n",
    "\n",
    "    # Inverse frequency weighting: weight_c = total_pixels / (num_classes * pixels_in_class_c)\n",
    "    # Add epsilon to avoid division by zero for classes not present\n",
    "    class_weights = total_pixels / (num_classes * (class_pixel_counts + 1e-6))\n",
    "\n",
    "    # Normalize weights (optional, can help stability)\n",
    "    # class_weights = class_weights / np.sum(class_weights)\n",
    "\n",
    "    class_weights_tensor = torch.from_numpy(class_weights).float()\n",
    "    print(f\"Calculated class weights: {class_weights_tensor.tolist()}\")\n",
    "    return class_weights_tensor\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- Improved Model Architectures ---\\\n",
    "# =============================================================================\n",
    "\n",
    "# --- ResNet Encoder ---\n",
    "# --- ResNet Encoder (Corrected Weight Init) ---\n",
    "class ResNetEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder using a pre-trained ResNet model.\n",
    "    Adapts the first layer to accept 6 channels (concatenated imgA and imgB).\n",
    "    CORRECTED: Weight initialization for the first layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, arch='resnet50', pretrained=True, freeze_layers=25):\n",
    "        super().__init__()\n",
    "        print(f\"Initializing ResNetEncoder with arch={arch}, pretrained={pretrained}\")\n",
    "        if arch == 'resnet18':\n",
    "            resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
    "            self.out_features_base = 512\n",
    "        elif arch == 'resnet34':\n",
    "            resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT if pretrained else None)\n",
    "            self.out_features_base = 512\n",
    "        elif arch == 'resnet50':\n",
    "            resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT if pretrained else None)\n",
    "            self.out_features_base = 2048\n",
    "        elif arch == 'resnet101':\n",
    "            resnet = models.resnet101(weights=models.ResNet101_Weights.DEFAULT if pretrained else None)\n",
    "            self.out_features_base = 2048\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported ResNet architecture: {arch}\")\n",
    "\n",
    "        # Adapt the first convolutional layer for 6 input channels\n",
    "        original_conv1 = resnet.conv1\n",
    "        self.conv1 = nn.Conv2d(6, original_conv1.out_channels, # Requesting 6 in_channels\n",
    "                               kernel_size=original_conv1.kernel_size,\n",
    "                               stride=original_conv1.stride,\n",
    "                               padding=original_conv1.padding,\n",
    "                               bias=original_conv1.bias)\n",
    "\n",
    "        # Initialize weights for the new conv1\n",
    "        if pretrained:\n",
    "            print(\"Adapting pre-trained weights for the first layer (6 channels)...\")\n",
    "            original_weights = original_conv1.weight.data # Shape: [out_channels, 3, k, k]\n",
    "\n",
    "            # --- CORRECTED WEIGHT INITIALIZATION ---\n",
    "            # Concatenate the original 3-channel weights twice along the input channel dim (dim=1)\n",
    "            # This results in a weight tensor expecting 6 input channels: [out_channels, 6, k, k]\n",
    "            # We divide by 2.0 to keep the initial activation scale similar to the original pre-trained model.\n",
    "            self.conv1.weight.data = torch.cat((original_weights, original_weights), dim=1) / 2.0\n",
    "            # --- END CORRECTION ---\n",
    "\n",
    "            # Copy bias if it exists\n",
    "            if original_conv1.bias is not None:\n",
    "                self.conv1.bias.data = original_conv1.bias.data\n",
    "\n",
    "\n",
    "        # Keep other layers from ResNet\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "\n",
    "        # --- Feature dimensions at different stages ---\n",
    "        # Output channels after each layer block\n",
    "        # Determine output channels based on the architecture (handling bottleneck vs basic block)\n",
    "        if hasattr(resnet.layer1[-1], 'conv3'): # Bottleneck block (ResNet50+)\n",
    "            layer1_out_ch = resnet.layer1[-1].conv3.out_channels\n",
    "            layer2_out_ch = resnet.layer2[-1].conv3.out_channels\n",
    "            layer3_out_ch = resnet.layer3[-1].conv3.out_channels\n",
    "        else: # Basic block (ResNet18/34)\n",
    "             layer1_out_ch = resnet.layer1[-1].conv2.out_channels\n",
    "             layer2_out_ch = resnet.layer2[-1].conv2.out_channels\n",
    "             layer3_out_ch = resnet.layer3[-1].conv2.out_channels\n",
    "\n",
    "\n",
    "        self.out_channels = [\n",
    "            resnet.conv1.out_channels, # After conv1/bn1/relu/maxpool (e.g., 64)\n",
    "            layer1_out_ch,             # After layer1\n",
    "            layer2_out_ch,             # After layer2\n",
    "            layer3_out_ch,             # After layer3\n",
    "            self.out_features_base     # After layer4\n",
    "        ]\n",
    "        print(f\"Encoder output channels per stage: {self.out_channels}\")\n",
    "\n",
    "\n",
    "        # Freeze early layers if requested\n",
    "        if freeze_layers > 0:\n",
    "            print(f\"Freezing first {freeze_layers} layers of the encoder.\")\n",
    "            layers_to_freeze = [self.conv1, self.bn1, self.relu, self.maxpool]\n",
    "            if freeze_layers >= 1: layers_to_freeze.append(self.layer1)\n",
    "            if freeze_layers >= 2: layers_to_freeze.append(self.layer2)\n",
    "            if freeze_layers >= 3: layers_to_freeze.append(self.layer3)\n",
    "            # Layer 4 is typically kept trainable for fine-tuning\n",
    "            for layer in layers_to_freeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, imgA, imgB):\n",
    "        # Concatenate images along the channel dimension\n",
    "        x = torch.cat((imgA, imgB), dim=1) # Shape: (N, 6, H, W)\n",
    "\n",
    "        # --- Forward through ResNet layers ---\n",
    "        x = self.conv1(x) # <--- This was the point of error\n",
    "        x = self.bn1(x)\n",
    "        x0 = self.relu(x) # Output after initial conv/relu (used for skip connection)\n",
    "        x = self.maxpool(x0)\n",
    "\n",
    "        x1 = self.layer1(x)  # Output after layer1\n",
    "        x2 = self.layer2(x1) # Output after layer2\n",
    "        x3 = self.layer3(x2) # Output after layer3\n",
    "        x4 = self.layer4(x3) # Output after layer4 (final feature map)\n",
    "\n",
    "        # Return features from multiple stages for U-Net decoder\n",
    "        return [x4, x3, x2, x1, x0] # From deepest to shallowest features\n",
    "\n",
    "# --- U-Net Decoder ---\n",
    "class UNetDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net decoder part. Takes features from encoder stages and upsamples.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_channels, decoder_channels, num_classes, final_upsample_mode='bilinear'):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.final_upsample_mode = final_upsample_mode\n",
    "        if len(encoder_channels) != 5:\n",
    "             raise ValueError(\"UNetDecoder expects 5 feature maps from the encoder (x4, x3, x2, x1, x0)\")\n",
    "        if len(decoder_channels) != 4:\n",
    "             raise ValueError(\"UNetDecoder expects 4 decoder channel sizes\")\n",
    "\n",
    "        # Encoder channels (deepest first): e.g., [2048, 1024, 512, 256, 64] for ResNet50\n",
    "        enc_c4, enc_c3, enc_c2, enc_c1, enc_c0 = encoder_channels\n",
    "        # Decoder channels (bottom-up): e.g., [512, 256, 128, 64]\n",
    "        dec_c3, dec_c2, dec_c1, dec_c0 = decoder_channels\n",
    "\n",
    "        # --- Upsampling Blocks ---\n",
    "        # Block 1 (Upsample x4, combine with x3)\n",
    "        self.upconv3 = nn.ConvTranspose2d(enc_c4, dec_c3, kernel_size=2, stride=2)\n",
    "        self.dec_conv3 = self._conv_block(dec_c3 + enc_c3, dec_c3)\n",
    "\n",
    "        # Block 2 (Upsample previous, combine with x2)\n",
    "        self.upconv2 = nn.ConvTranspose2d(dec_c3, dec_c2, kernel_size=2, stride=2)\n",
    "        self.dec_conv2 = self._conv_block(dec_c2 + enc_c2, dec_c2)\n",
    "\n",
    "        # Block 3 (Upsample previous, combine with x1)\n",
    "        self.upconv1 = nn.ConvTranspose2d(dec_c2, dec_c1, kernel_size=2, stride=2)\n",
    "        self.dec_conv1 = self._conv_block(dec_c1 + enc_c1, dec_c1)\n",
    "\n",
    "        # Block 4 (Upsample previous, combine with x0)\n",
    "        self.upconv0 = nn.ConvTranspose2d(dec_c1, dec_c0, kernel_size=2, stride=2)\n",
    "        self.dec_conv0 = self._conv_block(dec_c0 + enc_c0, dec_c0)\n",
    "\n",
    "        # Final convolution layer\n",
    "        self.final_conv = nn.Conv2d(dec_c0, num_classes, kernel_size=1)\n",
    "\n",
    "    def _conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def _center_crop_and_concat(self, upsampled, bypass):\n",
    "        \"\"\"Center crop bypass connection to match spatial dimensions and concatenate.\"\"\"\n",
    "        diffY = bypass.size()[2] - upsampled.size()[2]\n",
    "        diffX = bypass.size()[3] - upsampled.size()[3]\n",
    "        bypass_cropped = bypass[:, :, diffY // 2 : diffY // 2 + upsampled.size()[2],\n",
    "                                     diffX // 2 : diffX // 2 + upsampled.size()[3]]\n",
    "        return torch.cat([upsampled, bypass_cropped], dim=1)\n",
    "\n",
    "    def forward(self, encoder_features, target_size):\n",
    "        # encoder_features is a list [x4, x3, x2, x1, x0] (deepest to shallowest)\n",
    "        x4, x3, x2, x1, x0 = encoder_features\n",
    "\n",
    "        # Decode block 3\n",
    "        d3 = self.upconv3(x4)\n",
    "        d3 = self._center_crop_and_concat(d3, x3)\n",
    "        d3 = self.dec_conv3(d3)\n",
    "\n",
    "        # Decode block 2\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = self._center_crop_and_concat(d2, x2)\n",
    "        d2 = self.dec_conv2(d2)\n",
    "\n",
    "        # Decode block 1\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = self._center_crop_and_concat(d1, x1)\n",
    "        d1 = self.dec_conv1(d1)\n",
    "\n",
    "        # Decode block 0\n",
    "        d0 = self.upconv0(d1)\n",
    "        d0 = self._center_crop_and_concat(d0, x0)\n",
    "        d0 = self.dec_conv0(d0)\n",
    "\n",
    "        # Final convolution\n",
    "        logits = self.final_conv(d0)\n",
    "\n",
    "        # Upsample to the original target size\n",
    "        # Using interpolate is often more flexible than ConvTranspose for the final layer\n",
    "        if logits.shape[-2:] != target_size:\n",
    "            logits = F.interpolate(logits, size=target_size, mode=self.final_upsample_mode, align_corners=False if self.final_upsample_mode=='bilinear' else None)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# --- Segmentation Model (ResNet + U-Net) ---\n",
    "\n",
    "# --- Segmentation Model (ResNet + U-Net - CORRECTED Initialization) ---\n",
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, num_classes, encoder_arch='resnet34', pretrained=True, freeze_encoder_layers=0,\n",
    "                 decoder_channels=(256, 128, 64, 32), final_upsample_mode='bilinear'):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNetEncoder(arch=encoder_arch, pretrained=pretrained, freeze_layers=freeze_encoder_layers)\n",
    "\n",
    "        # --- CORRECTION: Reverse encoder channels list ---\n",
    "        # The encoder outputs channels in order [x0, x1, x2, x3, x4]\n",
    "        # The decoder expects them in order [x4, x3, x2, x1, x0] for its initialization logic\n",
    "        encoder_channels_reversed = self.encoder.out_channels[::-1] # Reverse the list\n",
    "        # Example for ResNet34: [64, 64, 128, 256, 512] -> [512, 256, 128, 64, 64]\n",
    "        # --- END CORRECTION ---\n",
    "\n",
    "        self.decoder = UNetDecoder(encoder_channels=encoder_channels_reversed, # Pass the reversed list\n",
    "                                   decoder_channels=decoder_channels,\n",
    "                                   num_classes=num_classes,\n",
    "                                   final_upsample_mode=final_upsample_mode)\n",
    "        self.num_classes = num_classes # Make sure num_classes is accessible\n",
    "\n",
    "    # --- THIS METHOD MUST EXIST ---\n",
    "    def forward(self, imgA, imgB):\n",
    "        target_size = imgA.shape[-2:] # H, W of original input\n",
    "        # Get features from the encoder (List: [x4, x3, x2, x1, x0])\n",
    "        encoder_features = self.encoder(imgA, imgB)\n",
    "        # Pass features and target size to the decoder\n",
    "        segmentation_output = self.decoder(encoder_features, target_size) # (N, num_classes, H, W)\n",
    "        return segmentation_output\n",
    "    # --- END of forward method ---\n",
    "\n",
    "# --- Transformer Components ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Injects positional information into the input embeddings.\"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=50): # max_len should accommodate max_length\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) # Shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe) # Not a model parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        # x.size(1) is the sequence length\n",
    "        # Add positional encoding up to the length of the sequence\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    \"\"\"A single layer for the Transformer Decoder.\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\", layer_norm_eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu # Example activation choices\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Target sequence (batch_size, tgt_len, d_model)\n",
    "            memory: Memory sequence from encoder (batch_size, src_len, d_model)\n",
    "            tgt_mask: Mask for target sequence (tgt_len, tgt_len) - prevents attending to future tokens\n",
    "            memory_mask: Mask for memory sequence (Not typically used in standard image captioning)\n",
    "            tgt_key_padding_mask: Mask for padding in target sequence (batch_size, tgt_len)\n",
    "            memory_key_padding_mask: Mask for padding in memory sequence (batch_size, src_len)\n",
    "        \"\"\"\n",
    "        # Self-attention block\n",
    "        tgt2, self_attn_weights = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                                                  key_padding_mask=tgt_key_padding_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        # Multihead attention block (attends to encoder memory)\n",
    "        tgt2, cross_attn_weights = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "                                                        key_padding_mask=memory_key_padding_mask)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "\n",
    "        # Feedforward block\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt, self_attn_weights, cross_attn_weights # Return attention weights if needed\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"Transformer Decoder composed of multiple layers.\"\"\"\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm # Optional final layer norm\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        output = tgt\n",
    "        all_self_attn_weights = []\n",
    "        all_cross_attn_weights = []\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output, self_attn, cross_attn = mod(output, memory, tgt_mask=tgt_mask,\n",
    "                                                 memory_mask=memory_mask,\n",
    "                                                 tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                                 memory_key_padding_mask=memory_key_padding_mask)\n",
    "            all_self_attn_weights.append(self_attn)\n",
    "            all_cross_attn_weights.append(cross_attn)\n",
    "\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        # Return attention weights from the last layer, or all layers if needed\n",
    "        return output, all_self_attn_weights, all_cross_attn_weights\n",
    "\n",
    "\n",
    "# --- Captioning Model (ResNet Encoder + Transformer Decoder) ---\n",
    "class CaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_decoder_layers=6,\n",
    "                 dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
    "                 encoder_arch='resnet34', pretrained=True, freeze_encoder_layers=0,\n",
    "                 max_length=41): # max_length needed for positional encoding\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # --- Encoder ---\n",
    "        # Use a separate ResNet instance or share if memory allows and tasks are related\n",
    "        # Here, we use a separate instance for simplicity.\n",
    "        self.encoder = ResNetEncoder(arch=encoder_arch, pretrained=pretrained, freeze_layers=freeze_encoder_layers)\n",
    "        encoder_output_dim = self.encoder.out_features_base # e.g., 512 for ResNet34, 2048 for ResNet50\n",
    "\n",
    "        # --- Input Projection ---\n",
    "        # Project encoder output features to the decoder's expected dimension (d_model)\n",
    "        self.input_proj = nn.Conv2d(encoder_output_dim, d_model, kernel_size=1)\n",
    "\n",
    "        # --- Decoder Components ---\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=max_length)\n",
    "\n",
    "        # Standard Transformer Decoder Layer\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, norm=decoder_norm)\n",
    "\n",
    "        # --- Output Layer ---\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # --- Initialize Weights ---\n",
    "        self._reset_parameters()\n",
    "\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        \"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        # Initialize embedding weights specifically if needed\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=self.d_model**-0.5)\n",
    "        # Ensure padding index embedding is zero\n",
    "        if hasattr(self.embedding, 'padding_idx') and self.embedding.padding_idx is not None:\n",
    "             with torch.no_grad():\n",
    "                 self.embedding.weight[self.embedding.padding_idx].fill_(0)\n",
    "\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz, device):\n",
    "        \"\"\"Generates a square mask for the sequence. Used in self-attention.\"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz, device=device)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def _create_padding_mask(self, sequence, pad_idx):\n",
    "        \"\"\"Creates a mask for padding tokens.\"\"\"\n",
    "        # True where sequence == pad_idx, False otherwise\n",
    "        return (sequence == pad_idx)\n",
    "\n",
    "\n",
    "    def forward(self, imgA, imgB, caption_tokens, pad_idx):\n",
    "        \"\"\"\n",
    "        Forward pass for training.\n",
    "        Args:\n",
    "            imgA, imgB: Input images (N, 3, H, W)\n",
    "            caption_tokens: Target caption tokens (N, T), including <START> but shifted for input.\n",
    "                            Input to decoder: <START> w1 w2 ... wn\n",
    "                            Target for loss:      w1 w2 ... wn <END>\n",
    "            pad_idx: Index of the padding token.\n",
    "        Returns:\n",
    "            Output logits (N, T, vocab_size)\n",
    "        \"\"\"\n",
    "        # --- Encode Images ---\n",
    "        # Get the deepest features from the encoder (x4)\n",
    "        encoder_features_list = self.encoder(imgA, imgB)\n",
    "        encoder_output = encoder_features_list[0] # Shape: (N, C_enc, H', W')\n",
    "\n",
    "        # Project encoder features to d_model and flatten\n",
    "        memory = self.input_proj(encoder_output) # (N, d_model, H', W')\n",
    "        N, _, H_mem, W_mem = memory.shape\n",
    "        memory = memory.permute(0, 2, 3, 1).view(N, -1, self.d_model) # (N, H'*W', d_model)\n",
    "\n",
    "        # --- Prepare Decoder Input ---\n",
    "        # For training, use teacher forcing. Input is shifted right.\n",
    "        # Input: <START> w1 w2 ... wn\n",
    "        tgt_input = caption_tokens[:, :-1] # (N, T-1)\n",
    "        # Target for loss: w1 w2 ... wn <END>\n",
    "        # tgt_output = caption_tokens[:, 1:] # (N, T-1) - Handled by loss function\n",
    "\n",
    "        # --- Embed and Add Positional Encoding ---\n",
    "        tgt_emb = self.embedding(tgt_input) * math.sqrt(self.d_model) # Scale embedding\n",
    "        tgt_pos = self.pos_encoder(tgt_emb) # (N, T-1, d_model)\n",
    "\n",
    "        # --- Create Masks ---\n",
    "        tgt_seq_len = tgt_input.size(1)\n",
    "        device = tgt_input.device\n",
    "        # Mask to prevent attending to future tokens\n",
    "        tgt_mask = self._generate_square_subsequent_mask(tgt_seq_len, device) # (T-1, T-1)\n",
    "        # Mask to ignore padding tokens in the target sequence input\n",
    "        tgt_padding_mask = self._create_padding_mask(tgt_input, pad_idx) # (N, T-1)\n",
    "        # Padding mask for encoder memory (if encoder output could be variable length - not typical here)\n",
    "        memory_padding_mask = None # Assuming fixed size encoder output\n",
    "\n",
    "        # --- Decode ---\n",
    "        # memory shape: (N, S, d_model) where S = H'*W'\n",
    "        # tgt_pos shape: (N, T-1, d_model)\n",
    "        decoder_output, _, _ = self.decoder(tgt_pos, memory,\n",
    "                                            tgt_mask=tgt_mask,\n",
    "                                            tgt_key_padding_mask=tgt_padding_mask,\n",
    "                                            memory_key_padding_mask=memory_padding_mask)\n",
    "                                            # Shape: (N, T-1, d_model)\n",
    "\n",
    "        # --- Final Output Layer ---\n",
    "        logits = self.output_layer(decoder_output) # Shape: (N, T-1, vocab_size)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    # --- Beam Search Decoding (for validation/inference) ---\n",
    "    def beam_search_decode(self, imgA, imgB, beam_size, start_idx, end_idx, pad_idx, max_len=None):\n",
    "        \"\"\"\n",
    "        Generates captions using beam search.\n",
    "        Args:\n",
    "            imgA, imgB: Input images (1, 3, H, W) - Process one image pair at a time.\n",
    "            beam_size (int): Number of beams to keep.\n",
    "            start_idx, end_idx, pad_idx: Special token indices.\n",
    "            max_len (int, optional): Maximum generation length. Defaults to self.max_length.\n",
    "        Returns:\n",
    "            List[int]: The sequence of token indices for the best caption.\n",
    "        \"\"\"\n",
    "        if max_len is None:\n",
    "            max_len = self.max_length\n",
    "        device = imgA.device\n",
    "\n",
    "        # --- Encode Images ---\n",
    "        with torch.no_grad():\n",
    "            encoder_features_list = self.encoder(imgA, imgB)\n",
    "            encoder_output = encoder_features_list[0]\n",
    "            memory = self.input_proj(encoder_output)\n",
    "            N, _, H_mem, W_mem = memory.shape\n",
    "            memory = memory.permute(0, 2, 3, 1).view(N, -1, self.d_model) # (1, S, d_model)\n",
    "            # Expand memory for beam size\n",
    "            memory = memory.expand(beam_size, -1, -1) # (beam_size, S, d_model)\n",
    "            memory_padding_mask = None\n",
    "\n",
    "        # --- Initialize Beams ---\n",
    "        # Start with the <START> token\n",
    "        initial_input = torch.full((beam_size, 1), start_idx, dtype=torch.long, device=device) # (beam_size, 1)\n",
    "\n",
    "        # Top k sequences found so far (log_prob, sequence)\n",
    "        # Use negative log probability because heapq is a min-heap\n",
    "        top_k_sequences = [(-0.0, initial_input)] # Start with zero log prob\n",
    "        completed_sequences = [] # (neg_log_prob, sequence)\n",
    "\n",
    "        # --- Decoding Loop ---\n",
    "        for t in range(max_len - 1): # Max length steps\n",
    "            if not top_k_sequences: # Stop if no active beams\n",
    "                break\n",
    "\n",
    "            new_candidates = [] # Store candidates for the next step (neg_log_prob, sequence)\n",
    "\n",
    "            for neg_log_prob, current_seq in top_k_sequences:\n",
    "                # current_seq shape: (beam_size, current_len) -> need (1, current_len) for model\n",
    "                # We process one beam candidate at a time here for simplicity,\n",
    "                # although batching across beams is more efficient.\n",
    "                current_seq_single = current_seq[0:1, :] # Take the first row (batch size 1)\n",
    "\n",
    "                # Prepare decoder input\n",
    "                tgt_emb = self.embedding(current_seq_single) * math.sqrt(self.d_model)\n",
    "                tgt_pos = self.pos_encoder(tgt_emb)\n",
    "\n",
    "                tgt_mask = self._generate_square_subsequent_mask(current_seq_single.size(1), device)\n",
    "                tgt_padding_mask = self._create_padding_mask(current_seq_single, pad_idx)\n",
    "\n",
    "                # Decode one step\n",
    "                # Use only the first beam's memory, as it's expanded\n",
    "                decoder_output, _, _ = self.decoder(tgt_pos, memory[0:1,:,:], # Use memory for batch size 1\n",
    "                                                    tgt_mask=tgt_mask,\n",
    "                                                    tgt_key_padding_mask=tgt_padding_mask,\n",
    "                                                    memory_key_padding_mask=memory_padding_mask)\n",
    "                                                    # Shape: (1, current_len, d_model)\n",
    "\n",
    "                # Get logits for the last token\n",
    "                last_token_logits = self.output_layer(decoder_output[:, -1, :]) # (1, vocab_size)\n",
    "                log_probs = F.log_softmax(last_token_logits, dim=-1) # (1, vocab_size)\n",
    "\n",
    "                # Get top k next tokens and their log probabilities\n",
    "                # Add current sequence's log prob to the next token's log prob\n",
    "                top_log_probs, top_indices = torch.topk(log_probs.squeeze(0), beam_size, dim=-1) # (beam_size,)\n",
    "\n",
    "                # Add candidates to the list\n",
    "                for i in range(beam_size):\n",
    "                    next_token_idx = top_indices[i].item()\n",
    "                    next_token_log_prob = top_log_probs[i].item()\n",
    "                    total_neg_log_prob = neg_log_prob - next_token_log_prob # Add log probs = subtract neg log probs\n",
    "\n",
    "                    # Create the new sequence\n",
    "                    new_seq = torch.cat([current_seq_single, top_indices[i].unsqueeze(0).unsqueeze(0)], dim=1) # (1, current_len + 1)\n",
    "\n",
    "                    if next_token_idx == end_idx:\n",
    "                        # Add completed sequence (normalize log prob by length?)\n",
    "                        # Simple normalization: divide by length\n",
    "                        # More sophisticated: length penalty alpha * log(len) / log(start_len)\n",
    "                        normalized_neg_log_prob = total_neg_log_prob / (new_seq.size(1)**0.7) # Length penalty\n",
    "                        completed_sequences.append((normalized_neg_log_prob, new_seq))\n",
    "                    else:\n",
    "                        # Add to candidates for next step\n",
    "                         new_candidates.append((total_neg_log_prob, new_seq))\n",
    "\n",
    "            # --- Prune Candidates ---\n",
    "            # Sort all candidates by negative log probability and keep top beam_size\n",
    "            new_candidates.sort(key=lambda x: x[0])\n",
    "            top_k_sequences = new_candidates[:beam_size]\n",
    "\n",
    "            # --- Prune Completed Sequences ---\n",
    "            # Keep only the top beam_size completed sequences\n",
    "            completed_sequences.sort(key=lambda x: x[0])\n",
    "            completed_sequences = completed_sequences[:beam_size]\n",
    "\n",
    "\n",
    "        # If no sequences completed, return the best active beam\n",
    "        if not completed_sequences:\n",
    "             if top_k_sequences:\n",
    "                 # Normalize the score of the best active beam\n",
    "                 best_neg_log_prob, best_seq = top_k_sequences[0]\n",
    "                 normalized_prob = best_neg_log_prob / (best_seq.size(1)**0.7)\n",
    "                 completed_sequences.append((normalized_prob, best_seq))\n",
    "             else:\n",
    "                 # Should not happen if start token is valid, but handle anyway\n",
    "                 return [start_idx, end_idx] # Return minimal sequence\n",
    "\n",
    "\n",
    "        # Return the best completed sequence (lowest normalized negative log prob)\n",
    "        best_normalized_prob, best_sequence = min(completed_sequences, key=lambda x: x[0])\n",
    "        return best_sequence.squeeze(0).tolist() # Return as list of ints\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- Training & Validation Functions (Updated) ---\\\n",
    "# =============================================================================\n",
    "\n",
    "def train_segmentation(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    \"\"\"Trains the segmentation model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    ious = AverageMeter()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, data in enumerate(tqdm(train_loader, desc=f\"Seg Training Epoch {epoch}\")):\n",
    "        imgA = data['imgA'].to(device, non_blocking=True)\n",
    "        imgB = data['imgB'].to(device, non_blocking=True)\n",
    "        seg_labels = data['seg_label'].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        seg_outputs = model(imgA, imgB) # Shape: (N, num_classes, H, W)\n",
    "\n",
    "        # Loss calculation (CrossEntropyLoss handles logits directly)\n",
    "        loss = criterion(seg_outputs, seg_labels)\n",
    "\n",
    "        loss.backward()\n",
    "        # Optional: Gradient clipping\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.update(loss.item(), imgA.size(0))\n",
    "\n",
    "        # Calculate IoU\n",
    "        with torch.no_grad():\n",
    "             _, predicted_masks = torch.max(seg_outputs, 1)\n",
    "             batch_iou_per_class = calculate_iou(predicted_masks, seg_labels, model) # Pass model\n",
    "             # Average IoU across classes for the batch (handle NaNs)\n",
    "             mean_batch_iou = torch.nanmean(batch_iou_per_class).item() if not torch.all(torch.isnan(batch_iou_per_class)) else 0.0\n",
    "             ious.update(mean_batch_iou, imgA.size(0))\n",
    "\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch} Seg Training Loss: {losses.avg:.4f}, Mean IoU: {ious.avg:.4f}, Time: {epoch_time:.2f}s\")\n",
    "    return losses.avg # Return loss for LR scheduler\n",
    "\n",
    "\n",
    "def validate_segmentation(val_loader, model, criterion, device):\n",
    "    \"\"\"Validates the segmentation model.\"\"\"\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    all_class_ious = [] # List to store IoU tensor (num_classes,) for each batch\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(val_loader, desc=\"Seg Validation\")):\n",
    "            imgA = data['imgA'].to(device, non_blocking=True)\n",
    "            imgB = data['imgB'].to(device, non_blocking=True)\n",
    "            seg_labels = data['seg_label'].to(device, non_blocking=True)\n",
    "\n",
    "            seg_outputs = model(imgA, imgB)\n",
    "            loss = criterion(seg_outputs, seg_labels)\n",
    "            losses.update(loss.item(), imgA.size(0))\n",
    "\n",
    "            _, predicted_masks = torch.max(seg_outputs, 1)\n",
    "            batch_iou_per_class = calculate_iou(predicted_masks, seg_labels, model) # Pass model\n",
    "            # Store the IoU tensor for this batch (handle potential all-NaN case)\n",
    "            if not torch.all(torch.isnan(batch_iou_per_class)):\n",
    "                 all_class_ious.append(batch_iou_per_class)\n",
    "\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    # Get num_classes from the model\n",
    "    model_ref = model.module if isinstance(model, nn.DataParallel) else model\n",
    "    num_classes = model_ref.num_classes\n",
    "\n",
    "    # Calculate mean IoU across all batches and classes\n",
    "    if all_class_ious:\n",
    "        # Stack IoUs from all batches: (num_valid_batches, num_classes)\n",
    "        stacked_ious = torch.stack([iou for iou in all_class_ious if not torch.all(torch.isnan(iou))])\n",
    "        # Mean IoU per class across batches (ignoring NaNs within a class's calculation)\n",
    "        mean_iou_per_class = torch.nanmean(stacked_ious, dim=0) # (num_classes,)\n",
    "        # Overall mean IoU (mean of per-class means, ignoring NaNs)\n",
    "        mean_overall_iou = torch.nanmean(mean_iou_per_class).item()\n",
    "    else:\n",
    "        mean_iou_per_class = torch.zeros(num_classes, device=device) * float('nan')\n",
    "        mean_overall_iou = 0.0\n",
    "\n",
    "    print(f\"Seg Validation Loss: {losses.avg:.4f}, Mean Overall IoU: {mean_overall_iou:.4f}\")\n",
    "    # Replace NaN with 0 for printing if desired\n",
    "    iou_list_print = [f\"{iou:.4f}\" if not torch.isnan(iou) else \"NaN\" for iou in mean_iou_per_class]\n",
    "    print(f\"IoU per class: {iou_list_print}\")\n",
    "    print(f\"Validation Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    return losses.avg, mean_overall_iou\n",
    "\n",
    "\n",
    "def train_captioning(train_loader, model, criterion, optimizer, epoch, device, pad_idx):\n",
    "    \"\"\"Trains the captioning model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, data in enumerate(tqdm(train_loader, desc=f\"Cap Training Epoch {epoch}\")):\n",
    "        imgA = data['imgA'].to(device, non_blocking=True)\n",
    "        imgB = data['imgB'].to(device, non_blocking=True)\n",
    "        # caption_tokens shape: (N, max_length) - includes <START> and <END>/<NULL>\n",
    "        caption_tokens = data['caption_tokens'].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Input is shifted right, excludes last token (<END>/<NULL>)\n",
    "        # Output logits correspond to targets shifted left, excludes first token (<START>)\n",
    "        logits = model(imgA, imgB, caption_tokens, pad_idx) # (N, T-1, vocab_size)\n",
    "\n",
    "        # Targets for loss: Exclude <START> token\n",
    "        targets = caption_tokens[:, 1:] # (N, T-1)\n",
    "\n",
    "        # Calculate loss\n",
    "        # Reshape for CrossEntropyLoss: (N * (T-1), vocab_size) and (N * (T-1),)\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        # Optional: Gradient clipping\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.update(loss.item(), imgA.size(0)) # Use batch size for averaging loss\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch} Cap Training Loss: {losses.avg:.4f}, Time: {epoch_time:.2f}s\")\n",
    "    return losses.avg # Return loss for LR scheduler\n",
    "\n",
    "\n",
    "def validate_captioning(val_loader, model, criterion, device, idx_to_word, start_idx, end_idx, pad_idx, beam_size=3):\n",
    "    \"\"\"Validates the captioning model using Beam Search and calculates BLEU score.\"\"\"\n",
    "    model.eval()\n",
    "    references = [] # List of lists of lists of words (ground truth)\n",
    "    hypotheses = [] # List of lists of words (predicted)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Get the actual model object if wrapped in DataParallel\n",
    "    model_ref = model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(val_loader, desc=\"Cap Validation (Beam Search)\")):\n",
    "            # Process one image pair at a time for beam search simplicity\n",
    "            # More efficient implementations would batch the beam search.\n",
    "            batch_size = data['imgA'].size(0)\n",
    "            for j in range(batch_size):\n",
    "                imgA_single = data['imgA'][j:j+1].to(device) # Keep batch dim: (1, 3, H, W)\n",
    "                imgB_single = data['imgB'][j:j+1].to(device) # Keep batch dim: (1, 3, H, W)\n",
    "                # all_caption_tokens_single: list of lists of token IDs for this image\n",
    "                all_caption_tokens_single = data['all_caption_tokens'][j]\n",
    "\n",
    "                # --- Beam Search Decoding ---\n",
    "                generated_ids = model_ref.beam_search_decode(imgA_single, imgB_single,\n",
    "                                                             beam_size, start_idx, end_idx, pad_idx,\n",
    "                                                             max_len=val_loader.dataset.max_length)\n",
    "\n",
    "                # Convert generated IDs to words\n",
    "                generated_words = []\n",
    "                for token_id in generated_ids:\n",
    "                    if token_id == end_idx: break\n",
    "                    if token_id not in [start_idx, pad_idx]: # Exclude special tokens\n",
    "                        generated_words.append(idx_to_word.get(token_id, '<UNK>'))\n",
    "                hypotheses.append(generated_words)\n",
    "\n",
    "                # Convert ground truth token IDs to words\n",
    "                image_references = []\n",
    "                for gt_caption_tokens in all_caption_tokens_single:\n",
    "                    gt_words = []\n",
    "                    for token_id in gt_caption_tokens:\n",
    "                        if token_id == end_idx: break\n",
    "                        if token_id not in [start_idx, pad_idx]: # Exclude special tokens\n",
    "                             gt_words.append(idx_to_word.get(token_id, '<UNK>'))\n",
    "                    # Add non-empty references only\n",
    "                    if gt_words:\n",
    "                        image_references.append(gt_words)\n",
    "                # Add references only if there are any valid ones\n",
    "                if image_references:\n",
    "                    references.append(image_references)\n",
    "                else:\n",
    "                    # If no valid references, add a dummy one to align with hypothesis?\n",
    "                    # Or handle potential mismatch later in BLEU calc. Let's add dummy.\n",
    "                    references.append([['<UNK>']]) # Add a dummy reference\n",
    "                    print(f\"Warning: No valid reference captions found for image {data['name'][j]}. Added dummy reference.\")\n",
    "\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    # --- Calculate BLEU scores ---\n",
    "    bleu_1, bleu_2, bleu_3, bleu_4 = 0.0, 0.0, 0.0, 0.0\n",
    "    if not references or not hypotheses or len(references) != len(hypotheses):\n",
    "         print(f\"Warning: Mismatch in number of references ({len(references)}) and hypotheses ({len(hypotheses)}) or empty lists. Cannot calculate BLEU.\")\n",
    "    else:\n",
    "        try:\n",
    "            smooth = SmoothingFunction().method1 # Choose a smoothing method\n",
    "            bleu_1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
    "            bleu_2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth)\n",
    "            bleu_3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth)\n",
    "            bleu_4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
    "            print(f\"Cap Validation BLEU-1: {bleu_1:.4f}, BLEU-2: {bleu_2:.4f}, BLEU-3: {bleu_3:.4f}, BLEU-4: {bleu_4:.4f}\")\n",
    "        except ZeroDivisionError:\n",
    "            print(\"Could not calculate BLEU score (division by zero). Check generated captions.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating BLEU score: {e}\")\n",
    "\n",
    "\n",
    "    print(f\"Validation Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Return BLEU-4 score (commonly used primary metric)\n",
    "    # Note: Validation loss is not calculated here as we use beam search, not teacher forcing.\n",
    "    return bleu_4\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- Main Execution ---\\\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # --- Configuration Parameters ---\n",
    "    # Choose the mode: 'train_segmentation' or 'train_captioning'\n",
    "    RUN_MODE = 'train_captioning' # <-- SET MODE HERE ('train_segmentation' or 'train_captioning')\n",
    "\n",
    "    # General Parameters\n",
    "    DATA_FOLDER = DATASET_ROOT\n",
    "    PROCESSED_DATA_DIR = os.path.join(SAVE_OUTPUT_DIR, 'processed_data_v2') # Use new dir for potentially different vocab/splits\n",
    "    CAPTION_FILE = 'LevirCCcaptions.json'\n",
    "    CHECKPOINT_DIR = os.path.join(SAVE_OUTPUT_DIR, 'checkpoints_v2') # Use new dir for new models\n",
    "    EPOCHS = 20 # Increased epochs might be needed for larger models\n",
    "    BATCH_SIZE = 8 # Reduced batch size might be needed due to larger models\n",
    "    NUM_WORKERS = 4 # Adjust based on Kaggle instance resources\n",
    "    LR = 5e-5 # Potentially lower LR for fine-tuning pre-trained models\n",
    "    WEIGHT_DECAY = 1e-4 # Added weight decay\n",
    "    RESUME_CHECKPOINT = '' # Set path to resume\n",
    "\n",
    "    # Preprocessing Parameters\n",
    "    MAX_LENGTH = 45 # Increased slightly, check preprocessing output warning\n",
    "    WORD_COUNT_THRESHOLD = 3 # Slightly lower threshold?\n",
    "\n",
    "    # --- Model Specific Parameters ---\n",
    "    # Shared Encoder (ResNet)\n",
    "    ENCODER_ARCH = 'resnet34' # Options: 'resnet18', 'resnet34', 'resnet50', 'resnet101'\n",
    "    ENCODER_PRETRAINED = True\n",
    "    ENCODER_FREEZE_LAYERS = 25 # 0: train all, 1: freeze conv1/layer1, 2: freeze up to layer2, etc.\n",
    "\n",
    "    # Segmentation Model (U-Net Decoder)\n",
    "    SEG_DECODER_CHANNELS = (256, 128, 64, 32) # Adjust based on encoder size if needed\n",
    "    SEG_FINAL_UPSAMPLE_MODE = 'bilinear' # 'bilinear' or 'nearest'\n",
    "\n",
    "    # Captioning Model (Transformer Decoder)\n",
    "    CAP_D_MODEL = 512 # Must match embedding dim, proj dim\n",
    "    CAP_NHEAD = 8\n",
    "    CAP_NUM_DECODER_LAYERS = 4 # Fewer layers might be faster to train initially\n",
    "    CAP_DIM_FEEDFORWARD = 1024 # Adjust based on d_model and layers\n",
    "    CAP_DROPOUT = 0.1\n",
    "    # Beam Search for Validation\n",
    "    VAL_BEAM_SIZE = 3\n",
    "\n",
    "    # --- Setup ---\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    if DEVICE == 'cuda':\n",
    "        print(f\"CUDA Devices: {torch.cuda.device_count()}\")\n",
    "        # Set a specific device if needed, e.g., torch.cuda.set_device(0)\n",
    "\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    os.makedirs(PROCESSED_DATA_DIR, exist_ok=True) # Ensure processed data dir exists\n",
    "\n",
    "    # --- Check and Run Preprocessing ---\n",
    "    # Check for essential files from preprocessing\n",
    "    vocab_exists = os.path.exists(os.path.join(PROCESSED_DATA_DIR, 'vocab.json'))\n",
    "    split_exists = os.path.exists(os.path.join(PROCESSED_DATA_DIR, 'train.txt'))\n",
    "    tokens_exist = os.path.exists(os.path.join(PROCESSED_DATA_DIR, 'tokens'))\n",
    "    preprocessing_needed = not (vocab_exists and split_exists and tokens_exist)\n",
    "\n",
    "    if preprocessing_needed:\n",
    "        print(f\"Preprocessing output not found or incomplete in {PROCESSED_DATA_DIR}. Running preprocessing.\")\n",
    "        try:\n",
    "            run_preprocessing_direct(DATA_FOLDER, PROCESSED_DATA_DIR, CAPTION_FILE, MAX_LENGTH, WORD_COUNT_THRESHOLD)\n",
    "            print(\"Preprocessing complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during preprocessing: {e}\")\n",
    "            print(\"Please ensure the dataset path and structure are correct.\")\n",
    "            sys.exit(1) # Exit if preprocessing fails\n",
    "    else:\n",
    "        print(f\"Preprocessing output found in {PROCESSED_DATA_DIR}. Skipping preprocessing.\")\n",
    "\n",
    "\n",
    "    # --- Main Logic ---\n",
    "    try:\n",
    "        if RUN_MODE == 'train_segmentation':\n",
    "            print(f\"\\n--- Starting Segmentation Training ({ENCODER_ARCH} + U-Net) ---\")\n",
    "\n",
    "            # Calculate class weights\n",
    "            class_weights = calculate_class_weights(DATA_FOLDER, PROCESSED_DATA_DIR, split='train', num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "            # Load Data\n",
    "            print(\"Loading segmentation datasets...\")\n",
    "            train_dataset = LEVIRCCDataset(DATA_FOLDER, PROCESSED_DATA_DIR, 'train', load_segmentation=True, max_length=MAX_LENGTH)\n",
    "            val_dataset = LEVIRCCDataset(DATA_FOLDER, PROCESSED_DATA_DIR, 'val', load_segmentation=True, max_length=MAX_LENGTH)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "            # Initialize Model, Loss, Optimizer, Scheduler\n",
    "            print(\"Initializing segmentation model...\")\n",
    "            model = SegmentationModel(num_classes=NUM_CLASSES,\n",
    "                                      encoder_arch=ENCODER_ARCH,\n",
    "                                      pretrained=ENCODER_PRETRAINED,\n",
    "                                      freeze_encoder_layers=ENCODER_FREEZE_LAYERS,\n",
    "                                      decoder_channels=SEG_DECODER_CHANNELS,\n",
    "                                      final_upsample_mode=SEG_FINAL_UPSAMPLE_MODE).to(DEVICE)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss(weight=class_weights).to(DEVICE)\n",
    "            # Filter parameters that require gradients for the optimizer\n",
    "            params_to_optimize = filter(lambda p: p.requires_grad, model.parameters())\n",
    "            optimizer = optim.AdamW(params_to_optimize, lr=LR, weight_decay=WEIGHT_DECAY) # Use AdamW\n",
    "            # Learning Rate Scheduler\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=3, verbose=True) # Monitor validation IoU\n",
    "\n",
    "            # DataParallel\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                print(f\"Using {torch.cuda.device_count()} GPUs for Segmentation Training.\")\n",
    "                model = nn.DataParallel(model)\n",
    "\n",
    "            # Resume Checkpoint\n",
    "            start_epoch = 0\n",
    "            best_metric = 0.0 # Use IoU as the metric\n",
    "            if RESUME_CHECKPOINT and os.path.isfile(RESUME_CHECKPOINT):\n",
    "                print(f\"Loading checkpoint '{RESUME_CHECKPOINT}'\")\n",
    "                checkpoint = torch.load(RESUME_CHECKPOINT, map_location=DEVICE)\n",
    "                start_epoch = checkpoint['epoch'] + 1\n",
    "                # Handle DataParallel state dict loading\n",
    "                state_dict = checkpoint['state_dict']\n",
    "                if isinstance(model, nn.DataParallel) and not list(state_dict.keys())[0].startswith('module.'):\n",
    "                    state_dict = {'module.' + k: v for k, v in state_dict.items()}\n",
    "                elif not isinstance(model, nn.DataParallel) and list(state_dict.keys())[0].startswith('module.'):\n",
    "                     state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "                model.load_state_dict(state_dict)\n",
    "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "                best_metric = checkpoint.get('best_metric', 0.0)\n",
    "                if 'scheduler_state_dict' in checkpoint and hasattr(scheduler, 'load_state_dict'):\n",
    "                     scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "                print(f\"Loaded checkpoint (epoch {checkpoint['epoch']}), Best IoU: {best_metric:.4f}\")\n",
    "            elif RESUME_CHECKPOINT:\n",
    "                 print(f\"Checkpoint not found at '{RESUME_CHECKPOINT}'\")\n",
    "\n",
    "\n",
    "            # Training Loop\n",
    "            print(\"Starting training loop...\")\n",
    "            for epoch in range(start_epoch, EPOCHS):\n",
    "                train_loss = train_segmentation(train_loader, model, criterion, optimizer, epoch, DEVICE)\n",
    "                val_loss, val_metric = validate_segmentation(val_loader, model, criterion, DEVICE) # val_metric is mIoU\n",
    "\n",
    "                # LR Scheduler Step (based on validation IoU)\n",
    "                scheduler.step(val_metric)\n",
    "\n",
    "                # Save checkpoint\n",
    "                is_best = val_metric > best_metric\n",
    "                best_metric = max(val_metric, best_metric)\n",
    "\n",
    "                # Prepare state dict correctly (save without 'module.' prefix)\n",
    "                model_state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "\n",
    "                save_dict = {\n",
    "                    'epoch': epoch,\n",
    "                    'arch': ENCODER_ARCH,\n",
    "                    'state_dict': model_state,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'best_metric': best_metric, # Best IoU\n",
    "                    'current_metric': val_metric, # Current IoU\n",
    "                    'val_loss': val_loss\n",
    "                }\n",
    "\n",
    "                checkpoint_path = os.path.join(CHECKPOINT_DIR, f'segmentation_{ENCODER_ARCH}_ep{epoch}.pth.tar')\n",
    "                torch.save(save_dict, checkpoint_path)\n",
    "                print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "                if is_best:\n",
    "                    best_checkpoint_path = os.path.join(CHECKPOINT_DIR, f'BEST_segmentation_{ENCODER_ARCH}.pth.tar')\n",
    "                    torch.save(save_dict, best_checkpoint_path)\n",
    "                    print(f\"*** Best segmentation checkpoint saved to {best_checkpoint_path} with IoU: {best_metric:.4f} ***\")\n",
    "\n",
    "            print(\"Segmentation Training Finished.\")\n",
    "\n",
    "        elif RUN_MODE == 'train_captioning':\n",
    "            print(f\"\\n--- Starting Captioning Training ({ENCODER_ARCH} + Transformer) ---\")\n",
    "\n",
    "            # Load Data (load_segmentation=False)\n",
    "            print(\"Loading captioning datasets...\")\n",
    "            train_dataset = LEVIRCCDataset(DATA_FOLDER, PROCESSED_DATA_DIR, 'train', load_segmentation=False, max_length=MAX_LENGTH)\n",
    "            val_dataset = LEVIRCCDataset(DATA_FOLDER, PROCESSED_DATA_DIR, 'val', load_segmentation=False, max_length=MAX_LENGTH)\n",
    "\n",
    "            # Get vocab info from dataset\n",
    "            vocab_size = train_dataset.vocab_size\n",
    "            pad_idx = train_dataset.pad_idx\n",
    "            start_idx = train_dataset.start_idx\n",
    "            end_idx = train_dataset.end_idx\n",
    "            idx_to_word = train_dataset.idx_to_word\n",
    "            print(f\"Vocab Size: {vocab_size}, Pad Index: {pad_idx}\")\n",
    "\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "            # Use batch_size=1 for validation if beam search isn't batched\n",
    "            val_batch_size = 1 # Adjust if beam search implementation handles batches\n",
    "            val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "\n",
    "            # Initialize Model, Loss, Optimizer, Scheduler\n",
    "            print(\"Initializing captioning model...\")\n",
    "            model = CaptioningModel(vocab_size=vocab_size,\n",
    "                                    d_model=CAP_D_MODEL,\n",
    "                                    nhead=CAP_NHEAD,\n",
    "                                    num_decoder_layers=CAP_NUM_DECODER_LAYERS,\n",
    "                                    dim_feedforward=CAP_DIM_FEEDFORWARD,\n",
    "                                    dropout=CAP_DROPOUT,\n",
    "                                    encoder_arch=ENCODER_ARCH,\n",
    "                                    pretrained=ENCODER_PRETRAINED,\n",
    "                                    freeze_encoder_layers=ENCODER_FREEZE_LAYERS,\n",
    "                                    max_length=MAX_LENGTH).to(DEVICE)\n",
    "\n",
    "            # Set padding_idx for embedding if applicable\n",
    "            model.embedding.padding_idx = pad_idx\n",
    "\n",
    "            # Cross-Entropy loss, ignore padding\n",
    "            criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(DEVICE)\n",
    "            # Filter parameters that require gradients\n",
    "            params_to_optimize = filter(lambda p: p.requires_grad, model.parameters())\n",
    "            optimizer = optim.AdamW(params_to_optimize, lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "            # LR Scheduler (Monitor validation BLEU-4)\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=3, verbose=True)\n",
    "\n",
    "            # DataParallel\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                print(f\"Using {torch.cuda.device_count()} GPUs for Captioning Training.\")\n",
    "                model = nn.DataParallel(model)\n",
    "\n",
    "            # Resume Checkpoint\n",
    "            start_epoch = 0\n",
    "            best_metric = 0.0 # Use BLEU-4 as the metric\n",
    "            if RESUME_CHECKPOINT and os.path.isfile(RESUME_CHECKPOINT):\n",
    "                print(f\"Loading checkpoint '{RESUME_CHECKPOINT}'\")\n",
    "                checkpoint = torch.load(RESUME_CHECKPOINT, map_location=DEVICE)\n",
    "                start_epoch = checkpoint['epoch'] + 1\n",
    "                 # Handle DataParallel state dict loading\n",
    "                state_dict = checkpoint['state_dict']\n",
    "                if isinstance(model, nn.DataParallel) and not list(state_dict.keys())[0].startswith('module.'):\n",
    "                    state_dict = {'module.' + k: v for k, v in state_dict.items()}\n",
    "                elif not isinstance(model, nn.DataParallel) and list(state_dict.keys())[0].startswith('module.'):\n",
    "                     state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "                model.load_state_dict(state_dict)\n",
    "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "                best_metric = checkpoint.get('best_metric', 0.0) # Best BLEU-4\n",
    "                if 'scheduler_state_dict' in checkpoint and hasattr(scheduler, 'load_state_dict'):\n",
    "                     scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "                print(f\"Loaded checkpoint (epoch {checkpoint['epoch']}), Best BLEU-4: {best_metric:.4f}\")\n",
    "            elif RESUME_CHECKPOINT:\n",
    "                 print(f\"Checkpoint not found at '{RESUME_CHECKPOINT}'\")\n",
    "\n",
    "\n",
    "            # Training Loop\n",
    "            print(\"Starting training loop...\")\n",
    "            for epoch in range(start_epoch, EPOCHS):\n",
    "                train_loss = train_captioning(train_loader, model, criterion, optimizer, epoch, DEVICE, pad_idx)\n",
    "                val_metric = validate_captioning(val_loader, model, criterion, DEVICE, idx_to_word, start_idx, end_idx, pad_idx, beam_size=VAL_BEAM_SIZE) # val_metric is BLEU-4\n",
    "\n",
    "                # LR Scheduler Step (based on validation BLEU-4)\n",
    "                scheduler.step(val_metric)\n",
    "\n",
    "                # Save checkpoint\n",
    "                is_best = val_metric > best_metric\n",
    "                best_metric = max(val_metric, best_metric)\n",
    "\n",
    "                # Prepare state dict correctly\n",
    "                model_state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "\n",
    "                save_dict = {\n",
    "                    'epoch': epoch,\n",
    "                    'arch': ENCODER_ARCH,\n",
    "                    'state_dict': model_state,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'best_metric': best_metric, # Best BLEU-4\n",
    "                    'current_metric': val_metric, # Current BLEU-4\n",
    "                    'train_loss': train_loss,\n",
    "                    'vocab_size': vocab_size, # Save vocab info if needed\n",
    "                    'pad_idx': pad_idx,\n",
    "                    'start_idx': start_idx,\n",
    "                    'end_idx': end_idx\n",
    "                }\n",
    "\n",
    "                checkpoint_path = os.path.join(CHECKPOINT_DIR, f'captioning_{ENCODER_ARCH}_ep{epoch}.pth.tar')\n",
    "                torch.save(save_dict, checkpoint_path)\n",
    "                print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "                if is_best:\n",
    "                    best_checkpoint_path = os.path.join(CHECKPOINT_DIR, f'BEST_captioning_{ENCODER_ARCH}.pth.tar')\n",
    "                    torch.save(save_dict, best_checkpoint_path)\n",
    "                    print(f\"*** Best captioning checkpoint saved to {best_checkpoint_path} with BLEU-4: {best_metric:.4f} ***\")\n",
    "\n",
    "\n",
    "            print(\"Captioning Training Finished.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Invalid RUN_MODE: {RUN_MODE}. Choose 'train_segmentation' or 'train_captioning'.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nError: A required file or directory was not found.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        print(\"Please ensure the dataset path (DATASET_ROOT) is correct and that preprocessing ran successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7241459,
     "sourceId": 11547271,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
