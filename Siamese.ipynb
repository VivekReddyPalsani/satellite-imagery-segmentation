{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-01T18:32:57.003Z",
     "iopub.execute_input": "2025-05-01T18:05:09.275856Z",
     "iopub.status.busy": "2025-05-01T18:05:09.275575Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in mode: train\n",
      "Preprocessing output not found. Running preprocessing first.\n",
      "Using LEVIR_MCI dataset from: /kaggle/input/levir-mci-dataset/LEVIR-MCI-dataset\n",
      "Saving processed data to: /kaggle/working/\n",
      "Loading captions from JSON...\n",
      "Processing pre-tokenized captions...\n",
      "Processed captions for 10077 images.\n",
      "Building vocabulary...\n",
      "Vocabulary size: 501\n",
      "Saving vocabulary to /kaggle/working/vocab.json\n",
      "Encoding and padding captions...\n",
      "Saving train image list to /kaggle/working/train.txt (6815 images)\n",
      "Saving val image list to /kaggle/working/val.txt (1333 images)\n",
      "Saving test image list to /kaggle/working/test.txt (1929 images)\n",
      "Preprocessing finished.\n",
      "Using CUDA GPU: Tesla T4\n",
      "Detected 2 GPUs. Will use DataParallel.\n",
      "\n",
      "Checkpoints and logs will be saved in: /kaggle/working/training_output\n",
      "Loading Datasets...\n",
      "Initialized LEVIRCCDataset for split 'train' with 6815 items (after potential repeat for max_iters).\n",
      "Initialized LEVIRCCDataset for split 'val' with 1333 items (after potential repeat for max_iters).\n",
      "Initializing Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 218MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training weights for encoder: resnet50\n",
      "Decoder loaded vocabulary with size: 501\n",
      "Wrapping model with DataParallel across 2 GPUs.\n",
      "\n",
      "No checkpoint found. Starting training from scratch.\n",
      "Starting Training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]:   0%|          | 0/425 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Epoch 1/30 [Train]: 100%|██████████| 425/425 [04:01<00:00,  1.76it/s, CapL=1.759, SegL=0.357, CombL=2.116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Avg Loss -> Combined: 2.5386 (Cap: 2.0488, Seg: 0.4898)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Validate]: 100%|██████████| 84/84 [00:47<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation -> BLEU-4: 0.2041, mIoU: 0.4123\n",
      "\n",
      "*** New best BLEU score: 0.2041 ***\n",
      "\n",
      "*** New best mIoU score: 0.4123 ***\n",
      "\n",
      "Saved best BLEU checkpoint to /kaggle/working/training_output/checkpoint_best_bleu.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|██████████| 425/425 [04:04<00:00,  1.74it/s, CapL=1.217, SegL=0.161, CombL=1.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Avg Loss -> Combined: 1.6848 (Cap: 1.4543, Seg: 0.2305)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Validate]: 100%|██████████| 84/84 [00:40<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation -> BLEU-4: 0.6292, mIoU: 0.5436\n",
      "\n",
      "*** New best BLEU score: 0.6292 ***\n",
      "\n",
      "*** New best mIoU score: 0.5436 ***\n",
      "\n",
      "Saved best BLEU checkpoint to /kaggle/working/training_output/checkpoint_best_bleu.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|██████████| 425/425 [04:05<00:00,  1.73it/s, CapL=1.825, SegL=0.188, CombL=2.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Avg Loss -> Combined: 1.4697 (Cap: 1.3127, Seg: 0.1570)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Validate]: 100%|██████████| 84/84 [00:41<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation -> BLEU-4: 0.6063, mIoU: 0.5715\n",
      "\n",
      "*** New best mIoU score: 0.5715 ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|██████████| 425/425 [04:04<00:00,  1.74it/s, CapL=1.067, SegL=0.134, CombL=1.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Avg Loss -> Combined: 1.3778 (Cap: 1.2530, Seg: 0.1248)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Validate]: 100%|██████████| 84/84 [00:41<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation -> BLEU-4: 0.5381, mIoU: 0.5915\n",
      "\n",
      "*** New best mIoU score: 0.5915 ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|██████████| 425/425 [04:04<00:00,  1.74it/s, CapL=0.982, SegL=0.088, CombL=1.070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Avg Loss -> Combined: 1.3194 (Cap: 1.2094, Seg: 0.1100)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Validate]: 100%|██████████| 84/84 [00:41<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation -> BLEU-4: 0.5818, mIoU: 0.6067\n",
      "\n",
      "*** New best mIoU score: 0.6067 ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]:  59%|█████▊    | 249/425 [02:23<01:40,  1.75it/s, CapL=0.755, SegL=0.034, CombL=0.789]"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Combined Script for Kaggle Notebook with Resume Logic and DataParallel\n",
    "# This script combines dataset.py, model_arch.py, preprocessing.py, and training.py\n",
    "# Adjusted for Kaggle file paths, includes resume logic, and uses torch.nn.DataParallel\n",
    "# for multi-GPU training if available.\n",
    "# Includes fixes for zero BLEU score issue.\n",
    "# This version does NOT include LLM integration.\n",
    "# Corrected AttributeError in LEVIRCCDataset.\n",
    "# Corrected structure of references for BLEU calculation.\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader # Explicitly import Dataset and DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from imageio.v2 import imread # Make sure imageio is installed in your Kaggle environment\n",
    "from random import randint\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# --- Hardcoded Paths for Kaggle ---\n",
    "# IMPORTANT: You MUST update DATASET_ROOT to the correct path of your dataset\n",
    "# after adding it as an input to your Kaggle Notebook.\n",
    "# It will typically be something like '/kaggle/input/your-dataset-name/LEVIR-MCI-dataset'\n",
    "# Check the input path in the Kaggle Notebook's file explorer.\n",
    "# SAVE_OUTPUT_DIR is set to the standard Kaggle output directory, which is persistent\n",
    "# within a single notebook session and can be saved with the notebook.\n",
    "# =============================================================================\n",
    "# Replace 'your-dataset-name' with the actual name you give your dataset when uploading to Kaggle\n",
    "# You might need to inspect the input directory structure to get the exact path.\n",
    "DATASET_ROOT = '/kaggle/input/levir-mci-dataset/LEVIR-MCI-dataset' # <-- *** UPDATE THIS PATH ***\n",
    "SAVE_OUTPUT_DIR = '/kaggle/working/' # Standard Kaggle writable output directory\n",
    "\n",
    "# =============================================================================\n",
    "# --- dataset.py content starts here ---\n",
    "# =============================================================================\n",
    "\n",
    "# Define the mapping from RGB colors to class IDs\n",
    "# Background: 0 (black), Road: 1 (grey), Building: 2 (white)\n",
    "COLOR_TO_ID_MAPPING = {\n",
    "    (0, 0, 0): 0,          # Background\n",
    "    (128, 128, 128): 1,    # Road (Grey)\n",
    "    (255, 255, 255): 2,    # Building (White)\n",
    "}\n",
    "NUM_CLASSES = 3 # Background, Road, Building\n",
    "\n",
    "def rgb_to_class_id_mask(rgb_mask_np):\n",
    "    \"\"\"Converts an RGB mask (H, W, 3) to a class ID mask (H, W).\"\"\"\n",
    "    h, w, c = rgb_mask_np.shape\n",
    "    if c != 3:\n",
    "        # Handle grayscale masks if they appear unexpectedly\n",
    "        if c == 1 or rgb_mask_np.ndim == 2:\n",
    "             print(f\"Warning: Expected RGB mask (H, W, 3) but got grayscale shape {rgb_mask_np.shape}. Trying to process.\")\n",
    "             # Attempt to handle based on intensity if possible (e.g., if 0, 128, 255 are used)\n",
    "             # This requires specific logic based on how grayscale encodes classes.\n",
    "             # For now, assuming direct mapping if values are 0, 1, 2 might be wrong.\n",
    "             # Safest is to raise error or map all to background.\n",
    "             class_id_mask = np.full((h, w), 0, dtype=np.int64) # Default to background class (0)\n",
    "             if rgb_mask_np.ndim == 2:\n",
    "                grey_mask = rgb_mask_np\n",
    "             else: # c == 1\n",
    "                grey_mask = rgb_mask_np.squeeze(-1)\n",
    "\n",
    "             # Apply mapping based on intensity values IF they match expected grayscale equivalents\n",
    "             class_id_mask[grey_mask == 0] = 0    # Background\n",
    "             class_id_mask[grey_mask == 128] = 1  # Road\n",
    "             class_id_mask[grey_mask == 255] = 2  # Building\n",
    "             return class_id_mask\n",
    "        else:\n",
    "            raise ValueError(f\"Input mask must have 3 channels (RGB), but got {c} with shape {rgb_mask_np.shape}\")\n",
    "\n",
    "    # Initialize with background class ID\n",
    "    class_id_mask = np.full((h, w), 0, dtype=np.int64)\n",
    "\n",
    "    # Iterate through the defined mappings\n",
    "    for color, class_id in COLOR_TO_ID_MAPPING.items():\n",
    "        matches = np.all(rgb_mask_np == np.array(color, dtype=rgb_mask_np.dtype), axis=-1)\n",
    "        class_id_mask[matches] = class_id\n",
    "\n",
    "    # Optional: Check for pixels not matching any defined color\n",
    "    # known_mask = np.zeros((h, w), dtype=bool)\n",
    "    # for color in COLOR_TO_ID_MAPPING:\n",
    "    #     known_mask |= np.all(rgb_mask_np == np.array(color, dtype=rgb_mask_np.dtype), axis=-1)\n",
    "    # if not np.all(known_mask):\n",
    "    #     unmatched_pixels = rgb_mask_np[~known_mask]\n",
    "    #     unique_unmatched = np.unique(unmatched_pixels.reshape(-1, 3), axis=0)\n",
    "    #     print(f\"Warning: Some pixels in the RGB mask did not match known class colors. Unique unmatched colors: {unique_unmatched}\")\n",
    "\n",
    "    return class_id_mask\n",
    "\n",
    "\n",
    "class LEVIRCCDataset(Dataset):\n",
    "    def __init__(self, data_folder, processed_data_dir, split,\n",
    "                 load_segmentation=True,\n",
    "                 max_length=41,\n",
    "                 vocab_file='vocab.json',\n",
    "                 allow_unk=True,\n",
    "                 max_iters=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_folder (str): Path to the root LEVIR-MCI dataset folder (containing 'images/').\n",
    "            processed_data_dir (str): Path to the folder where preprocessed data (vocab.json, splits, tokens/) is saved.\n",
    "            split (str): 'train', 'val', or 'test'.\n",
    "            load_segmentation (bool): If True, loads and returns segmentation maps (as class IDs).\n",
    "            max_length (int): Max caption sequence length (from preprocessing).\n",
    "            vocab_file (str): Name of the vocabulary JSON file within processed_data_dir.\n",
    "            allow_unk (bool): Whether to allow unknown tokens when loading captions.\n",
    "            max_iters (int, optional): If specified, repeats the dataset to provide this many items per epoch.\n",
    "        \"\"\"\n",
    "        self.data_folder = data_folder\n",
    "        self.processed_data_dir = processed_data_dir\n",
    "        self.split = split\n",
    "        self.load_segmentation = load_segmentation\n",
    "        self.max_length = max_length\n",
    "        self.allow_unk = allow_unk\n",
    "\n",
    "        # Image normalization parameters (verify these are suitable for your images)\n",
    "        # These are standard ImageNet means/stds, might need to calculate for your dataset\n",
    "        self.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32) * 255 # Standard ImageNet mean * 255\n",
    "        self.std = np.array([0.229, 0.224, 0.225], dtype=np.float32) * 255  # Standard ImageNet std * 255\n",
    "\n",
    "\n",
    "        assert self.split in {'train', 'val', 'test'}\n",
    "\n",
    "        # ---- Load Vocabulary ----\n",
    "        vocab_path = os.path.join(self.processed_data_dir, vocab_file)\n",
    "        try:\n",
    "            with open(vocab_path, 'r') as f:\n",
    "                self.word_vocab = json.load(f)\n",
    "            self.idx_to_word = {v: k for k, v in self.word_vocab.items()}\n",
    "            self.pad_idx = self.word_vocab.get('<NULL>', 0)\n",
    "            self.start_idx = self.word_vocab.get('<START>', 2) # Ensure start_idx is available\n",
    "            self.end_idx = self.word_vocab.get('<END>', 3) # Ensure end_idx is available\n",
    "            # Removed the erroneous print statement here\n",
    "            # print(f\"Decoder loaded vocabulary with size: {self.vocab_size}\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Vocabulary file not found at {vocab_path}. Run preprocessing script first.\")\n",
    "\n",
    "        # --- Load Image Filenames/Base Names ---\n",
    "        split_file_path = os.path.join(self.processed_data_dir, f'{split}.txt')\n",
    "        try:\n",
    "            with open(split_file_path, 'r') as f:\n",
    "                self.img_ids = [line.strip() for line in f if line.strip()]\n",
    "        except FileNotFoundError:\n",
    "             raise FileNotFoundError(f\"Split file not found at {split_file_path}. Run preprocessing script first.\")\n",
    "\n",
    "        if not self.img_ids:\n",
    "            raise ValueError(f\"No image IDs found in split file: {split_file_path}\")\n",
    "\n",
    "        # ---- Prepare file paths ----\n",
    "        self.files = []\n",
    "        image_base_path = os.path.join(self.data_folder, 'images', self.split)\n",
    "        token_base_path = os.path.join(self.processed_data_dir, 'tokens')\n",
    "        label_folder_name = 'label' # Assuming label folder name is 'label'\n",
    "\n",
    "        missing_files_count = 0\n",
    "        for img_base_name in self.img_ids:\n",
    "            img_file_name = f\"{img_base_name}.png\"\n",
    "            token_file_name = f\"{img_base_name}.json\"\n",
    "\n",
    "            file_paths = {\n",
    "                \"name\": img_base_name,\n",
    "                \"imgA\": os.path.join(image_base_path, 'A', img_file_name),\n",
    "                \"imgB\": os.path.join(image_base_path, 'B', img_file_name),\n",
    "                \"token\": os.path.join(token_base_path, token_file_name)\n",
    "            }\n",
    "            seg_path = None\n",
    "            if self.load_segmentation:\n",
    "                # Use the updated label folder name\n",
    "                seg_path = os.path.join(image_base_path, label_folder_name, img_file_name)\n",
    "                file_paths[\"seg_label\"] = seg_path # Changed key name for clarity\n",
    "\n",
    "            # Check if ALL required files exist\n",
    "            paths_to_check = [file_paths[\"imgA\"], file_paths[\"imgB\"], file_paths[\"token\"]]\n",
    "            if self.load_segmentation:\n",
    "                 paths_to_check.append(seg_path)\n",
    "\n",
    "            files_exist = all(os.path.exists(p) for p in paths_to_check if p is not None)\n",
    "\n",
    "            if not files_exist:\n",
    "                missing_files_count += 1\n",
    "                continue\n",
    "\n",
    "            self.files.append(file_paths)\n",
    "\n",
    "        if missing_files_count > 0:\n",
    "            print(f\"Warning: Skipped {missing_files_count} entries due to missing files in split '{self.split}'.\")\n",
    "\n",
    "        if not self.files:\n",
    "             raise ValueError(f\"No valid file sets found for split '{self.split}'. Check file paths and preprocessing output.\")\n",
    "\n",
    "        # --- Handle max_iters ---\n",
    "        self.max_iters = max_iters\n",
    "        if max_iters is not None and max_iters > 0:\n",
    "            if not self.files:\n",
    "                 raise ValueError(f\"Cannot use max_iters > 0 when no valid files were loaded for split '{self.split}'.\")\n",
    "            n_repeat = int(np.ceil(max_iters / len(self.files)))\n",
    "            self.files = self.files * n_repeat\n",
    "\n",
    "        print(f\"Initialized LEVIRCCDataset for split '{self.split}' with {len(self.files)} items (after potential repeat for max_iters).\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.max_iters is not None and self.max_iters > 0:\n",
    "            return self.max_iters\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        actual_index = index % len(self.files) if self.files else 0\n",
    "        if not self.files:\n",
    "             raise IndexError(\"Dataset is empty.\")\n",
    "        datafiles = self.files[actual_index]\n",
    "\n",
    "        # --- Load Images ---\n",
    "        try:\n",
    "            imgA_np = np.array(imread(datafiles[\"imgA\"]), dtype=np.uint8)\n",
    "            imgB_np = np.array(imread(datafiles[\"imgB\"]), dtype=np.uint8)\n",
    "            if imgA_np.ndim != 3 or imgA_np.shape[-1] != 3 or imgB_np.ndim != 3 or imgB_np.shape[-1] != 3:\n",
    "                 raise ValueError(f\"Image dimensions incorrect for {datafiles['name']}. Expected (H, W, 3), got {imgA_np.shape} and {imgB_np.shape}\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error loading images for {datafiles['name']}: {e}\")\n",
    "             raise\n",
    "\n",
    "        # Convert to float32 and HWC -> CHW\n",
    "        imgA = np.asarray(imgA_np, np.float32).transpose(2, 0, 1)\n",
    "        imgB = np.asarray(imgB_np, np.float32).transpose(2, 0, 1)\n",
    "\n",
    "        # --- Normalize Images ---\n",
    "        imgA = (imgA - self.mean[:, None, None]) / self.std[:, None, None]\n",
    "        imgB = (imgB - self.mean[:, None, None]) / self.std[:, None, None]\n",
    "\n",
    "        # --- Load and Process Segmentation ---\n",
    "        seg_mask_class_ids = None\n",
    "        if self.load_segmentation:\n",
    "            try:\n",
    "                # Load mask using the \"seg_label\" key\n",
    "                seg_label_np = np.array(imread(datafiles[\"seg_label\"]), dtype=np.uint8)\n",
    "\n",
    "                # Convert RGB mask to Class ID mask (H, W) using the defined mapping\n",
    "                seg_mask_class_ids_np = rgb_to_class_id_mask(seg_label_np)\n",
    "\n",
    "                # Convert numpy array (int64) to torch LongTensor\n",
    "                seg_mask_class_ids = torch.from_numpy(seg_mask_class_ids_np).long()\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                 print(f\"Error loading segmentation label for {datafiles['name']}. Path: {datafiles['seg_label']}\")\n",
    "                 raise\n",
    "            except Exception as e:\n",
    "                 print(f\"Error processing segmentation label for {datafiles['name']}: {e}\")\n",
    "                 raise\n",
    "\n",
    "\n",
    "        # --- Load Captions ---\n",
    "        token_all = np.array([[self.pad_idx] * self.max_length])\n",
    "        token_all_len = np.array([0])\n",
    "        caption_list = [] # Initialize caption_list\n",
    "\n",
    "        try:\n",
    "            with open(datafiles[\"token\"], 'r') as f:\n",
    "                caption_list = json.load(f)\n",
    "\n",
    "            if not caption_list:\n",
    "                print(f\"Warning: Empty caption list found in file {datafiles['token']} for {datafiles['name']}. Using default padding.\")\n",
    "            else:\n",
    "                # Ensure consistency check happens only if list is not empty\n",
    "                for i, cap in enumerate(caption_list):\n",
    "                     if len(cap) != self.max_length:\n",
    "                          raise ValueError(f\"Inconsistent caption length in {datafiles['token']} for caption {i}. Expected {self.max_length}, got {len(cap)}.\")\n",
    "                token_all = np.array(caption_list, dtype=np.int64)\n",
    "                token_all_len = np.array([(caption != self.pad_idx).sum() for caption in token_all], dtype=np.int64)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             print(f\"Error loading token file for {datafiles['name']}. Path: {datafiles['token']}\")\n",
    "             raise\n",
    "        except Exception as e:\n",
    "             print(f\"Error processing token file {datafiles['token']}: {e}\")\n",
    "             raise\n",
    "\n",
    "        # --- Select one caption for training ---\n",
    "        if not caption_list:\n",
    "             token = token_all[0]\n",
    "             token_len = token_all_len[0].item()\n",
    "        else:\n",
    "            token_index = randint(0, len(caption_list) - 1)\n",
    "            token = token_all[token_index]\n",
    "            token_len = token_all_len[token_index].item()\n",
    "\n",
    "\n",
    "        # --- Convert to Tensors ---\n",
    "        imgA = torch.from_numpy(imgA).float()\n",
    "        imgB = torch.from_numpy(imgB).float()\n",
    "        token = torch.from_numpy(token).long()\n",
    "        token_len = torch.tensor(token_len).long()\n",
    "        token_all = torch.from_numpy(token_all).long()\n",
    "        token_all_len = torch.from_numpy(token_all_len).long()\n",
    "\n",
    "\n",
    "        # --- Return Dictionary ---\n",
    "        batch = {\n",
    "            'imgA': imgA,\n",
    "            'imgB': imgB,\n",
    "            'token': token,\n",
    "            'token_len': token_len,\n",
    "            'token_all': token_all,\n",
    "            'token_all_len': token_all_len,\n",
    "            'name': datafiles['name'],\n",
    "        }\n",
    "        # Add segmentation mask if loaded\n",
    "        if self.load_segmentation and seg_mask_class_ids is not None:\n",
    "             batch['seg_mask'] = seg_mask_class_ids # Key name matches training loop\n",
    "\n",
    "        return batch\n",
    "\n",
    "# =============================================================================\n",
    "# --- model_arch.py content starts here ---\n",
    "# =============================================================================\n",
    "\n",
    "# --- Positional Encoding ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # Shape: (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# --- Image Encoder ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_type=\"resnet50\", pretrained=True, freeze=True):\n",
    "        super().__init__()\n",
    "        self.encoder_type = encoder_type\n",
    "        self.freeze = freeze\n",
    "        self.out_channels = 0 # Will be set after loading backbone\n",
    "\n",
    "        try:\n",
    "            if encoder_type == \"resnet50\":\n",
    "                # Use torch.hub.load to get the pretrained model\n",
    "                self.backbone = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=pretrained)\n",
    "                 # Get out_channels before removing layers\n",
    "                self.out_channels = self.backbone.fc.in_features # Usually 2048 for ResNet50\n",
    "                self.backbone = nn.Sequential(*list(self.backbone.children())[:-2]) # Keep up to layer4 (before avgpool and fc)\n",
    "            elif encoder_type == \"efficientnet_b0\":\n",
    "                 # EfficientNet needs careful handling - check specific model architecture\n",
    "                _effnet = torch.hub.load('pytorch/vision:v0.10.0', 'efficientnet_b0', pretrained=pretrained)\n",
    "                self.backbone = _effnet.features # Keep the feature extractor part\n",
    "                # Determine output channels - for B0, the last block output before pooling/classifier\n",
    "                # This might vary slightly depending on torchvision version. Check model structure.\n",
    "                # Typically the last conv layer in features has 1280 channels for b0\n",
    "                self.out_channels = 1280 # Adjust if necessary by inspecting _effnet.classifier[1].in_features\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown encoder type: {encoder_type}\")\n",
    "\n",
    "            if self.out_channels == 0:\n",
    "                 raise ValueError(f\"Could not determine output channels for encoder type: {encoder_type}\")\n",
    "\n",
    "        except Exception as e:\n",
    "             print(f\"Error loading pretrained model '{encoder_type}': {e}\")\n",
    "             raise\n",
    "\n",
    "        if freeze:\n",
    "            print(f\"Freezing weights for encoder: {encoder_type}\")\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            print(f\"Training weights for encoder: {encoder_type}\")\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        features = self.backbone(img)\n",
    "        # Expected output shape (B, C_out, H_feat, W_feat)\n",
    "        # For ResNet50 pre-avgpool, H_feat = H/32, W_feat = W/32 (e.g., 256 -> 8)\n",
    "        # For EfficientNet features, check output shape, might be H/32, W/32 as well\n",
    "        return features\n",
    "\n",
    "\n",
    "# --- Attentive Feature Fusion ---\n",
    "class AttentiveEncoder(nn.Module):\n",
    "    def __init__(self, encoder_dim, n_layers=1, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        # Simple projection/combination instead of MHA for potential simplicity/efficiency\n",
    "        # You can revert to the MultiheadAttention version if preferred\n",
    "        self.combination = nn.Sequential(\n",
    "            nn.Conv2d(encoder_dim * 2, encoder_dim, kernel_size=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(encoder_dim, encoder_dim, kernel_size=3, padding=1), # Add some local context mixing\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # --- Alternative: Using Attention (Original approach) ---\n",
    "        # self.attention_layers = nn.ModuleList([\n",
    "        #     nn.MultiheadAttention(embed_dim=encoder_dim, num_heads=heads, dropout=dropout, batch_first=True)\n",
    "        #     for _ in range(n_layers)\n",
    "        # ])\n",
    "        # self.norm1 = nn.LayerNorm(encoder_dim)\n",
    "        # self.norm2 = nn.LayerNorm(encoder_dim)\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        # self.combination = nn.Sequential(\n",
    "        #     nn.Conv2d(encoder_dim * 2, encoder_dim, kernel_size=1, padding=0),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, feat1, feat2):\n",
    "        b, c, h, w = feat1.shape\n",
    "        assert c == self.encoder_dim, f\"Input feature dim ({c}) doesn't match encoder_dim ({self.encoder_dim})\"\\\n",
    "\n",
    "        # Simple Concatenation and Combination\n",
    "        combined = torch.cat([feat1, feat2], dim=1) # (B, 2*C, H, W)\n",
    "        final_features = self.combination(combined) # (B, C, H, W)\n",
    "\n",
    "        # --- Alternative: Attention mechanism ---\n",
    "        # feat1_seq = feat1.flatten(2).permute(0, 2, 1) # (B, H*W, C)\n",
    "        # feat2_seq = feat2.flatten(2).permute(0, 2, 1) # (B, H*W, C)\n",
    "        # for attn_layer in self.attention_layers:\n",
    "        #      attn_output1, _ = attn_layer(query=feat1_seq, key=feat2_seq, value=feat2_seq)\n",
    "        #      feat1_seq = self.norm1(feat1_seq + self.dropout(attn_output1))\n",
    "        #      attn_output2, _ = attn_layer(query=feat2_seq, key=feat1_seq, value=feat1_seq)\n",
    "        #      feat2_seq = self.norm2(feat2_seq + self.dropout(attn_output2))\n",
    "        # attended_feat1 = feat1_seq.permute(0, 2, 1).view(b, c, h, w)\n",
    "        # attended_feat2 = feat2_seq.permute(0, 2, 1).view(b, c, h, w)\n",
    "        # combined = torch.cat([attended_feat1, attended_feat2], dim=1)\n",
    "        # final_features = self.combination(combined)\n",
    "\n",
    "        return final_features # Return only the fused features\n",
    "\n",
    "# --- Caption Decoder (Transformer-based) ---\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, vocab_path, embed_dim, encoder_dim_attentive, n_layers=2, heads=8, dropout=0.1, ff_dim=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        try:\n",
    "            with open(vocab_path, 'r') as f:\n",
    "                self.word_vocab = json.load(f)\n",
    "            self.vocab_size = len(self.word_vocab)\n",
    "            self.pad_idx = self.word_vocab.get('<NULL>', 0)\n",
    "            self.start_idx = self.word_vocab.get('<START>', 2)\n",
    "            self.end_idx = self.word_vocab.get('<END>', 3)\n",
    "            print(f\"Decoder loaded vocabulary with size: {self.vocab_size}\")\n",
    "        except FileNotFoundError:\n",
    "             raise FileNotFoundError(f\"Vocabulary file not found at {vocab_path}\")\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.encoder_dim_attentive = encoder_dim_attentive\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embed_dim, padding_idx=self.pad_idx)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "\n",
    "        # Project FUSED Encoder Features to Decoder's Embedding Dimension\n",
    "        self.encoder_to_decoder_proj = nn.Linear(encoder_dim_attentive, embed_dim)\n",
    "\n",
    "        # Using batch_first=True for consistency with DataLoader batch format\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim, nhead=heads, dim_feedforward=ff_dim,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, self.vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # --- Masking for Transformer Decoder ---\n",
    "    # This function generates the causal mask for the target sequence (captions)\n",
    "    def _generate_square_subsequent_mask(self, sz, device):\n",
    "        # Create a mask where True indicates positions that *cannot* be attended to.\n",
    "        # We want to attend to current and previous tokens, but not future ones.\n",
    "        # triu creates an upper triangular matrix (including diagonal)\n",
    "        # We want the *lower* triangular part for causal attention\n",
    "        # So, we create upper triangular and invert it (or create lower directly)\n",
    "        # Let's create lower triangle mask (including diagonal)\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=device) * float('-inf'), diagonal=1)\n",
    "        return mask # Shape: (sz, sz)\n",
    "\n",
    "\n",
    "    def forward(self, attentive_features, captions):\n",
    "        \"\"\" Forward pass for training (teacher forcing). \"\"\"\n",
    "        b, c_enc, h, w = attentive_features.shape\n",
    "        max_len = captions.size(1)\n",
    "\n",
    "        # 1. Project and reshape attentive features to act as memory\n",
    "        # (B, C_enc, H, W) -> (B, H*W, C_enc) -> (B, H*W, C_dec)\n",
    "        memory = attentive_features.flatten(2).permute(0, 2, 1) # (B, H*W, C_enc)\n",
    "        memory = self.encoder_to_decoder_proj(memory) # (B, H*W, embed_dim)\n",
    "\n",
    "        # 2. Embed captions and add positional encoding\n",
    "        # Note: PositionalEncoding expects (seq_len, batch_size, embed_dim) if batch_first=False (default)\n",
    "        # But our TransformerDecoder is batch_first=True.\n",
    "        # Let's keep PE consistent with TransformerDecoder input shape (B, seq_len, embed_dim)\n",
    "        embedded_captions = self.embedding(captions) * math.sqrt(self.embed_dim) # (B, max_len, embed_dim)\n",
    "        pos_encoded_captions = self.pos_encoder(embedded_captions.permute(1, 0, 2)).permute(1, 0, 2) # Apply PE and swap back\n",
    "\n",
    "        # 3. Prepare masks\n",
    "        # Causal mask to prevent attending to future tokens in the target sequence\n",
    "        tgt_mask = self._generate_square_subsequent_mask(max_len, captions.device)\n",
    "\n",
    "        # Padding mask to prevent attending to padding tokens in the target sequence\n",
    "        # True indicates positions that should be ignored (masked)\n",
    "        tgt_padding_mask = (captions == self.pad_idx) # (B, max_len)\n",
    "\n",
    "        # Memory key padding mask: Mask for the encoder output (attentive_features)\n",
    "        # Not needed here as attentive_features are not padded in the sequence dimension (H*W)\n",
    "        memory_key_padding_mask = None\n",
    "\n",
    "        # 4. Pass through Transformer Decoder\n",
    "        # tgt: target sequence (pos_encoded_captions)\n",
    "        # memory: encoder output (projected attentive_features)\n",
    "        # tgt_mask: causal mask for target\n",
    "        # tgt_key_padding_mask: padding mask for target\n",
    "        # memory_key_padding_mask: padding mask for memory (encoder output)\n",
    "        decoder_output = self.transformer_decoder(\n",
    "            tgt=pos_encoded_captions,\n",
    "            memory=memory, # Encoder output as memory\n",
    "            tgt_mask=tgt_mask, # Causal mask\n",
    "            tgt_key_padding_mask=tgt_padding_mask, # Padding mask (if used)\n",
    "            memory_key_padding_mask=memory_key_padding_mask # Memory padding mask (if used)\n",
    "        ) # Output shape: (B, max_len, embed_dim)\n",
    "\n",
    "        # 5. Final prediction layer\n",
    "        predictions = self.fc_out(decoder_output) # (B, max_len, vocab_size)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    @torch.no_grad() # Ensure no gradients are computed during generation\n",
    "    def generate_caption(self, attentive_features, max_length=41, beam_size=1):\n",
    "        \"\"\" Generates captions using greedy search (beam search not implemented). \"\"\"\n",
    "        if beam_size != 1:\n",
    "            raise NotImplementedError(\"Beam search not yet implemented.\")\n",
    "\n",
    "        self.eval() # Set model to evaluation mode\n",
    "        batch_size = attentive_features.size(0)\n",
    "        device = attentive_features.device\n",
    "\n",
    "        # --- Project Attentive Features (Memory) ---\n",
    "        memory = attentive_features.flatten(2).permute(0, 2, 1) # (B, H*W, C_enc)\n",
    "        memory = self.encoder_to_decoder_proj(memory) # (B, H*W, embed_dim)\n",
    "\n",
    "        # --- Initialize Captions ---\n",
    "        # Start with the <START> token for each sequence in the batch\n",
    "        generated_caps = torch.full((batch_size, 1), self.start_idx, dtype=torch.long, device=device)\n",
    "        # Keep track of which sequences have finished (generated <END>)\n",
    "        completed_sequences = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "\n",
    "        # --- Autoregressive Decoding Loop ---\n",
    "        # Generate tokens one by one up to max_length\n",
    "        for t in range(1, max_length): # Start from the second token position (t=1)\n",
    "            current_seq_len = generated_caps.size(1) # Current length of generated sequences\n",
    "\n",
    "            # Embed the currently generated sequence\n",
    "            embedded_captions = self.embedding(generated_caps) * math.sqrt(self.embed_dim) # (B, current_seq_len, embed_dim)\n",
    "\n",
    "            # Add positional encoding\n",
    "            # PE expects (seq_len, batch_size, embed_dim), then swap back\n",
    "            pos_encoded_captions = self.pos_encoder(embedded_captions.permute(1, 0, 2)).permute(1, 0, 2) # (B, current_seq_len, embed_dim)\n",
    "\n",
    "            # Prepare causal mask for the current sequence length\n",
    "            # This mask prevents attending to future tokens within the generated sequence\n",
    "            tgt_mask = self._generate_square_subsequent_mask(current_seq_len, device)\n",
    "\n",
    "            # No target padding mask needed during greedy generation as we generate token by token\n",
    "            tgt_padding_mask = None # (generated_caps == self.pad_idx) # Not needed here\n",
    "\n",
    "            # Memory key padding mask: Not needed for encoder output\n",
    "            memory_key_padding_mask = None\n",
    "\n",
    "            # Pass through Transformer Decoder\n",
    "            # Use the entire generated sequence so far as target\n",
    "            decoder_output = self.transformer_decoder(\n",
    "                tgt=pos_encoded_captions,\n",
    "                memory=memory, # Encoder output as memory\n",
    "                tgt_mask=tgt_mask, # Causal mask\n",
    "                tgt_key_padding_mask=tgt_padding_mask, # Padding mask (if used)\n",
    "                memory_key_padding_mask=memory_key_padding_mask # Memory padding mask (if used)\n",
    "            ) # Output shape: (B, current_seq_len, embed_dim)\n",
    "\n",
    "            # Predict the *next* token based on the last position of the decoder output\n",
    "            predictions = self.fc_out(decoder_output[:, -1, :]) # (B, vocab_size)\n",
    "\n",
    "            # Greedy selection: Choose the token with the highest probability\n",
    "            predicted_idx = predictions.argmax(dim=-1) # (B,)\n",
    "\n",
    "            # Append the predicted token to the generated sequences\n",
    "            # Only append to sequences that haven't finished yet\n",
    "            predicted_idx = predicted_idx.masked_fill(completed_sequences, self.pad_idx) # Mask out predictions for finished sequences\n",
    "            generated_caps = torch.cat([generated_caps, predicted_idx.unsqueeze(1)], dim=1)\n",
    "\n",
    "            # Update completion status: Mark sequences that generated the <END> token\n",
    "            just_completed = (predicted_idx == self.end_idx)\n",
    "            completed_sequences |= just_completed\n",
    "\n",
    "            # Stop decoding if all sequences in the batch have generated the <END> token\n",
    "            if completed_sequences.all():\n",
    "                break\n",
    "\n",
    "        # --- Padding to ensure max_length ---\n",
    "        # If the loop finished before reaching max_length, pad the remaining positions\n",
    "        final_len = generated_caps.size(1)\n",
    "        if final_len < max_length:\n",
    "             padding = torch.full((batch_size, max_length - final_len), self.pad_idx, dtype=torch.long, device=device)\n",
    "             generated_caps = torch.cat([generated_caps, padding], dim=1)\n",
    "        # If somehow generated more than max_length (shouldn't happen with loop limit)\n",
    "        elif final_len > max_length:\n",
    "             generated_caps = generated_caps[:, :max_length]\n",
    "\n",
    "        # --- Post-processing: Set tokens after <END> to <NULL> ---\n",
    "        # This ensures that even if tokens were generated after <END> before the loop broke,\n",
    "        # they are treated as padding.\n",
    "        for i in range(batch_size):\n",
    "            # Find the first occurrence of the <END> token\n",
    "            end_indices = (generated_caps[i] == self.end_idx).nonzero(as_tuple=True)\n",
    "            if end_indices[0].numel() > 0: # If <END> token is found\n",
    "                end_pos = end_indices[0][0] # Get the index of the first <END> token\n",
    "                # Set all tokens after the <END> token to the padding index\n",
    "                generated_caps[i, end_pos + 1:] = self.pad_idx\n",
    "\n",
    "\n",
    "        return generated_caps # Final shape: (B, max_length)\n",
    "\n",
    "\n",
    "# --- Segmentation Head ---\n",
    "# Simple example using ConvTranspose2d for upsampling\n",
    "# Assumes attentive_features are H/32, W/32 relative to input image size\n",
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, target_size=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.target_size = target_size\n",
    "        # Upsample H/32, W/32 -> H/16, W/16 -> H/8, W/8 -> H/4, W/4 -> H/2, W/2 -> H, W\n",
    "        # Adjust intermediate channels as needed\n",
    "        inter_channels = in_channels // 2 # Example: 1024 if in_channels=2048\n",
    "        inter_channels2 = inter_channels // 2 # Example: 512\n",
    "        inter_channels3 = inter_channels2 // 2 # Example: 256\n",
    "        inter_channels4 = inter_channels3 // 2 # Example: 128\n",
    "        inter_channels5 = inter_channels4 // 2 # Example: 64\n",
    "\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(in_channels, inter_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(inter_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(inter_channels, inter_channels, kernel_size=3, padding=1) # Conv after upsample\n",
    "        self.bn1_c = nn.BatchNorm2d(inter_channels)\n",
    "        self.relu1_c = nn.ReLU()\n",
    "\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(inter_channels, inter_channels2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(inter_channels2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(inter_channels2, inter_channels2, kernel_size=3, padding=1)\n",
    "        self.bn2_c = nn.BatchNorm2d(inter_channels2)\n",
    "        self.relu2_c = nn.ReLU()\n",
    "\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(inter_channels2, inter_channels3, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(inter_channels3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(inter_channels3, inter_channels3, kernel_size=3, padding=1)\n",
    "        self.bn3_c = nn.BatchNorm2d(inter_channels3)\n",
    "        self.relu3_c = nn.ReLU()\n",
    "\n",
    "        # Need two more upsampling stages (stride 2) to get from H/8 to H\n",
    "        self.upconv4 = nn.ConvTranspose2d(inter_channels3, inter_channels4, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(inter_channels4)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.upconv5 = nn.ConvTranspose2d(inter_channels4, inter_channels5, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(inter_channels5)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        # Final convolution to get num_classes channels\n",
    "        self.final_conv = nn.Conv2d(inter_channels5, num_classes, kernel_size=1)\n",
    "\n",
    "        # Alternative: Using bilinear interpolation + conv\n",
    "        # self.upsample = nn.Upsample(size=target_size, mode='bilinear', align_corners=False)\n",
    "        # self.final_conv = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Using ConvTranspose\n",
    "        x = self.relu1(self.bn1(self.upconv1(x)))\n",
    "        x = self.relu1_c(self.bn1_c(self.conv1(x)))\n",
    "\n",
    "        x = self.relu2(self.bn2(self.upconv2(x)))\n",
    "        x = self.relu2_c(self.bn2_c(self.conv2(x)))\n",
    "\n",
    "        x = self.relu3(self.bn3(self.upconv3(x)))\n",
    "        x = self.relu3_c(self.bn3_c(self.conv3(x)))\n",
    "\n",
    "        x = self.relu4(self.bn4(self.upconv4(x)))\n",
    "        x = self.relu5(self.bn5(self.upconv5(x)))\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # Ensure output size matches target_size (e.g., 256x256)\n",
    "        # This might be needed if ConvTranspose output padding doesn't align perfectly\n",
    "        # Or if using the interpolation method.\n",
    "        # x = F.interpolate(x, size=self.target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return x # Output shape (B, num_classes, H, W)\n",
    "\n",
    "# --- Main Multi-Task Model ---\n",
    "class ChangeDetectionCaptioningModel(nn.Module):\n",
    "    def __init__(self, args, vocab_path):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        # Image Encoder (Siamese)\n",
    "        self.encoder = Encoder(\n",
    "            encoder_type=args.encoder_type,\n",
    "            pretrained=not args.encoder_load_random,\n",
    "            freeze=args.freeze_encoder\n",
    "        )\n",
    "        encoder_out_channels = self.encoder.out_channels\n",
    "\n",
    "        # Attentive Feature Fusion\n",
    "        self.attentive_encoder = AttentiveEncoder(\n",
    "            encoder_dim=encoder_out_channels,\n",
    "            n_layers=args.attn_layers, # Use separate arg if needed\n",
    "            heads=args.heads,\n",
    "            dropout=args.dropout\n",
    "        )\n",
    "        # The output dim of attentive_encoder is also encoder_out_channels (based on current AttentiveEncoder design)\n",
    "        attentive_feature_dim = encoder_out_channels\n",
    "\n",
    "        # Segmentation Head\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=attentive_feature_dim,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            target_size=(args.image_size, args.image_size) # Pass image size from args\n",
    "        )\n",
    "\n",
    "        # Caption Decoder\n",
    "        self.decoder = DecoderTransformer(\n",
    "            vocab_path=vocab_path,\n",
    "            embed_dim=args.embed_dim,\n",
    "            encoder_dim_attentive=attentive_feature_dim, # Use output dim of attentive fusion\n",
    "            n_layers=args.decoder_layers, # Use separate arg if needed\n",
    "            heads=args.heads,\n",
    "            dropout=args.dropout,\n",
    "            ff_dim=args.ff_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, imgA, imgB, captions=None, mode='train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            imgA (Tensor): Batch of 'before' images (B, C, H, W).\n",
    "            imgB (Tensor): Batch of 'after' images (B, C, H, W).\n",
    "            captions (Tensor, optional): Ground truth captions for training (B, max_len).\n",
    "            mode (str): 'train' or 'eval'. In 'eval', only returns predictions.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "            - 'seg_logits': Logits for segmentation map (B, num_classes, H, W).\n",
    "            - 'caption_logits': Logits for caption tokens (B, max_len, vocab_size) (only in train mode).\n",
    "            - 'generated_captions': Generated caption indices (B, max_len) (only in eval mode).\n",
    "        \"\"\"\n",
    "        # 1. Extract features\n",
    "        featA = self.encoder(imgA) # (B, C_enc, H_feat, W_feat)\n",
    "        featB = self.encoder(imgB) # (B, C_enc, H_feat, W_feat)\n",
    "\n",
    "        # 2. Fuse features attentively\n",
    "        attentive_features = self.attentive_encoder(featA, featB) # (B, C_att, H_feat, W_feat)\n",
    "\n",
    "        # 3. Predict Segmentation Mask\n",
    "        seg_logits = self.segmentation_head(attentive_features) # (B, num_classes, H_out, W_out)\n",
    "\n",
    "        # 4. Predict/Generate Captions\n",
    "        outputs = {'seg_logits': seg_logits}\n",
    "        if mode == 'train':\n",
    "            if captions is None:\n",
    "                 raise ValueError(\"Captions must be provided in training mode.\")\n",
    "            # Pass attentive features and GT captions to decoder\n",
    "            caption_logits = self.decoder(attentive_features, captions) # (B, max_len, vocab_size)\n",
    "            outputs['caption_logits'] = caption_logits\n",
    "        elif mode == 'eval':\n",
    "            # Generate captions using the decoder's generation method\n",
    "            generated_captions = self.decoder.generate_caption(\n",
    "                attentive_features,\n",
    "                max_length=self.args.max_length # Use max_length from args\n",
    "            ) # (B, max_length)\n",
    "            outputs['generated_captions'] = generated_captions\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- preprocessing.py content starts here ---\n",
    "# =============================================================================\n",
    "\n",
    "# --- Special Tokens ---\n",
    "SPECIAL_TOKENS = {\n",
    "    '<NULL>': 0, # Padding\n",
    "    '<UNK>': 1,  # Unknown word\n",
    "    '<START>': 2, # Start of sequence\n",
    "    '<END>': 3,   # End of sequence\n",
    "}\n",
    "\n",
    "# --- Helper Functions (from preprocessing.py) ---\n",
    "def build_vocab(sequences, min_token_count=1):\n",
    "    \"\"\"Builds vocabulary from pre-tokenized sequences.\"\"\"\n",
    "    token_to_count = defaultdict(int)\n",
    "    for img_data in sequences:\n",
    "        for seq_tokens in img_data['tokenized_captions']: # Already includes <START>/<END>\n",
    "             # Skip counting special tokens if they are already added\n",
    "            for token in seq_tokens:\n",
    "                 if token not in SPECIAL_TOKENS: # Only count actual words\n",
    "                    token_to_count[token] += 1\n",
    "\n",
    "    token_to_idx = {}\n",
    "    # Add special tokens first\n",
    "    for token, idx in SPECIAL_TOKENS.items():\n",
    "        token_to_idx[token] = idx\n",
    "\n",
    "    # Add words meeting the frequency threshold, sorted by frequency\n",
    "    sorted_tokens = sorted(token_to_count.items(), key=lambda item: item[1], reverse=True)\n",
    "    current_idx = len(token_to_idx) # Start indexing after special tokens\n",
    "    for token, count in sorted_tokens:\n",
    "        if count >= min_token_count and token not in token_to_idx:\n",
    "            token_to_idx[token] = current_idx\n",
    "            current_idx += 1\n",
    "\n",
    "    return token_to_idx\n",
    "\n",
    "def encode(seq_tokens, token_to_idx, allow_unk=False):\n",
    "    \"\"\"Encodes a sequence of tokens into indices.\"\"\"\n",
    "    seq_idx = []\n",
    "    unk_idx = token_to_idx.get('<UNK>')\n",
    "    for token in seq_tokens:\n",
    "        idx = token_to_idx.get(token, unk_idx)\n",
    "        if idx == unk_idx and not allow_unk:\n",
    "             # If allow_unk is False and token is not in vocab and not <UNK> itself\n",
    "             if token != '<UNK>':\n",
    "                raise KeyError(f'Token \"{token}\" not in vocab and allow_unk=False')\n",
    "             # If token is '<UNK>' and allow_unk is False, it means <UNK> wasn't added to vocab\n",
    "             elif token == '<UNK>' and unk_idx is None:\n",
    "                 raise KeyError(f'Token \"<UNK>\" itself is not in vocab (check threshold) and allow_unk=False')\n",
    "\n",
    "        elif idx is None and token == '<UNK>':\n",
    "             # This case should ideally not happen if <UNK> is always added, but defensive\n",
    "             print(f\"Warning: Encountered '<UNK>' token which is somehow not in the vocabulary.\")\n",
    "             # Decide how to handle - skip? raise error? For now, let's skip it.\n",
    "             continue\n",
    "        elif idx is None: # Should only happen if allow_unk=False and token not found\n",
    "             # Should have been caught by the earlier check, but defensive coding\n",
    "             raise KeyError(f'Unhandled case: Token \"{token}\" not found.')\n",
    "\n",
    "        # If idx is None here, it means token was not in vocab and allow_unk is True,\n",
    "        # and unk_idx is None (meaning <UNK> wasn't added to vocab).\n",
    "        # This shouldn't happen if SPECIAL_TOKENS are always added.\n",
    "        if idx is not None:\n",
    "             seq_idx.append(idx)\n",
    "        else:\n",
    "             # Fallback for unexpected None idx\n",
    "             print(f\"Warning: Skipping token '{token}' with None index.\")\n",
    "\n",
    "\n",
    "    return seq_idx\n",
    "\n",
    "\n",
    "def pad_sequence(sequence, max_length, pad_value=0):\n",
    "    \"\"\"Pads a sequence (list of indices) to a specified max_length.\"\"\"\n",
    "    padding_length = max_length - len(sequence)\n",
    "    if padding_length < 0:\n",
    "        # Truncate if longer than max_length\n",
    "        # print(f\"Warning: Truncating sequence of length {len(sequence)} to max_length {max_length}\") # Reduce print frequency\n",
    "        return sequence[:max_length]\n",
    "    return sequence + [pad_value] * padding_length\n",
    "\n",
    "# --- Main Preprocessing Logic (as a function) ---\n",
    "def run_preprocessing(args):\n",
    "    if args.dataset != 'LEVIR_MCI':\n",
    "        raise ValueError(f\"Dataset '{args.dataset}' not supported by this script.\")\n",
    "\n",
    "    # Use the hardcoded dataset root for input\n",
    "    input_captions_json = os.path.join(DATASET_ROOT, 'LevirCCcaptions.json')\n",
    "    input_image_dir = os.path.join(DATASET_ROOT, 'images') # Base image dir\n",
    "    # Use the hardcoded save directory for output\n",
    "    token_save_dir = os.path.join(SAVE_OUTPUT_DIR, 'tokens')\n",
    "\n",
    "    print(f\"Using LEVIR_MCI dataset from: {DATASET_ROOT}\")\n",
    "    print(f\"Saving processed data to: {SAVE_OUTPUT_DIR}\")\n",
    "\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(SAVE_OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(token_save_dir, exist_ok=True)\n",
    "\n",
    "    print('Loading captions from JSON...')\n",
    "    try:\n",
    "        with open(input_captions_json, 'r') as f:\n",
    "            data = json.load(f)['images']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Caption file not found at {input_captions_json}\")\n",
    "        sys.exit(1)\n",
    "    except KeyError:\n",
    "        print(f\"Error: JSON file {input_captions_json} does not have the expected 'images' key.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print('Processing pre-tokenized captions...')\n",
    "    all_caption_data = []\n",
    "    split_filenames = {'train': [], 'val': [], 'test': []}\n",
    "\n",
    "    for img_info in data:\n",
    "        filename = img_info['filename']\n",
    "        filepath = img_info['filepath'] # train, val, or test\n",
    "        imgid = img_info['imgid']\n",
    "\n",
    "        if filepath not in split_filenames:\n",
    "            print(f\"Warning: Unknown filepath '{filepath}' found for {filename}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        tokenized_captions_with_specials = []\n",
    "        raw_captions = [] # Keep raw just for reference if needed\n",
    "        for sentence in img_info['sentences']:\n",
    "            raw = sentence['raw']\n",
    "            # Use pre-existing tokens and add start/end\n",
    "            tokens = ['<START>'] + sentence['tokens'] + ['<END>']\n",
    "            tokenized_captions_with_specials.append(tokens)\n",
    "            raw_captions.append(raw)\n",
    "\n",
    "        # Check if corresponding image files exist\n",
    "        img_a_path = os.path.join(input_image_dir, filepath, 'A', filename)\n",
    "        img_b_path = os.path.join(input_image_dir, filepath, 'B', filename)\n",
    "        # Also check label if needed for consistent processing (optional here)\n",
    "        # label_path = os.path.join(input_image_dir, filepath, 'label_rgb', filename)\n",
    "\n",
    "        if not os.path.exists(img_a_path) or not os.path.exists(img_b_path):\n",
    "             print(f\"Warning: Image pair not found for {filename} in {filepath}. Skipping caption processing for this entry.\")\n",
    "             continue\n",
    "\n",
    "        all_caption_data.append({\n",
    "            'filename': filename,\n",
    "            'filepath': filepath,\n",
    "            'imgid': imgid,\n",
    "            'tokenized_captions': tokenized_captions_with_specials, # Now using the list with added special tokens\n",
    "            'raw_captions': raw_captions\n",
    "        })\n",
    "        # Store base name without extension in split file list\n",
    "        split_filenames[filepath].append(os.path.splitext(filename)[0])\n",
    "\n",
    "\n",
    "    print(f\"Processed captions for {len(all_caption_data)} images.\")\n",
    "\n",
    "    print('Building vocabulary...')\n",
    "    # Use a slightly higher min_token_count for UNK to be included if needed\n",
    "    word_to_idx = build_vocab(all_caption_data, args.word_count_threshold)\n",
    "    print(f'Vocabulary size: {len(word_to_idx)}')\n",
    "\n",
    "    # Use the hardcoded save directory\n",
    "    output_vocab_json = os.path.join(SAVE_OUTPUT_DIR, 'vocab.json')\n",
    "    print(f'Saving vocabulary to {output_vocab_json}')\n",
    "    with open(output_vocab_json, 'w') as f:\n",
    "        json.dump(word_to_idx, f, indent=4)\n",
    "\n",
    "    print('Encoding and padding captions...')\n",
    "    null_token_value = SPECIAL_TOKENS['<NULL>']\n",
    "    for img_data in all_caption_data:\n",
    "        filename_base = os.path.splitext(img_data['filename'])[0]\n",
    "        token_file_path = os.path.join(token_save_dir, f'{filename_base}.json') # Save as JSON\n",
    "\n",
    "        padded_encoded_captions = []\n",
    "        for tokens in img_data['tokenized_captions']:\n",
    "            # Encode using the built vocabulary\n",
    "            encoded = encode(tokens, word_to_idx, allow_unk=True) # Allow UNK during encoding\n",
    "            # Pad the encoded sequence\n",
    "            padded = pad_sequence(encoded, args.max_length, pad_value=null_token_value)\n",
    "            padded_encoded_captions.append(padded)\n",
    "\n",
    "        # Save the list of padded, encoded captions for this image\n",
    "        with open(token_file_path, 'w') as f:\n",
    "            json.dump(padded_encoded_captions, f)\n",
    "\n",
    "    # Save train/val/test split BASE filenames (without extension)\n",
    "    for split, filenames_base in split_filenames.items():\n",
    "        # Use the hardcoded save directory\n",
    "        split_file_path = os.path.join(SAVE_OUTPUT_DIR, f'{split}.txt')\n",
    "        print(f\"Saving {split} image list to {split_file_path} ({len(filenames_base)} images)\")\n",
    "        with open(split_file_path, 'w') as f:\n",
    "            for fname_base in filenames_base:\n",
    "                 f.write(f\"{fname_base}\\n\")\n",
    "\n",
    "    print('Preprocessing finished.')\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- training.py content starts here ---\n",
    "# =============================================================================\n",
    "\n",
    "# Helper function for mIoU (add this or import from a utils file)\n",
    "def calculate_iou(pred, target, num_classes):\n",
    "    \"\"\"Calculates Intersection over Union (IoU) per class.\"\"\"\n",
    "    ious = []\n",
    "    pred = torch.argmax(pred, dim=1) # Convert logits to class IDs (B, H, W)\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds[target_inds]).long().sum().item()\n",
    "        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection\n",
    "        if union == 0:\n",
    "            # If there is no ground truth or prediction, it's undefined.\n",
    "            # Some implementations return 1.0 (perfect IoU for absent class), others NaN or 0.\n",
    "            # Returning 0 avoids NaN issues, but be aware of the interpretation.\n",
    "            ious.append(0.0)\n",
    "        else:\n",
    "            ious.append(float(intersection) / float(max(union, 1))) # Avoid division by zero\n",
    "    return np.array(ious)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        # ---- Device Setup ----\n",
    "        # Kaggle notebooks typically use CUDA if enabled\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "            self.num_gpus = torch.cuda.device_count()\n",
    "            print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            if self.num_gpus > 1:\n",
    "                print(f\"Detected {self.num_gpus} GPUs. Will use DataParallel.\\n\") # Added newline for clarity\n",
    "            else:\n",
    "                print(\"Only one GPU detected. DataParallel will not be used.\\n\") # Added newline for clarity\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            self.num_gpus = 0\n",
    "            print(\"Using CPU\\n\") # Added newline for clarity\n",
    "\n",
    "\n",
    "        # ---- Create Directories ----\n",
    "        # Use the hardcoded save directory\n",
    "        os.makedirs(SAVE_OUTPUT_DIR, exist_ok=True)\n",
    "        # Create a subdirectory for checkpoints and logs within the main save dir\n",
    "        # Note: Kaggle's /kaggle/working is ephemeral per session unless saved,\n",
    "        # but files saved here are included when you \\\\\\\"Save & Run All\\\\\\\" or commit.\n",
    "        self.run_save_dir = os.path.join(SAVE_OUTPUT_DIR, 'training_output')\n",
    "        os.makedirs(self.run_save_dir, exist_ok=True)\n",
    "        print(f\"Checkpoints and logs will be saved in: {self.run_save_dir}\")\n",
    "\n",
    "        # Define the path to the latest checkpoint\n",
    "        self.latest_checkpoint_path = os.path.join(self.run_save_dir, 'checkpoint_latest.pth')\n",
    "\n",
    "        # ---- Dataset & Dataloaders ----\n",
    "        print(\"Loading Datasets...\")\n",
    "        self.train_dataset = LEVIRCCDataset(\n",
    "            data_folder=DATASET_ROOT, # Use hardcoded dataset root from Kaggle input\n",
    "            processed_data_dir=SAVE_OUTPUT_DIR, # Use hardcoded save directory for processed data (Kaggle working)\n",
    "            split='train',\n",
    "            load_segmentation=True, # Ensure segmentation is loaded\n",
    "            max_length=args.max_length\n",
    "        )\n",
    "        self.val_dataset = LEVIRCCDataset(\n",
    "            data_folder=DATASET_ROOT, # Use hardcoded dataset root from Kaggle input\n",
    "            processed_data_dir=SAVE_OUTPUT_DIR, # Use hardcoded save directory for processed data (Kaggle working)\n",
    "            split='val',\n",
    "            load_segmentation=True, # Ensure segmentation is loaded\n",
    "            max_length=args.max_length\n",
    "        )\n",
    "\n",
    "        # Check if dataset loading was successful\n",
    "        if len(self.train_dataset) == 0 or len(self.val_dataset) == 0:\n",
    "             raise ValueError(\"One or both datasets are empty. Check paths and preprocessing.\")\n",
    "\n",
    "        # --- DataLoader Initialization ---\n",
    "        # num_workers: Adjust this value. Kaggle GPUs are powerful, increase workers\n",
    "        # to keep the GPU busy. Start with a value like 4 or 8, or higher depending\n",
    "        # on the dataset size and complexity of __getitem__.\n",
    "        # pin_memory=True is good for CUDA.\n",
    "        # If using DataParallel, the effective batch size per GPU is batch_size // num_gpus.\n",
    "        # Make sure your batch_size is divisible by the number of GPUs.\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "            num_workers=args.num_workers, pin_memory=True, drop_last=True # drop_last helps if batch norm sensitive\n",
    "        )\n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "            num_workers=args.num_workers, pin_memory=True\n",
    "        )\n",
    "\n",
    "        # ---- Model Initialization ----\n",
    "        print(\"Initializing Model...\")\n",
    "        # Use the hardcoded save directory to find vocab\n",
    "        vocab_path = os.path.join(SAVE_OUTPUT_DIR, 'vocab.json')\n",
    "        # Instantiate the combined model\n",
    "        self.model = ChangeDetectionCaptioningModel(args, vocab_path) # Don't move to device yet\n",
    "\n",
    "        # --- Wrap with DataParallel if multiple GPUs ---\n",
    "        if self.num_gpus > 1:\n",
    "            print(f\"Wrapping model with DataParallel across {self.num_gpus} GPUs.\\n\") # Added newline\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "            # Note: When using DataParallel, model state_dict keys will have 'module.' prefix.\n",
    "            # This needs to be handled during checkpoint loading/saving.\n",
    "\n",
    "        # Move model to device (primary GPU if DataParallel is used)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "\n",
    "        # ---- Optimizer ----\n",
    "        # Optimize all parameters of the combined model\n",
    "        # If using DataParallel, optimizer should optimize parameters of the wrapped model\n",
    "        self.optimizer = optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "            lr=args.learning_rate # Use a single initial LR\n",
    "        )\n",
    "\n",
    "        # ---- Loss Functions ----\n",
    "        # Captioning Loss: Ignore padding index\n",
    "        pad_idx = self.train_dataset.pad_idx\n",
    "        self.caption_criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(self.device)\n",
    "        # Segmentation Loss: Standard CE for multi-class segmentation\n",
    "        # You might explore other losses like DiceLoss or FocalLoss depending on class imbalance\n",
    "        self.segmentation_criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "\n",
    "        # ---- BLEU Score Smoothing ----\n",
    "        self.smooth_fn = SmoothingFunction().method1 # Example smoothing\n",
    "\n",
    "        # ---- Track Best Metrics ----\n",
    "        self.best_bleu = 0.0\n",
    "        self.best_miou = 0.0\n",
    "        self.start_epoch = 0 # Initialize starting epoch\n",
    "\n",
    "        # ---- Resume from Checkpoint (New Logic) ----\n",
    "        if os.path.exists(self.latest_checkpoint_path):\n",
    "            print(f\"Found checkpoint at {self.latest_checkpoint_path}. Resuming training...\")\n",
    "            try:\n",
    "                # Load checkpoint - map_location ensures it loads correctly regardless of device it was saved on\n",
    "                checkpoint = torch.load(self.latest_checkpoint_path, map_location=self.device)\n",
    "\n",
    "                # Load model state\n",
    "                # Handle 'module.' prefix if checkpoint was saved from a DataParallel model\n",
    "                model_state_dict = checkpoint['model_state_dict']\n",
    "                if self.num_gpus > 1 and not list(model_state_dict.keys())[0].startswith('module.'):\n",
    "                    # If current model is DataParallel but checkpoint keys don't have 'module.', add it\n",
    "                    model_state_dict = {'module.' + k: v for k, v in model_state_dict.items()}\n",
    "                elif self.num_gpus == 1 and list(model_state_dict.keys())[0].startswith('module.'):\n",
    "                     # If current model is NOT DataParallel but checkpoint keys have 'module.', remove it\n",
    "                     model_state_dict = {k.replace('module.', ''): v for k, v in model_state_dict.items()}\n",
    "\n",
    "                self.model.load_state_dict(model_state_dict)\n",
    "\n",
    "\n",
    "                # Load optimizer state\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "                # Resume epoch\n",
    "                self.start_epoch = checkpoint['epoch']\n",
    "                print(f\"Resuming from epoch {self.start_epoch + 1}\") # Print next epoch number\n",
    "\n",
    "                # Load best metrics achieved so far\n",
    "                self.best_bleu = checkpoint.get('best_bleu', 0.0) # Use .get() for backward compatibility\n",
    "                self.best_miou = checkpoint.get('best_miou', 0.0) # Use .get() for backward compatibility\n",
    "                print(f\"Loaded best BLEU: {self.best_bleu:.4f}, best mIoU: {self.best_miou:.4f}\")\n",
    "\n",
    "                # Optional: Adjust learning rate if resuming (e.g., if using a scheduler)\n",
    "                # for param_group in self.optimizer.param_groups:\n",
    "                #     param_group['lr'] = args.learning_rate # Reset or adjust LR\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading checkpoint {self.latest_checkpoint_path}: {e}\")\n",
    "                print(\"Starting training from scratch.\")\n",
    "                # Reset start_epoch and best metrics if loading fails\n",
    "                self.start_epoch = 0\n",
    "                self.best_bleu = 0.0\n",
    "                self.best_miou = 0.0\n",
    "        else:\n",
    "            print(\"No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "\n",
    "    def get_word_vocab(self):\n",
    "        # Access vocab from the original module if using DataParallel\n",
    "        if isinstance(self.model, nn.DataParallel):\n",
    "             return self.model.module.decoder.word_vocab # Access decoder vocab via the original module\n",
    "        else:\n",
    "             return self.model.decoder.word_vocab # Access decoder vocab directly\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train() # Set the main model to training mode\n",
    "\n",
    "        total_cap_loss = 0\n",
    "        total_seg_loss = 0\n",
    "        total_combined_loss = 0\n",
    "        num_batches = len(self.train_loader)\n",
    "        # --- Progress Bar with Loss Display ---\n",
    "        # The tqdm progress bar automatically displays the loss values set with .set_postfix()\n",
    "        progress_bar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{self.args.epochs} [Train]\")\n",
    "\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            # Move data to device\n",
    "            # DataParallel automatically moves data to GPUs, but it's good practice\n",
    "            # to ensure it's on the primary device before passing to the model.\n",
    "            # DataParallel expects the input batch to be on the device of the first GPU (cuda:0 by default).\n",
    "            imgA = batch['imgA'].to(self.device)\n",
    "            imgB = batch['imgB'].to(self.device)\n",
    "            captions = batch['token'].to(self.device) # Target captions (B, max_len)\n",
    "            seg_mask_gt = batch['seg_mask'].to(self.device) # Ground truth seg masks (B, H, W)\n",
    "\n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # ---- Forward Pass ----\n",
    "            # When using DataParallel, you pass the batch to the wrapped model.\n",
    "            # DataParallel handles splitting the batch across GPUs and gathering outputs.\n",
    "            outputs = self.model(imgA, imgB, captions, mode='train')\n",
    "            seg_logits = outputs['seg_logits'] # (B, num_classes, H, W)\n",
    "            caption_logits = outputs['caption_logits'] # (B, max_len, vocab_size)\n",
    "\n",
    "            # ---- Loss Calculation ----\n",
    "            # Losses are calculated on the gathered outputs on the primary device.\n",
    "            # 1. Captioning Loss\n",
    "            # Reshape for CrossEntropyLoss: (B, max_len, vocab_size) -> (B * (max_len-1), vocab_size)\n",
    "            # Target: (B, max_len) -> (B * (max_len-1))\n",
    "            # Access vocab_size from the decoder module (handling DataParallel)\n",
    "            decoder_vocab_size = self.model.module.decoder.vocab_size if isinstance(self.model, nn.DataParallel) else self.model.decoder.vocab_size\n",
    "            cap_loss = self.caption_criterion(\n",
    "                caption_logits[:, :-1, :].reshape(-1, decoder_vocab_size), # Predict steps 0 to max_len-2\n",
    "                captions[:, 1:].reshape(-1)             # Target are steps 1 to max_len-1\n",
    "            )\n",
    "\n",
    "            # 2. Segmentation Loss\n",
    "            # seg_logits: (B, num_classes, H, W), seg_mask_gt: (B, H, W) - shapes match criterion needs\n",
    "            # Optional: Resize logits to match GT mask if needed (depends on SegHead output size)\n",
    "            # if seg_logits.shape[-2:] != seg_mask_gt.shape[-2:]:\\\n",
    "            #     seg_logits = F.interpolate(seg_logits, size=seg_mask_gt.shape[-2:], mode='bilinear', align_corners=False)\\\n",
    "            seg_loss = self.segmentation_criterion(seg_logits, seg_mask_gt)\n",
    "\n",
    "            # 3. Combine Losses\n",
    "            combined_loss = cap_loss + self.args.seg_loss_weight * seg_loss\n",
    "\n",
    "            # ---- Backward Pass & Optimization ----\n",
    "            # DataParallel handles gradient averaging automatically\n",
    "            combined_loss.backward()\n",
    "\n",
    "            # Gradient Clipping (optional but recommended)\n",
    "            if self.args.grad_clip > 0:\n",
    "                # Apply gradient clipping to the parameters of the original module if using DataParallel\n",
    "                if isinstance(self.model, nn.DataParallel):\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.module.parameters(), self.args.grad_clip)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.grad_clip)\n",
    "\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # --- Accumulate and Display Loss ---\n",
    "            total_cap_loss += cap_loss.item()\n",
    "            total_seg_loss += seg_loss.item()\n",
    "            total_combined_loss += combined_loss.item()\n",
    "            # Update the progress bar postfix with current batch losses\n",
    "            progress_bar.set_postfix({\n",
    "                'CapL': f'{cap_loss.item():.3f}',\n",
    "                'SegL': f'{seg_loss.item():.3f}',\n",
    "                'CombL': f'{combined_loss.item():.3f}'\n",
    "             })\n",
    "\n",
    "        # --- Print Average Loss at End of Epoch ---\n",
    "        avg_cap_loss = total_cap_loss / num_batches\n",
    "        avg_seg_loss = total_seg_loss / num_batches\n",
    "        avg_combined_loss = total_combined_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1} Train Avg Loss -> Combined: {avg_combined_loss:.4f} (Cap: {avg_cap_loss:.4f}, Seg: {avg_seg_loss:.4f})\\n\") # Added newline\n",
    "        return avg_combined_loss\n",
    "\n",
    "\n",
    "    def validate_epoch(self, epoch):\n",
    "        self.model.eval() # Set the main model to evaluation mode\n",
    "\n",
    "        references_corpus = [] # List of lists of reference tokens for BLEU\n",
    "        hypotheses_corpus = [] # List of hypothesis tokens for BLEU\n",
    "        total_iou = np.zeros(NUM_CLASSES)\n",
    "        num_val_batches = 0\n",
    "\n",
    "        # Get inverse vocabulary mapping (index to word) and special indices from the VAL dataset\n",
    "        # These are correctly stored in the dataset object.\n",
    "        idx_to_word = self.val_dataset.idx_to_word\n",
    "        # Access word_vocab from the val_dataset as well\n",
    "        special_indices = {idx for token, idx in self.val_dataset.word_vocab.items() if token in ['<NULL>', '<START>', '<END>']}\n",
    "\n",
    "\n",
    "        progress_bar = tqdm(self.val_loader, desc=f\"Epoch {epoch+1}/{self.args.epochs} [Validate]\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(progress_bar):\n",
    "                # Move data to device (primary GPU if DataParallel)\n",
    "                imgA = batch['imgA'].to(self.device)\n",
    "                imgB = batch['imgB'].to(self.device)\n",
    "                all_ref_captions = batch['token_all'].to(self.device) # Move references to device for consistency\n",
    "                seg_mask_gt = batch['seg_mask'].to(self.device) # (B, H, W)\n",
    "\n",
    "                # ---- Forward Pass (Evaluation Mode) ----\n",
    "                # DataParallel handles splitting and gathering\n",
    "                outputs = self.model(imgA, imgB, mode='eval')\n",
    "                seg_logits = outputs['seg_logits'] # (B, num_classes, H, W)\n",
    "                generated_indices = outputs['generated_captions'] # (B, max_len)\n",
    "\n",
    "                # ---- Calculate mIoU ----\n",
    "                # Optional: Resize logits if needed\n",
    "                # if seg_logits.shape[-2:] != seg_mask_gt.shape[-2:]:\\\n",
    "                #     seg_logits = F.interpolate(seg_logits, size=seg_mask_gt.shape[-2:], mode='bilinear', align_corners=False)\\\n",
    "                batch_iou = calculate_iou(seg_logits.cpu(), seg_mask_gt.cpu(), NUM_CLASSES)\n",
    "                total_iou += batch_iou\n",
    "                num_val_batches += 1\n",
    "\n",
    "                # ---- Prepare for BLEU Score ----\n",
    "                # Convert indices to words, removing special tokens\n",
    "\n",
    "                # Generated hypotheses (move to CPU before converting to numpy)\n",
    "                generated_indices_np = generated_indices.cpu().numpy()\n",
    "                current_batch_hypotheses = []\n",
    "                for single_hyp_indices in generated_indices_np:\n",
    "                    hyp_words = [idx_to_word.get(idx, '<UNK>') for idx in single_hyp_indices if idx not in special_indices]\n",
    "                    current_batch_hypotheses.append(hyp_words)\n",
    "\n",
    "                # Ground truth references (move to CPU before converting to numpy)\n",
    "                all_ref_captions_np = all_ref_captions.cpu().numpy()\n",
    "                current_batch_references_for_bleu = [] # List of lists of lists of strings\n",
    "                current_batch_hypotheses_for_bleu = [] # List of lists of strings\n",
    "\n",
    "                for batch_idx in range(all_ref_captions_np.shape[0]): # Iterate through batch items\n",
    "                    item_references = [] # List of lists of strings for this item\n",
    "                    has_valid_reference = False\n",
    "                    for ref_idx in range(all_ref_captions_np.shape[1]): # Iterate through references for the item\n",
    "                        ref_indices = all_ref_captions_np[batch_idx, ref_idx, :]\n",
    "                        ref_words = [idx_to_word.get(idx, '<UNK>') for idx in ref_indices if idx not in special_indices]\n",
    "                        if ref_words: # Only add if the reference is not empty after removing special tokens\n",
    "                            item_references.append(ref_words)\n",
    "                            has_valid_reference = True\n",
    "\n",
    "                    if has_valid_reference: # Only add to corpus if there is at least one valid reference\n",
    "                        current_batch_references_for_bleu.append(item_references)\n",
    "                        current_batch_hypotheses_for_bleu.append(current_batch_hypotheses[batch_idx]) # Add corresponding hypothesis\n",
    "\n",
    "                # Append collected valid references and their corresponding hypotheses to corpus lists\n",
    "                references_corpus.extend(current_batch_references_for_bleu)\n",
    "                hypotheses_corpus.extend(current_batch_hypotheses_for_bleu)\n",
    "\n",
    "\n",
    "        # ---- Calculate Final Metrics ----\n",
    "        # mIoU\n",
    "        mean_iou = np.mean(total_iou / num_val_batches) if num_val_batches > 0 else 0.0\n",
    "\n",
    "        # BLEU Score\n",
    "        bleu_score = 0.0\n",
    "        if not references_corpus or not hypotheses_corpus:\n",
    "             print(\"Warning: No valid references or hypotheses collected for BLEU score calculation.\")\n",
    "        elif len(references_corpus) != len(hypotheses_corpus):\n",
    "              print(f\"Warning: Mismatch in reference ({len(references_corpus)}) and hypothesis ({len(hypotheses_corpus)}) counts for BLEU. BLEU score calculation might be inaccurate.\")\n",
    "              # Avoid calculating BLEU if lengths mismatch significantly, indicates an issue\n",
    "        else:\n",
    "            try:\n",
    "                 # corpus_bleu expects references as list of list of token strings, hypotheses as list of token strings\n",
    "                 # references_corpus is already list of list of list of token strings\n",
    "                 # hypotheses_corpus is list of list of token strings\n",
    "                 bleu_score = corpus_bleu(\n",
    "                     references_corpus,\n",
    "                     hypotheses_corpus,\n",
    "                     smoothing_function=self.smooth_fn\n",
    "                 )\n",
    "            except Exception as e:\n",
    "                 print(f\"Error calculating BLEU score: {e}\")\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Validation -> BLEU-4: {bleu_score:.4f}, mIoU: {mean_iou:.4f}\\n\") # Added newline\n",
    "        return bleu_score, mean_iou\n",
    "\n",
    "\n",
    "    def run_training(self):\n",
    "        print(\"Starting Training...\\n\") # Added newline\n",
    "\n",
    "        # Start the training loop from the loaded epoch or 0\n",
    "        for epoch in range(self.start_epoch, self.args.epochs):\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            val_bleu, val_miou = self.validate_epoch(epoch)\n",
    "\n",
    "            # ---- Save Checkpoint ----\n",
    "            # Decide best based on primary metric (e.g., BLEU) or a combination\n",
    "            is_best_bleu = val_bleu > self.best_bleu\n",
    "            if is_best_bleu:\n",
    "                self.best_bleu = val_bleu\n",
    "                print(f\"*** New best BLEU score: {self.best_bleu:.4f} ***\\n\") # Added newline\n",
    "\n",
    "            is_best_miou = val_miou > self.best_miou\n",
    "            if is_best_miou:\n",
    "                 self.best_miou = val_miou\n",
    "                 print(f\"*** New best mIoU score: {self.best_miou:.4f} ***\\n\") # Added newline\n",
    "\n",
    "\n",
    "            # Save checkpoint data\n",
    "            checkpoint_data = {\n",
    "                'epoch': epoch + 1, # Save the epoch *after* it completes\n",
    "                'args': self.args, # Save args for reproducibility\n",
    "                'best_bleu': self.best_bleu,\n",
    "                'best_miou': self.best_miou,\n",
    "                # Save state_dict of the original module if using DataParallel\n",
    "                'model_state_dict': self.model.module.state_dict() if isinstance(self.model, nn.DataParallel) else self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            }\n",
    "\n",
    "            # Use the run-specific save directory\n",
    "            checkpoint_path = os.path.join(self.run_save_dir, 'checkpoint_latest.pth')\n",
    "            torch.save(checkpoint_data, checkpoint_path)\n",
    "            # print(f\"Saved latest checkpoint to {checkpoint_path}\") # Reduce print frequency\n",
    "\n",
    "            # Use the run-specific save directory\n",
    "            if is_best_bleu:\n",
    "                best_checkpoint_path = os.path.join(self.run_save_dir, 'checkpoint_best_bleu.pth')\n",
    "                torch.save(checkpoint_data, best_checkpoint_path)\n",
    "                print(f\"Saved best BLEU checkpoint to {best_checkpoint_path}\\n\") # Added newline\n",
    "            # Optionally save best mIoU checkpoint separately\n",
    "            # if is_best_miou:\n",
    "            #     best_miou_checkpoint_path = os.path.join(self.run_save_dir, 'checkpoint_best_miou.pth')\n",
    "            #     torch.save(checkpoint_data, best_miou_checkpoint_path)\\\n",
    "            #     print(f\"Saved best mIoU checkpoint to {best_miou_checkpoint_path}\")\\\n",
    "\n",
    "\n",
    "        print(\"Training finished.\\n\") # Added newline\n",
    "        print(f\"Best Validation BLEU-4 achieved: {self.best_bleu:.4f}\")\n",
    "        print(f\"Best Validation mIoU achieved: {self.best_miou:.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- Main Execution Block ---\n",
    "# This block will run when you execute the notebook cell.\n",
    "# It parses arguments (or uses defaults) and starts preprocessing/training.\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train the Multi-Task Change Detection and Captioning Model')\n",
    "\n",
    "    # --- Paths (Hardcoded at the top, but keeping args for potential override) ---\n",
    "    # parser.add_argument('--data_folder', type=str, default=DATASET_ROOT, help='Path to root LEVIR-MCI dataset folder')\n",
    "    # parser.add_argument('--processed_data_dir', type=str, default=SAVE_OUTPUT_DIR, help='Path to folder with preprocessed data')\n",
    "    # parser.add_argument('--save_dir', type=str, default=SAVE_OUTPUT_DIR, help='Directory to save checkpoints and logs')\n",
    "\n",
    "    # --- Dataset ---\n",
    "    parser.add_argument('--dataset', type=str, default='LEVIR_MCI', help='The name of the dataset')\n",
    "    parser.add_argument('--max_length', type=int, default=41, help='Maximum caption length used during preprocessing')\n",
    "    parser.add_argument('--image_size', type=int, default=256, help='Input image size (H and W)')\n",
    "    parser.add_argument('--word_count_threshold', default=5, type=int, help='Minimum word count to include in vocabulary (for preprocessing)')\n",
    "\n",
    "\n",
    "    # --- Model Architecture ---\n",
    "    parser.add_argument('--encoder_type', type=str, default='resnet50', choices=['resnet50', 'efficientnet_b0'], help='Image encoder backbone')\n",
    "    parser.add_argument('--encoder_load_random', action='store_true', help='Initialize encoder with random weights')\n",
    "    parser.add_argument('--freeze_encoder', action='store_true', help='Freeze weights of the image encoder backbone')\n",
    "    parser.add_argument('--embed_dim', type=int, default=512, help='Embedding dimension for decoder')\n",
    "    parser.add_argument('--ff_dim', type=int, default=2048, help='Feed-forward dimension in Transformer layers')\n",
    "    parser.add_argument('--attn_layers', type=int, default=1, help='Number of layers in attentive fusion (if using MHA version)')\n",
    "    parser.add_argument('--decoder_layers', type=int, default=2, help='Number of layers in transformer decoder')\n",
    "    parser.add_argument('--heads', type=int, default=8, help='Number of attention heads')\n",
    "    parser.add_argument('--dropout', type=float, default=0.1, help='Dropout probability')\n",
    "\n",
    "    # --- Training ---\n",
    "    parser.add_argument('--epochs', type=int, default=30, help='Number of training epochs')\n",
    "    # batch_size: Adjust this value based on GPU memory. Start lower if needed.\n",
    "    # IMPORTANT: For DataParallel, batch_size should be divisible by the number of GPUs.\n",
    "    parser.add_argument('--batch_size', type=int, default=16, help='Training batch size (reduce if OOM). Should be divisible by num_gpus.')\n",
    "    # num_workers: Adjust based on Kaggle's CPU cores and dataset loading speed.\n",
    "    # Start with 4 or 8 and experiment.\n",
    "    parser.add_argument('--num_workers', type=int, default=4, help='Number of dataloader workers (adjust based on system)')\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate for the optimizer')\n",
    "    parser.add_argument('--grad_clip', type=float, default=5.0, help='Gradient clipping value (0 for no clipping)')\n",
    "    parser.add_argument('--seg_loss_weight', type=float, default=1.0, help='Weight for the segmentation loss term')\n",
    "\n",
    "    # --- Execution Mode ---\n",
    "    parser.add_argument('--run_mode', type=str, default='train', choices=['preprocess', 'train'],\n",
    "                        help='Which part of the pipeline to run: preprocess or train.')\n",
    "\n",
    "    # In a notebook, you might pass arguments like this for preprocessing:\n",
    "    # args = parser.parse_args(['--run_mode', 'preprocess', '--word_count_threshold', '5', '--max_length', '41'])\\\n",
    "    # And like this for training:\n",
    "    # args = parser.parse_args(['--run_mode', 'train', '--batch_size', '32', '--epochs', '50'])\n",
    "\n",
    "    # For running in a single cell, you can set the mode directly or use a simple list\n",
    "    # For the first run (preprocessing):\n",
    "    # args = parser.parse_args(['--run_mode', 'preprocess'])\n",
    "    # For the second run (training):\n",
    "    args = parser.parse_args([]) # <-- Set this to 'train' for training run\n",
    "\n",
    "    print(f\"Running in mode: {args.run_mode}\")\n",
    "\n",
    "    if args.run_mode == 'preprocess':\n",
    "        # Run preprocessing\n",
    "        run_preprocessing(args)\n",
    "    elif args.run_mode == 'train':\n",
    "        # Run training\n",
    "        # Ensure preprocessing has been run\n",
    "        if not os.path.exists(os.path.join(SAVE_OUTPUT_DIR, 'vocab.json')) or \\\n",
    "           not os.path.exists(os.path.join(SAVE_OUTPUT_DIR, 'train.txt')) or \\\n",
    "           not os.path.exists(os.path.join(SAVE_OUTPUT_DIR, 'tokens')):\n",
    "            print(\"Preprocessing output not found. Running preprocessing first.\")\n",
    "            run_preprocessing(args)\n",
    "\n",
    "        trainer = Trainer(args)\n",
    "        trainer.run_training()\n",
    "    else:\n",
    "        print(f\"Unknown run_mode: {args.run_mode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7240493,
     "sourceId": 11545714,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
