{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T10:26:56.582718Z",
     "iopub.status.busy": "2025-05-01T10:26:56.582365Z",
     "iopub.status.idle": "2025-05-01T10:28:23.119790Z",
     "shell.execute_reply": "2025-05-01T10:28:23.119062Z",
     "shell.execute_reply.started": "2025-05-01T10:26:56.582690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T10:28:23.121398Z",
     "iopub.status.busy": "2025-05-01T10:28:23.121157Z",
     "iopub.status.idle": "2025-05-01T10:28:26.104474Z",
     "shell.execute_reply": "2025-05-01T10:28:26.103787Z",
     "shell.execute_reply.started": "2025-05-01T10:28:23.121377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T10:28:26.105669Z",
     "iopub.status.busy": "2025-05-01T10:28:26.105386Z",
     "iopub.status.idle": "2025-05-01T10:28:26.550722Z",
     "shell.execute_reply": "2025-05-01T10:28:26.549985Z",
     "shell.execute_reply.started": "2025-05-01T10:28:26.105642Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T10:28:34.509657Z",
     "iopub.status.busy": "2025-05-01T10:28:34.509055Z",
     "iopub.status.idle": "2025-05-01T10:28:34.513558Z",
     "shell.execute_reply": "2025-05-01T10:28:34.512785Z",
     "shell.execute_reply.started": "2025-05-01T10:28:34.509626Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T10:28:34.913833Z",
     "iopub.status.busy": "2025-05-01T10:28:34.913277Z",
     "iopub.status.idle": "2025-05-01T10:28:40.027199Z",
     "shell.execute_reply": "2025-05-01T10:28:40.026632Z",
     "shell.execute_reply.started": "2025-05-01T10:28:34.913810Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from transformers import utils as hf_utils\n",
    "\n",
    "# Filter the specific warning from PaliGemmaProcessor\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"You are passing both `text` and `images` to `PaliGemmaProcessor`.*\",\n",
    "    category=UserWarning,\n",
    "    module='transformers.models.paligemma.processing_paligemma'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-05-01T10:51:46.733Z",
     "iopub.execute_input": "2025-05-01T10:28:40.029076Z",
     "iopub.status.busy": "2025-05-01T10:28:40.028748Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Combined Script for VLM-based Change Detection VQA\n",
    "# Integrates PaliGemma into the existing pipeline, replacing custom\n",
    "# encoder/decoder. Uses Hugging Face Transformers and Accelerate.\n",
    "# Includes segmentation and VQA/captioning tasks.\n",
    "#\n",
    "# Addresses OutOfMemoryError by suggesting/implementing memory-saving techniques:\n",
    "# - Gradient Accumulation (via args.gradient_accumulation_steps)\n",
    "# - Quantization (via args.quantization)\n",
    "# - Mixed Precision (via args.mixed_precision)\n",
    "# - Gradient Checkpointing (New option)\n",
    "# - Reducing Batch Size (User adjustable via args.batch_size_per_gpu)\n",
    "# - Reducing Image Size (User adjustable via args.image_size)\n",
    "#\n",
    "# Fixes:\n",
    "# - RuntimeError due to shape mismatch in VLM forward pass when handling labels/padding.\n",
    "# - Addressed OutOfMemoryError by setting more memory-friendly default arguments and\n",
    "#   highlighting key arguments for memory management.\n",
    "# - FIXED IndexError: Correctly prepare input_ids and labels for VLM by encoding\n",
    "#   the full prompt+caption sequence and masking labels appropriately.\n",
    "# - FIXED AttributeError: Moved collate_fn inside the Trainer class.\n",
    "# - FIXED SyntaxError: Removed misplaced backslash in dataset __init__.\n",
    "# - FIXED NameError: Added missing SegmentationHead class definition.\n",
    "# - FIXED ValueError: Ensured PaliGemmaProcessor is called with images argument\n",
    "#   even when processing prompt only in __getitem__.\n",
    "# - FIXED AttributeError: Corrected `parser.add_clip` to `parser.add_argument`\n",
    "#   for the `--grad_clip` argument.\n",
    "# - FIXED TypeError: Added the missing argument name ('--grad_clip') to `parser.add_argument`.\n",
    "# - FIXED Warning: Added explicit '<image>' token to the text input for PaliGemmaProcessor.\n",
    "# - FIXED NameError: Added the definition for the `ChangeDetectionVLM` class.\n",
    "# - FIXED TypeError: Enabled gradient checkpointing using the model's method after loading.\n",
    "# - **FIXED RuntimeError:** Corrected the input channel size for the SegmentationHead.\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from PIL import Image # Use PIL for image loading compatible with transformers processor\n",
    "# from imageio.v2 import imread # Keep if needed, but PIL is standard for HF\n",
    "from random import randint\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from tqdm.auto import tqdm # Use tqdm.auto for notebook compatibility\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs # For multi-GPU, mixed precision\n",
    "from accelerate.utils import set_seed\n",
    "from transformers import PaliGemmaForConditionalGeneration, PaliGemmaProcessor, BitsAndBytesConfig # Import VLM\n",
    "import bitsandbytes # Required for quantization\n",
    "from transformers.utils import is_accelerate_available # Check if accelerate is installed\n",
    "\n",
    "# =============================================================================\n",
    "# --- Hardcoded Paths for Kaggle ---\n",
    "# Update DATASET_ROOT to the correct path of your dataset input.\n",
    "# =============================================================================\n",
    "# Make sure your dataset is added to the Kaggle notebook input\n",
    "# Example: '/kaggle/input/levir-mci-dataset/LEVIR-MCI-dataset'\n",
    "DATASET_ROOT = '/kaggle/input/levir-mci-dataset/LEVIR-MCI-dataset' # <-- *** UPDATE THIS PATH ***\n",
    "SAVE_OUTPUT_DIR = '/kaggle/working/' # Standard Kaggle writable output directory\n",
    "\n",
    "# =============================================================================\n",
    "# --- Constants ---\n",
    "# =============================================================================\n",
    "# Define the mapping from RGB colors to class IDs\n",
    "# Background: 0 (black), Road: 1 (grey), Building: 2 (white)\n",
    "COLOR_TO_ID_MAPPING = {\n",
    "    (0, 0, 0): 0,          # Background\n",
    "    (128, 128, 128): 1,    # Road (Grey)\n",
    "    (255, 255, 255): 2,    # Building (White)\n",
    "}\n",
    "NUM_CLASSES = 3 # Background, Road, Building\n",
    "DEFAULT_VLM = \"google/paligemma-3b-pt-224\" # Default PaliGemma model\n",
    "\n",
    "# =============================================================================\n",
    "# --- Utility Functions ---\n",
    "# =============================================================================\n",
    "\n",
    "def rgb_to_class_id_mask(rgb_mask_pil):\n",
    "    \"\"\"Converts an RGB mask (PIL Image) to a class ID mask (H, W).\"\"\"\n",
    "    rgb_mask_np = np.array(rgb_mask_pil, dtype=np.uint8)\n",
    "    h, w, c = rgb_mask_np.shape\n",
    "    if c != 3:\n",
    "        raise ValueError(f\"Input mask must have 3 channels (RGB), but got {c}\")\n",
    "\n",
    "    class_id_mask = np.full((h, w), 0, dtype=np.int64) # Default to background\n",
    "    for color, class_id in COLOR_TO_ID_MAPPING.items():\n",
    "        matches = np.all(rgb_mask_np == np.array(color, dtype=rgb_mask_np.dtype), axis=-1)\n",
    "        class_id_mask[matches] = class_id\n",
    "    return class_id_mask\n",
    "\n",
    "# =============================================================================\n",
    "# --- dataset.py content adapted for VLM ---\n",
    "# =============================================================================\n",
    "\n",
    "class LEVIRCCDataset(Dataset):\n",
    "    def __init__(self, data_folder, processed_data_dir, split, processor,\n",
    "                 load_segmentation=True,\n",
    "                 max_length=64, # Increased default max length for prompt + caption\n",
    "                 vqa_prompt=\"Describe the changes between the two images.\", # Default prompt\n",
    "                 max_iters=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_folder (str): Path to the root LEVIR-MCI dataset folder.\n",
    "            processed_data_dir (str): Path to the folder with splits (train.txt etc.).\n",
    "            split (str): 'train', 'val', or 'test'.\n",
    "            processor (PaliGemmaProcessor): The processor for the VLM.\n",
    "            load_segmentation (bool): If True, loads segmentation maps.\n",
    "            max_length (int): Max sequence length for VLM tokenizer (prompt + caption).\n",
    "            vqa_prompt (str): Prompt used when framing captioning as VQA.\n",
    "            max_iters (int, optional): Repeats dataset for this many items per epoch.\n",
    "        \"\"\"\n",
    "        self.data_folder = data_folder\n",
    "        self.processed_data_dir = processed_data_dir\n",
    "        self.split = split\n",
    "        self.processor = processor\n",
    "        self.load_segmentation = load_segmentation\n",
    "        self.max_length = max_length\n",
    "        self.vqa_prompt = vqa_prompt\n",
    "        self.max_iters = max_iters\n",
    "\n",
    "        assert self.split in {'train', 'val', 'test'}\n",
    "\n",
    "        # --- Load Image Filenames/Base Names ---\n",
    "        split_file_path = os.path.join(self.processed_data_dir, f'{split}.txt')\n",
    "        try:\n",
    "            with open(split_file_path, 'r') as f:\n",
    "                self.img_ids = [line.strip() for line in f if line.strip()]\n",
    "        except FileNotFoundError:\n",
    "             raise FileNotFoundError(f\"Split file not found at {split_file_path}. Run preprocessing first.\")\n",
    "\n",
    "        if not self.img_ids:\n",
    "            raise ValueError(f\"No image IDs found in split file: {split_file_path}\")\n",
    "\n",
    "        # --- Load Captions from original JSON ---\n",
    "        # We need the raw captions to tokenize them with the VLM processor\n",
    "        captions_json_path = os.path.join(DATASET_ROOT, 'LevirCCcaptions.json')\n",
    "        self.captions_data = {}\n",
    "        try:\n",
    "            with open(captions_json_path, 'r') as f:\n",
    "                raw_data = json.load(f)['images']\n",
    "            for img_info in raw_data:\n",
    "                base_name = os.path.splitext(img_info['filename'])[0]\n",
    "                # Store list of raw sentences for each image ID\n",
    "                self.captions_data[base_name] = [s['raw'] for s in img_info['sentences']]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Caption file {captions_json_path} not found. Cannot load ground truth captions.\")\n",
    "            self.captions_data = {} # Ensure it's initialized\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Caption file {captions_json_path} has unexpected format. Cannot load ground truth captions.\")\n",
    "            self.captions_data = {}\n",
    "\n",
    "        # ---- Prepare file paths ----\n",
    "        self.files = []\n",
    "        image_base_path = os.path.join(self.data_folder, 'images', self.split)\n",
    "        label_folder_name = 'label' # Assuming label folder name is 'label'\n",
    "\n",
    "        missing_files_count = 0\n",
    "        for img_base_name in self.img_ids:\n",
    "            img_file_name = f\"{img_base_name}.png\"\n",
    "\n",
    "            file_paths = {\n",
    "                \"name\": img_base_name,\n",
    "                \"imgA\": os.path.join(image_base_path, 'A', img_file_name),\n",
    "                \"imgB\": os.path.join(image_base_path, 'B', img_file_name),\n",
    "            }\n",
    "            seg_path = None\n",
    "            if self.load_segmentation:\n",
    "                seg_path = os.path.join(image_base_path, label_folder_name, img_file_name)\n",
    "                file_paths[\"seg_label\"] = seg_path\n",
    "\n",
    "            # Check if image files exist\n",
    "            paths_to_check = [file_paths[\"imgA\"], file_paths[\"imgB\"]]\n",
    "            if self.load_segmentation:\n",
    "                 paths_to_check.append(seg_path)\n",
    "\n",
    "            files_exist = all(os.path.exists(p) for p in paths_to_check if p is not None)\n",
    "\n",
    "            # Check if captions exist for this image\n",
    "            captions_exist = img_base_name in self.captions_data and self.captions_data[img_base_name]\n",
    "\n",
    "            if not files_exist or not captions_exist:\n",
    "                if not files_exist:\n",
    "                    # print(f\"Debug: Missing image/label file for {img_base_name}\") # Keep logging minimal\n",
    "                    pass\n",
    "                if not captions_exist:\n",
    "                     # print(f\"Debug: Missing captions for {img_base_name}\") # Keep logging minimal\n",
    "                     pass\n",
    "                missing_files_count += 1\n",
    "                continue\n",
    "\n",
    "            self.files.append(file_paths)\n",
    "\n",
    "        if missing_files_count > 0:\n",
    "            print(f\"Warning: Skipped {missing_files_count} entries due to missing files or captions in split '{self.split}'.\")\n",
    "\n",
    "        if not self.files:\n",
    "             raise ValueError(f\"No valid file sets found for split '{self.split}'. Check paths and preprocessing output.\")\n",
    "\n",
    "        # --- Handle max_iters ---\n",
    "        if max_iters is not None and max_iters > 0:\n",
    "            n_repeat = int(np.ceil(max_iters / len(self.files)))\n",
    "            self.files = self.files * n_repeat\n",
    "\n",
    "        print(f\"Initialized LEVIRCCDataset for split '{self.split}' with {len(self.files)} items.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.max_iters is not None and self.max_iters > 0:\n",
    "            return self.max_iters\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        actual_index = index % len(self.files)\n",
    "        datafiles = self.files[actual_index]\n",
    "        img_base_name = datafiles['name']\n",
    "\n",
    "        try:\n",
    "            # Load images using PIL\n",
    "            imgA_pil = Image.open(datafiles[\"imgA\"]).convert(\"RGB\")\n",
    "            imgB_pil = Image.open(datafiles[\"imgB\"]).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error loading images for {datafiles['name']}: {e}\")\n",
    "             raise\n",
    "\n",
    "        # --- Load and Process Segmentation ---\n",
    "        seg_mask_class_ids = None\n",
    "        if self.load_segmentation:\n",
    "            try:\n",
    "                seg_label_pil = Image.open(datafiles[\"seg_label\"])\n",
    "                # Ensure it's RGB before converting\n",
    "                if seg_label_pil.mode != 'RGB':\n",
    "                     seg_label_pil = seg_label_pil.convert('RGB')\n",
    "                seg_mask_class_ids_np = rgb_to_class_id_mask(seg_label_pil)\n",
    "                seg_mask_class_ids = torch.from_numpy(seg_mask_class_ids_np).long()\n",
    "            except Exception as e:\n",
    "                 print(f\"Error loading/processing segmentation label for {datafiles['name']}: {e}\")\n",
    "                 # Return a dummy mask or raise error? Let's return dummy for now.\n",
    "                 # Get image size from loaded image A\n",
    "                 width, height = imgA_pil.size\n",
    "                 seg_mask_class_ids = torch.zeros((height, width), dtype=torch.long)\n",
    "\n",
    "\n",
    "        # --- Select one caption and prepare VLM inputs ---\n",
    "        # Choose a random caption for this image\n",
    "        available_captions = self.captions_data.get(img_base_name, [])\n",
    "        if not available_captions:\n",
    "             # Handle case where captions might be missing despite earlier check\n",
    "             chosen_caption = \"No changes detected.\" # Default caption\n",
    "             # print(f\"Warning: No captions found for {img_base_name} at getitem, using default.\") # Keep logging minimal\n",
    "        else:\n",
    "             chosen_caption = available_captions[randint(0, len(available_captions) - 1)]\n",
    "\n",
    "        # --- Prepare VLM inputs (image + text) ---\n",
    "        # The VLM expects the input sequence to be <image> + text\n",
    "        # For our change detection task, we're using image B as the primary VLM input\n",
    "        # The target sequence for the VLM is the prompt followed by the caption.\n",
    "        # We need to encode the full sequence (prompt + caption) and then create labels\n",
    "        # by masking out the prompt tokens.\n",
    "\n",
    "        # Construct the full text sequence for the VLM\n",
    "        # Add the explicit <image> token at the beginning\n",
    "        full_text_sequence = \"<image>\" + self.vqa_prompt + chosen_caption\n",
    "\n",
    "        try:\n",
    "            # Process Image B and the full text sequence\n",
    "            # The processor will add the <image> token before the text\n",
    "            vlm_inputs = self.processor(\n",
    "                text=full_text_sequence,\n",
    "                images=imgB_pil, # Use image B for the VLM input\n",
    "                return_tensors=\"pt\",\n",
    "                # padding='max_length', # Padding will be handled by collate_fn\n",
    "                # max_length=self.max_length # Max length will be handled by collate_fn\n",
    "            )\n",
    "\n",
    "            # Extract input_ids, attention_mask, and pixel_values\n",
    "            input_ids = vlm_inputs['input_ids'].squeeze(0) # Remove batch dim\n",
    "            attention_mask = vlm_inputs['attention_mask'].squeeze(0) # Remove batch dim\n",
    "            pixel_values_b = vlm_inputs['pixel_values'].squeeze(0) # Remove batch dim\n",
    "\n",
    "            # Create labels by shifting input_ids and masking out prompt tokens\n",
    "            # The VLM calculates loss on the tokens *after* the prompt and image tokens.\n",
    "            # We need to find where the caption starts in the input_ids.\n",
    "            # A simple way is to find the first token of the caption within the input_ids\n",
    "            # after the prompt tokens. However, this can be fragile.\n",
    "            # A more robust way is to encode the prompt and full sequence separately\n",
    "            # to find the split point.\n",
    "\n",
    "            # Encode prompt separately to find its length - ENSURE images are passed here too\n",
    "            # Add the explicit <image> token here as well for consistency\n",
    "            prompt_inputs = self.processor(\n",
    "                text=\"<image>\" + self.vqa_prompt,\n",
    "                images=imgB_pil, # Pass images here as well\n",
    "                return_tensors=\"pt\",\n",
    "                add_special_tokens=False # Don't add BOS/EOS to prompt-only input\n",
    "            )\n",
    "            # Note: PaliGemmaProcessor adds one image token ID at the beginning when image is provided.\n",
    "            # The `input_ids` from `vlm_inputs` will be [<image_token_id>, prompt_token_ids, caption_token_ids]\n",
    "            # The `input_ids` from `prompt_inputs` will be [<image_token_id>, prompt_token_ids]\n",
    "            # We can use the length of `prompt_inputs['input_ids']` to find where the caption starts.\n",
    "            caption_start_index = prompt_inputs['input_ids'].shape[1]\n",
    "\n",
    "\n",
    "            # Create labels: copy input_ids and mask out everything before caption_start_index\n",
    "            labels = input_ids.clone()\n",
    "            # Get the ignore index from the model config\n",
    "            ignore_index = self.processor.tokenizer.pad_token_id # Use pad token ID for masking, or model's ignore_index\n",
    "            # PaliGemma config uses -100 by default for ignore_index in loss calculation\n",
    "            # Let's use -100 as the ignore index for labels\n",
    "            ignore_index_for_loss = -100 # Standard ignore index for CrossEntropyLoss\n",
    "\n",
    "            # Mask out tokens before the caption starts\n",
    "            labels[:caption_start_index] = ignore_index_for_loss\n",
    "\n",
    "            # Note: Padding will be applied in the collate_fn, which will add pad_token_id.\n",
    "            # The VLM's loss function is configured to ignore tokens with ignore_index.\n",
    "            # We need to make sure the pad_token_id is also ignored by the loss.\n",
    "            # PaliGemma's default ignore_index is -100, and its pad_token_id is 0.\n",
    "            # The loss function should ignore both. The model's forward pass handles this\n",
    "            # if its `ignore_index` is set correctly (default -100).\n",
    "            # We should ensure our padding value (0) is mapped to -100 in labels if needed,\n",
    "            # or rely on the model's internal ignore_index handling.\n",
    "            # Let's stick to setting labels before caption to -100. The collate_fn's padding\n",
    "            # will add 0s, which the model's loss should ignore if its ignore_index is -100.\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data with VLM processor for {img_base_name}: {e}\")\n",
    "            # Return None or raise error? Let's raise for now to debug data issues.\n",
    "            raise\n",
    "\n",
    "\n",
    "        # Need to process imgA through the processor as well to get pixel_values_a\n",
    "        try:\n",
    "            inputs_a = self.processor(\n",
    "                images=imgA_pil,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            pixel_values_a = inputs_a['pixel_values'].squeeze(0) # Remove batch dim\n",
    "        except Exception as e:\n",
    "             print(f\"Error processing image A with VLM processor for {img_base_name}: {e}\")\n",
    "             # Handle error - e.g., return dummy or raise\n",
    "             raise\n",
    "\n",
    "\n",
    "        batch = {\n",
    "            'pixel_values_a': pixel_values_a, # Use the processed pixel_values_a\n",
    "            'pixel_values_b': pixel_values_b, # Use the pixel_values from the processed VLM input\n",
    "            'input_ids': input_ids, # Use the input_ids from the processed VLM input (prompt + caption)\n",
    "            'attention_mask': attention_mask, # Use the attention_mask from the processed VLM input\n",
    "            'labels': labels, # Use the correctly masked labels\n",
    "            'name': datafiles['name'],\n",
    "        }\n",
    "\n",
    "        if self.load_segmentation and seg_mask_class_ids is not None:\n",
    "             batch['seg_mask'] = seg_mask_class_ids\n",
    "\n",
    "        # Store all reference captions (raw text) for BLEU calculation during validation\n",
    "        batch['reference_captions'] = self.captions_data.get(img_base_name, [])\n",
    "\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "# --- Segmentation Head Module ---\n",
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, target_size, patch_size):\n",
    "        super().__init__()\n",
    "        self.target_size = target_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Simple convolutional layers for segmentation\n",
    "        # Takes patch embeddings, reshapes, and upsamples\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, num_classes, kernel_size=1) # Output channels = num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, vision_outputs):\n",
    "        # vision_outputs is expected to be a simple object with .last_hidden_state\n",
    "        # Shape of last_hidden_state: (B, num_patches, in_channels)\n",
    "        patch_embeddings = vision_outputs.last_hidden_state\n",
    "        batch_size, num_patches, in_channels = patch_embeddings.shape\n",
    "\n",
    "        # Calculate grid size (assuming square patches and square image for simplicity)\n",
    "        # This might need adjustment for non-square images/patches or different VLM outputs\n",
    "        grid_size = int(math.sqrt(num_patches))\n",
    "        if grid_size * grid_size != num_patches:\n",
    "             # Handle cases where num_patches isn't a perfect square (e.g., class token)\n",
    "             # For now, let's assume the first token is class token and remove it\n",
    "             if num_patches == grid_size * grid_size + 1:\n",
    "                  patch_embeddings = patch_embeddings[:, 1:, :] # Remove class token\n",
    "                  num_patches = patch_embeddings.shape[1]\n",
    "                  grid_size = int(math.sqrt(num_patches))\n",
    "             else:\n",
    "                  raise ValueError(f\"Number of patches ({num_patches}) is not a perfect square or square + 1.\")\n",
    "\n",
    "\n",
    "        # Reshape patch embeddings to a grid (B, in_channels, grid_size, grid_size)\n",
    "        # Permute dimensions: (B, num_patches, in_channels) -> (B, in_channels, num_patches)\n",
    "        # Reshape last dim: (B, in_channels, grid_size, grid_size)\n",
    "        reshaped_features = patch_embeddings.permute(0, 2, 1).reshape(batch_size, in_channels, grid_size, grid_size)\n",
    "\n",
    "        # Pass through convolutional layers\n",
    "        conv_output = self.conv_layers(reshaped_features)\n",
    "\n",
    "        # Upsample to target size\n",
    "        # Use bilinear interpolation for upsampling logits\n",
    "        seg_logits = F.interpolate(conv_output, size=self.target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return seg_logits\n",
    "\n",
    "\n",
    "# Helper class to pass combined features to segmentation head\n",
    "class CombinedVisionOutputs:\n",
    "    def __init__(self, last_hidden_state):\n",
    "        self.last_hidden_state = last_hidden_state\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- ChangeDetectionVLM Model ---\n",
    "# This class wraps the PaliGemma model and adds a segmentation head.\n",
    "# =============================================================================\n",
    "class ChangeDetectionVLM(nn.Module):\n",
    "    def __init__(self, args, model_name_or_path, processor, num_classes, image_size, patch_size, freeze_vlm_base=False, quantization_config=None):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.processor = processor\n",
    "        self.num_classes = num_classes\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # --- Load VLM ---\n",
    "        # Use from_pretrained with quantization_config\n",
    "        self.vlm = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            quantization_config=quantization_config,\n",
    "            # Removed gradient_checkpointing from here\n",
    "        )\n",
    "\n",
    "        # Enable gradient checkpointing after loading if the argument is set\n",
    "        if args.gradient_checkpointing:\n",
    "            self.vlm.gradient_checkpointing_enable()\n",
    "            print(\"Gradient checkpointing enabled.\")\n",
    "\n",
    "\n",
    "        # --- Freeze VLM Base (Optional) ---\n",
    "        if freeze_vlm_base:\n",
    "            self.vlm.vision_tower.requires_grad_(False)\n",
    "            self.vlm.multi_modal_projector.requires_grad_(False)\n",
    "            # Freeze base LLM layers except for the language modeling head\n",
    "            for name, param in self.vlm.language_model.named_parameters():\n",
    "                 if 'lm_head' not in name:\n",
    "                      param.requires_grad_(False)\n",
    "            print(\"Frozen VLM vision tower, projector, and base language model layers.\")\n",
    "\n",
    "\n",
    "        # --- Segmentation Head ---\n",
    "        # Get the output dimension of the VLM's vision tower\n",
    "        vision_hidden_size = self.vlm.config.vision_config.hidden_size\n",
    "\n",
    "        # The segmentation head takes concatenated features from two images,\n",
    "        # so the input channels should be twice the vision hidden size.\n",
    "        segmentation_input_channels = 2 * vision_hidden_size\n",
    "\n",
    "        # Calculate target size for segmentation head output (should match original image size)\n",
    "        segmentation_target_size = (args.image_size, args.image_size) # Assuming square images\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=segmentation_input_channels, # Corrected input channels\n",
    "            num_classes=num_classes,\n",
    "            target_size=segmentation_target_size,\n",
    "            patch_size=patch_size # Pass patch size to segmentation head if needed\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, pixel_values_a, pixel_values_b, input_ids, attention_mask, labels=None, mode='train'):\n",
    "        \"\"\"\n",
    "        Forward pass for combined VQA and Segmentation.\n",
    "\n",
    "        Args:\n",
    "            pixel_values_a (torch.Tensor): Pixel values for image A (B, C, H, W).\n",
    "            pixel_values_b (torch.Tensor): Pixel values for image B (B, C, H, W).\n",
    "            input_ids (torch.Tensor): Tokenized input text (prompt + caption) (B, L).\n",
    "            attention_mask (torch.Tensor): Attention mask for input_ids (B, L).\n",
    "            labels (torch.Tensor, optional): Tokenized target labels (masked) (B, L).\n",
    "                                              Required for caption loss calculation.\n",
    "            mode (str): 'train' or 'eval'. Affects VLM loss calculation.\n",
    "\n",
    "        Returns:\n",
    "            dict: Contains 'caption_loss' (if labels provided) and 'seg_logits'.\n",
    "        \"\"\"\n",
    "        # --- VLM Forward Pass (for captioning loss and vision features) ---\n",
    "        # The VLM forward pass handles the image and text inputs.\n",
    "        # We need to pass image B and the combined input_ids/attention_mask.\n",
    "        # Labels are passed for loss calculation during training.\n",
    "\n",
    "        # Concatenate pixel values of A and B along the batch dimension\n",
    "        # The VLM expects images to be part of the batch.\n",
    "        # We'll process them together to get vision features.\n",
    "        # Note: PaliGemma is designed for single image input per text sequence.\n",
    "        # For change detection, we're using image B with text, and image A for segmentation features.\n",
    "        # Let's process image B with the text through the main VLM forward pass for captioning.\n",
    "        # We'll get vision features for image A separately using the vision tower.\n",
    "\n",
    "        # VLM forward pass for captioning (using image B and text)\n",
    "        # Pass labels if in train mode for loss calculation\n",
    "        vlm_outputs = self.vlm(\n",
    "            pixel_values=pixel_values_b,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels if mode == 'train' else None, # Pass labels only in train mode\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True # Need hidden states for segmentation head\n",
    "        )\n",
    "\n",
    "        caption_loss = vlm_outputs.loss if mode == 'train' else None\n",
    "\n",
    "        # --- Segmentation Branch ---\n",
    "        # We need vision features from *both* image A and image B for segmentation.\n",
    "        # PaliGemma's vision tower processes images independently.\n",
    "        # We can pass image A through the vision tower to get its features.\n",
    "\n",
    "        # Get vision features for image A\n",
    "        # Access the vision tower directly (path might vary slightly by model)\n",
    "        # For PaliGemma, the vision tower is self.vlm.vision_tower\n",
    "        # The vision tower takes pixel values and returns outputs similar to a vision transformer\n",
    "        # We need the patch embeddings, which are usually in the last_hidden_state of the vision tower output.\n",
    "        vision_outputs_a = self.vlm.vision_tower(pixel_values_a)\n",
    "\n",
    "        # Get vision features for image B from the main VLM output\n",
    "        # The VLM output includes vision hidden states before the multimodal projector\n",
    "        # Accessing these might require inspecting the model's output structure.\n",
    "        # A common pattern is that the vision tower output is part of the VLM's hidden states.\n",
    "        # Let's assume the vision hidden states for image B are available in vlm_outputs\n",
    "        # This might need adjustment based on actual PaliGemmaForConditionalGeneration output structure.\n",
    "        # A safer approach might be to pass image B through the vision tower separately as well.\n",
    "        # Let's pass image B through the vision tower separately for consistency.\n",
    "        vision_outputs_b = self.vlm.vision_tower(pixel_values_b)\n",
    "\n",
    "\n",
    "        # Combine vision features from A and B for segmentation\n",
    "        # Simple concatenation for now. More sophisticated fusion might be needed.\n",
    "        # Shape of vision_outputs_a.last_hidden_state: (B, num_patches, hidden_size)\n",
    "        # Shape of vision_outputs_b.last_hidden_state: (B, num_patches, hidden_size)\n",
    "        # Concatenate along the last dimension (feature dimension)\n",
    "        combined_vision_features = torch.cat(\n",
    "            [vision_outputs_a.last_hidden_state, vision_outputs_b.last_hidden_state],\n",
    "            dim=-1 # Concatenate features\n",
    "        ) # Shape (B, num_patches, 2 * hidden_size)\n",
    "\n",
    "        # Pass combined features to the segmentation head\n",
    "        # The segmentation head expects a simple object with .last_hidden_state\n",
    "        seg_outputs = self.segmentation_head(CombinedVisionOutputs(combined_vision_features)) # Shape (B, num_classes, H, W)\n",
    "        seg_logits = seg_outputs\n",
    "\n",
    "\n",
    "        # --- Return Outputs ---\n",
    "        outputs = {\n",
    "            'caption_loss': caption_loss,\n",
    "            'seg_logits': seg_logits,\n",
    "        }\n",
    "\n",
    "        # Include VLM outputs if needed for debugging or further analysis\n",
    "        # outputs['vlm_outputs'] = vlm_outputs\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def generate(self, pixel_values, input_ids, attention_mask, **kwargs):\n",
    "         \"\"\"\n",
    "         Generates captions using the VLM.\n",
    "\n",
    "         Args:\n",
    "             pixel_values (torch.Tensor): Pixel values for the image (B, C, H, W).\n",
    "             input_ids (torch.Tensor): Tokenized input text (prompt) (B, L).\n",
    "             attention_mask (torch.Tensor): Attention mask for input_ids (B, L).\n",
    "             **kwargs: Additional generation arguments (e.g., max_length, num_beams).\n",
    "\n",
    "         Returns:\n",
    "             torch.Tensor: Generated token IDs (B, generated_seq_len).\n",
    "         \"\"\"\n",
    "         # The VLM's generate method handles the generation process.\n",
    "         # We need to pass the image pixel values and the prompt input_ids/attention_mask.\n",
    "         # The VLM will generate tokens following the prompt based on the image content.\n",
    "\n",
    "         # Ensure the model is in evaluation mode for generation\n",
    "         self.eval()\n",
    "\n",
    "         # Use the VLM's built-in generate method\n",
    "         generated_ids = self.vlm.generate(\n",
    "             pixel_values=pixel_values,\n",
    "             input_ids=input_ids,\n",
    "             attention_mask=attention_mask,\n",
    "             **kwargs # Pass additional generation arguments\n",
    "         )\n",
    "\n",
    "         return generated_ids\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- preprocessing.py content (minimal changes needed) ---\n",
    "# =============================================================================\n",
    "# The preprocessing step is mainly needed to create the train/val/test splits (txt files)\n",
    "# and potentially organize data. Vocabulary generation is less critical now\n",
    "# as the VLM has its own tokenizer via the processor.\n",
    "\n",
    "# --- Special Tokens (Less relevant for VLM tokenizer) ---\n",
    "# SPECIAL_TOKENS = { ... } # Keep if needed for compatibility, but VLM handles its own\n",
    "\n",
    "# --- Helper Functions (build_vocab, encode, pad_sequence) ---\n",
    "# These might not be directly used by the VLM dataset, but keep if other parts rely on them.\n",
    "# Ensure they don't conflict with VLM processor logic.\n",
    "\n",
    "def build_vocab(sequences, min_token_count=1):\n",
    "    \"\"\"Builds vocabulary from pre-tokenized sequences (less relevant for VLM).\"\"\"\n",
    "    # ... (keep original implementation if needed elsewhere) ...\n",
    "    print(\"Note: build_vocab is less relevant when using a VLM's pre-trained tokenizer.\")\n",
    "    return {'<VLM_TOKENIZER>': 0} # Return dummy vocab\n",
    "\n",
    "def encode(seq_tokens, token_to_idx, allow_unk=False):\n",
    "    \"\"\"Encodes tokens (less relevant for VLM).\"\"\"\n",
    "    # ... (keep original implementation if needed elsewhere) ...\n",
    "    print(\"Note: encode function is less relevant when using a VLM's pre-trained tokenizer.\")\n",
    "    return [0] * len(seq_tokens) # Return dummy encoding\n",
    "\n",
    "def pad_sequence(sequence, max_length, pad_value=0):\n",
    "    print(\"Note: pad_sequence function may be less relevant if VLM processor/collation handles padding.\")\n",
    "\n",
    "    return sequence[:max_length] + [pad_value] * max(0, max_length - len(sequence))\n",
    "\n",
    "\n",
    "# --- Main Preprocessing Logic (Focus on creating split files) ---\n",
    "def run_preprocessing(args):\n",
    "    if args.dataset != 'LEVIR_MCI':\n",
    "        raise ValueError(f\"Dataset '{args.dataset}' not supported by this script.\")\n",
    "\n",
    "    input_captions_json = os.path.join(DATASET_ROOT, 'LevirCCcaptions.json')\n",
    "    input_image_dir = os.path.join(DATASET_ROOT, 'images')\n",
    "    # Use the hardcoded save directory for output splits\n",
    "    output_splits_dir = SAVE_OUTPUT_DIR\n",
    "\n",
    "    print(f\"Using LEVIR_MCI dataset from: {DATASET_ROOT}\")\n",
    "    print(f\"Saving split files to: {output_splits_dir}\")\n",
    "\n",
    "    os.makedirs(output_splits_dir, exist_ok=True)\n",
    "    # No need to create token_save_dir if not saving intermediate tokens\n",
    "\n",
    "    print('Loading captions from JSON to determine splits...')\n",
    "    try:\n",
    "        with open(input_captions_json, 'r') as f:\n",
    "            data = json.load(f)['images']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Caption file not found at {input_captions_json}\")\n",
    "        sys.exit(1)\n",
    "    except KeyError:\n",
    "        print(f\"Error: JSON file {input_captions_json} does not have the expected 'images' key.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    split_filenames = {'train': [], 'val': [], 'test': []}\n",
    "    image_check_counts = {'train': 0, 'val': 0, 'test': 0}\n",
    "    missing_image_counts = {'train': 0, 'val': 0, 'test': 0}\n",
    "\n",
    "    for img_info in data:\n",
    "        filename = img_info['filename']\n",
    "        filepath = img_info['filepath'] # train, val, or test\n",
    "        imgid = img_info['imgid']\n",
    "\n",
    "        if filepath not in split_filenames:\n",
    "            print(f\"Warning: Unknown filepath '{filepath}' found for {filename}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        image_check_counts[filepath] += 1\n",
    "        # Check if corresponding image files exist (essential check)\n",
    "        img_a_path = os.path.join(input_image_dir, filepath, 'A', filename)\n",
    "        img_b_path = os.path.join(input_image_dir, filepath, 'B', filename)\n",
    "        # Also check label exists if segmentation is required downstream\n",
    "        label_path = os.path.join(input_image_dir, filepath, 'label', filename) # Check 'label' folder\n",
    "\n",
    "        # Ensure segmentation mask exists as it's crucial for one task\n",
    "        if not os.path.exists(img_a_path) or not os.path.exists(img_b_path) or not os.path.exists(label_path):\n",
    "             # print(f\"Warning: Image pair or label not found for {filename} in {filepath}. Skipping this entry for split files.\")\n",
    "             missing_image_counts[filepath] += 1\n",
    "             continue\n",
    "\n",
    "        # Store base name without extension in split file list\n",
    "        split_filenames[filepath].append(os.path.splitext(filename)[0])\n",
    "\n",
    "    print(\"Image existence check complete.\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(f\"Split '{split}': Found {len(split_filenames[split])} valid entries out of {image_check_counts[split]} checked ({missing_image_counts[split]} missing files).\\n\")\n",
    "\n",
    "    # Save train/val/test split BASE filenames (without extension)\n",
    "    for split, filenames_base in split_filenames.items():\n",
    "        if not filenames_base:\n",
    "             print(f\"Warning: No valid entries found for split '{split}'. Split file will be empty.\")\n",
    "\n",
    "        split_file_path = os.path.join(output_splits_dir, f'{split}.txt')\n",
    "        print(f\"Saving {split} image list to {split_file_path} ({len(filenames_base)} images)\")\n",
    "        with open(split_file_path, 'w') as f:\n",
    "            for fname_base in filenames_base:\n",
    "                 f.write(f\"{fname_base}\\n\")\n",
    "\n",
    "    # --- Skip Vocab Generation and Token Saving ---\n",
    "    print(\"Skipping vocabulary building and individual token saving (using VLM processor instead).\\n\")\n",
    "    # Create a dummy vocab file just to satisfy potential checks in Trainer init (if not removed)\n",
    "    dummy_vocab_path = os.path.join(output_splits_dir, 'vocab.json')\n",
    "    with open(dummy_vocab_path, 'w') as f:\n",
    "         json.dump({'<VLM_TOKENIZER>': 0}, f)\n",
    "    print(f\"Created dummy vocab file at {dummy_vocab_path}\\n\")\n",
    "\n",
    "\n",
    "    print('Preprocessing (split file generation) finished.\\n')\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- training.py content adapted for VLM and Accelerate ---\n",
    "# =============================================================================\n",
    "\n",
    "# Helper function for mIoU (remains the same)\n",
    "def calculate_iou(pred, target, num_classes):\n",
    "    \"\"\"Calculates Intersection over Union (IoU) per class.\"\"\"\n",
    "    ious = []\n",
    "    # Ensure pred is class indices\n",
    "    if pred.shape[1] == num_classes: # If logits (B, C, H, W)\n",
    "        pred = torch.argmax(pred, dim=1) # Convert logits to class IDs (B, H, W)\n",
    "    elif pred.ndim != 3: # Should be (B, H, W)\n",
    "         raise ValueError(f\"Unexpected prediction shape for IoU: {pred.shape}\")\n",
    "\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds[target_inds]).long().sum().item()\n",
    "        # Cast to float before calculating union to avoid overflow with large tensors\n",
    "        union = pred_inds.long().sum().float().item() + target_inds.long().sum().float().item() - intersection\n",
    "        if union == 0:\n",
    "            ious.append(0.0) # Or float('nan') ? Let's use 0 for simplicity\n",
    "        else:\n",
    "            ious.append(float(intersection) / float(max(union, 1e-6))) # Avoid division by zero\n",
    "    return np.array(ious)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.config = args # Use args directly as config\n",
    "\n",
    "        # ---- Accelerator Setup ----\n",
    "        # ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True) # May need if parts of VLM aren't used\n",
    "        self.accelerator = Accelerator(\n",
    "            gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "            mixed_precision=args.mixed_precision, # 'fp16', 'bf16', or 'no'\n",
    "            log_with=\"tensorboard\", # Or \"wandb\", \"none\"\n",
    "            project_dir=os.path.join(SAVE_OUTPUT_DIR, 'accelerate_logs')\n",
    "            # kwargs_handlers=[ddp_kwargs]\n",
    "        )\n",
    "        set_seed(args.seed)\n",
    "\n",
    "        # Log args\n",
    "        self.accelerator.print(f\"Accelerator config: {self.accelerator.state}\\n\")\n",
    "        self.accelerator.print(f\"Running with args: {args}\\n\")\n",
    "\n",
    "        # ---- Create Directories ----\n",
    "        self.run_save_dir = os.path.join(SAVE_OUTPUT_DIR, 'training_output')\n",
    "        if self.accelerator.is_main_process:\n",
    "            os.makedirs(self.run_save_dir, exist_ok=True)\n",
    "            self.accelerator.init_trackers(args.run_name or \"vlm_change_detection\") # Init TensorBoard/WandB\n",
    "        self.accelerator.wait_for_everyone() # Ensure dir exists before anyone tries to write\n",
    "        self.latest_checkpoint_path = os.path.join(self.run_save_dir, 'checkpoint_latest') # Accelerate saves folders\n",
    "\n",
    "        # ---- VLM Processor ----\n",
    "        self.accelerator.print(\"Loading VLM Processor...\\n\")\n",
    "        self.processor = PaliGemmaProcessor.from_pretrained(args.model_name_or_path)\n",
    "        # Set pad token if processor doesn't have one (shouldn't be needed for PaliGemma)\n",
    "        # if self.processor.tokenizer.pad_token is None:\n",
    "        #     self.processor.tokenizer.pad_token = self.processor.tokenizer.eos_token\n",
    "\n",
    "        # ---- Dataset & Dataloaders ----\n",
    "        self.accelerator.print(\"Loading Datasets...\\n\")\n",
    "        self.train_dataset = LEVIRCCDataset(\n",
    "            data_folder=DATASET_ROOT,\n",
    "            processed_data_dir=SAVE_OUTPUT_DIR, # Where train.txt etc. are\n",
    "            split='train',\n",
    "            processor=self.processor,\n",
    "            load_segmentation=True, # Ensure this matches your intended use\n",
    "            max_length=args.max_length,\n",
    "            vqa_prompt=args.vqa_prompt\n",
    "        )\n",
    "        self.val_dataset = LEVIRCCDataset(\n",
    "            data_folder=DATASET_ROOT,\n",
    "            processed_data_dir=SAVE_OUTPUT_DIR,\n",
    "            split='val',\n",
    "            processor=self.processor,\n",
    "            load_segmentation=True, # Ensure this matches your intended use\n",
    "            max_length=args.max_length,\n",
    "            vqa_prompt=args.vqa_prompt\n",
    "        )\n",
    "\n",
    "        if len(self.train_dataset) == 0 or len(self.val_dataset) == 0:\n",
    "             raise ValueError(\"One or both datasets are empty. Check paths and preprocessing.\")\n",
    "\n",
    "        # Custom collate function for padding labels\n",
    "        # Use the collate_fn defined outside __init__ to access self.processor\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset, batch_size=args.batch_size_per_gpu, shuffle=True,\n",
    "            num_workers=args.num_workers, pin_memory=False,\n",
    "            collate_fn=self.collate_fn # Use the custom collate function from the class\n",
    "        )\n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_dataset, batch_size=args.batch_size_per_gpu, shuffle=False,\n",
    "            num_workers=args.num_workers, pin_memory=False,\n",
    "            collate_fn=self.collate_fn # Use the custom collate function from the class\n",
    "        )\n",
    "\n",
    "        # ---- Model Initialization ----\n",
    "        self.accelerator.print(\"Initializing Model...\\n\")\n",
    "        # Quantization Config\n",
    "        quantization_config = None\n",
    "        if args.quantization == '4bit':\n",
    "             quantization_config = BitsAndBytesConfig(\n",
    "                 load_in_4bit=True,\n",
    "                 bnb_4bit_compute_dtype=torch.bfloat16 # Or torch.float16\n",
    "             )\n",
    "        elif args.quantization == '8bit':\n",
    "             quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "        # Instantiate the VLM-based model\n",
    "        self.model = ChangeDetectionVLM(\n",
    "             args,\n",
    "             args.model_name_or_path,\n",
    "             self.processor,\n",
    "             num_classes=NUM_CLASSES,\n",
    "             image_size=args.image_size, # Use VLM default (e.g., 224) or resize in dataset\n",
    "             patch_size=self.get_model_patch_size(args.model_name_or_path), # Get patch size from config\n",
    "             freeze_vlm_base=args.freeze_encoder, # Use freeze_encoder flag\n",
    "             quantization_config=quantization_config\n",
    "         )\n",
    "\n",
    "        # ---- Optimizer ----\n",
    "        # Filter parameters that require gradients\n",
    "        params_to_optimize = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        self.optimizer = optim.AdamW(params_to_optimize, lr=args.learning_rate) # Use AdamW\n",
    "\n",
    "        # ---- Loss Functions ----\n",
    "        # Captioning loss is handled internally by the VLM when labels are provided\n",
    "        # Segmentation Loss: Standard CE\n",
    "        self.segmentation_criterion = nn.CrossEntropyLoss().to(self.accelerator.device)\n",
    "\n",
    "        # ---- Prepare with Accelerator ----\n",
    "        self.model, self.optimizer, self.train_loader, self.val_loader = self.accelerator.prepare(\n",
    "            self.model, self.optimizer, self.train_loader, self.val_loader\n",
    "        )\n",
    "\n",
    "        # ---- BLEU Score Smoothing ----\n",
    "        self.smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "        # ---- Track Best Metrics & State ----\n",
    "        self.best_bleu = 0.0\n",
    "        self.best_miou = 0.0\n",
    "        self.start_epoch = 0\n",
    "        self.global_step = 0\n",
    "\n",
    "        # ---- Resume from Checkpoint (Using Accelerate) ----\n",
    "        if os.path.exists(self.latest_checkpoint_path):\n",
    "            self.accelerator.print(f\"Resuming from checkpoint: {self.latest_checkpoint_path}\\n\")\n",
    "            try:\n",
    "                self.accelerator.load_state(self.latest_checkpoint_path)\n",
    "                # Extract state if needed (Accelerate loads model/opt automatically)\n",
    "                # Checkpoint structure might vary, need to inspect or save explicitly\n",
    "                # Example: Load custom state saved with accelerator.save()\n",
    "                custom_state_path = os.path.join(self.latest_checkpoint_path, \"custom_state.pth\")\n",
    "                if os.path.exists(custom_state_path):\n",
    "                     custom_state = torch.load(custom_state_path, map_location='cpu') # Load state to CPU first\n",
    "                     self.start_epoch = custom_state.get('epoch', 0)\n",
    "                     self.global_step = custom_state.get('global_step', 0)\n",
    "                     self.best_bleu = custom_state.get('best_bleu', 0.0)\n",
    "                     self.best_miou = custom_state.get('best_miou', 0.0)\n",
    "                     self.accelerator.print(f\"Resumed custom state: Epoch {self.start_epoch}, Step {self.global_step}, Best BLEU {self.best_bleu:.4f}, Best mIoU {self.best_miou:.4f}\\n\")\n",
    "                else:\n",
    "                     self.accelerator.print(\"Warning: Custom state file not found in checkpoint. Starting from epoch 0.\\n\")\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                self.accelerator.print(f\"Error loading checkpoint with Accelerator: {e}\\n\")\n",
    "                self.accelerator.print(\"Starting training from scratch.\\n\")\n",
    "                self.start_epoch = 0\n",
    "                self.global_step = 0\n",
    "                self.best_bleu = 0.0\n",
    "                self.best_miou = 0.0\n",
    "        else:\n",
    "            self.accelerator.print(\"No checkpoint found. Starting training from scratch.\\n\")\n",
    "\n",
    "    def get_model_patch_size(self, model_name_or_path):\n",
    "        \"\"\" Helper to get patch size from model config. \"\"\"\n",
    "        try:\n",
    "            # Load only the config first\n",
    "            config = PaliGemmaForConditionalGeneration.from_pretrained(model_name_or_path).config\n",
    "            # Access patch size (path might vary depending on model)\n",
    "            # For SigLIP (used in PaliGemma):\n",
    "            patch_size = config.vision_config.patch_size\n",
    "            self.accelerator.print(f\"Detected patch size: {patch_size}\\n\")\n",
    "            return patch_size\n",
    "        except Exception as e:\n",
    "            self.accelerator.print(f\"Warning: Could not auto-detect patch size for {model_name_or_path}: {e}. Using default 14.\\n\")\n",
    "            return 14 # Default patch size\n",
    "\n",
    "    # Custom collate function for padding input_ids, attention_mask, and labels\n",
    "    def collate_fn(self, batch):\n",
    "        # Separate elements that need padding (input_ids, attention_mask, labels)\n",
    "        input_ids = [item['input_ids'] for item in batch]\n",
    "        attention_mask = [item['attention_mask'] for item in batch]\n",
    "        labels = [item['labels'] for item in batch] # These are already masked\n",
    "\n",
    "        # Pad input_ids, attention_mask, and labels simultaneously using the VLM tokenizer's pad method\n",
    "        # This ensures they all have the same length after padding.\n",
    "        # We need to handle labels carefully: pad with ignore_index or pad_token_id?\n",
    "        # The VLM processor's pad method pads with pad_token_id (usually 0).\n",
    "        # The VLM's loss function ignores ignore_index (-100).\n",
    "        # So, we should pad labels with -100 to ensure padded parts are ignored by loss.\n",
    "        # However, the processor's pad method doesn't directly support padding labels with a different value.\n",
    "        # A common approach is to pad input_ids/attention_mask normally, and then pad labels separately\n",
    "        # with the ignore_index.\n",
    "\n",
    "        # Pad input_ids and attention_mask using the processor's tokenizer\n",
    "        padded_inputs = self.processor.tokenizer.pad(\n",
    "            {'input_ids': input_ids, 'attention_mask': attention_mask},\n",
    "            padding='longest',\n",
    "            return_tensors='pt',\n",
    "            # pad_to_multiple_of=8 # Optional\n",
    "        )\n",
    "\n",
    "        padded_input_ids = padded_inputs['input_ids']\n",
    "        padded_attention_mask = padded_inputs['attention_mask']\n",
    "\n",
    "        # Pad labels manually with the ignore_index\n",
    "        max_len = padded_input_ids.shape[1] # Get the padded length\n",
    "        ignore_index_for_loss = -100 # Standard ignore index\n",
    "\n",
    "        padded_labels = []\n",
    "        for label_seq in labels:\n",
    "            # Calculate padding needed\n",
    "            padding_len = max_len - len(label_seq)\n",
    "            if padding_len > 0:\n",
    "                # Pad with ignore_index\n",
    "                padded_label_seq = torch.cat([label_seq, torch.full((padding_len,), ignore_index_for_loss, dtype=label_seq.dtype)])\n",
    "            else:\n",
    "                padded_label_seq = label_seq[:max_len] # Truncate if somehow longer\n",
    "\n",
    "            padded_labels.append(padded_label_seq)\n",
    "\n",
    "        padded_labels = torch.stack(padded_labels)\n",
    "\n",
    "\n",
    "        # Manually collect other items in the batch\n",
    "        collated_batch = {\n",
    "            'pixel_values_a': torch.stack([item['pixel_values_a'] for item in batch]),\n",
    "            'pixel_values_b': torch.stack([item['pixel_values_b'] for item in batch]),\n",
    "            'input_ids': padded_input_ids,\n",
    "            'attention_mask': padded_attention_mask,\n",
    "            'labels': padded_labels, # Use the correctly padded labels\n",
    "            'name': [item['name'] for item in batch], # Collect names as a list\n",
    "            'reference_captions': [item['reference_captions'] for item in batch], # Collect reference captions as a list of lists\n",
    "        }\n",
    "\n",
    "        # Include seg_mask if loading segmentation - Access from self.args\n",
    "        if self.args.load_segmentation:\n",
    "             # Assuming seg_masks are already consistent size or can be stacked\n",
    "             # If not, they would also need padding/resizing here\n",
    "             collated_batch['seg_mask'] = torch.stack([item['seg_mask'] for item in batch])\n",
    "\n",
    "\n",
    "        return collated_batch\n",
    "\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_combined_loss = 0\n",
    "        total_cap_loss = 0\n",
    "        total_seg_loss = 0\n",
    "        num_batches = len(self.train_loader)\n",
    "\n",
    "        progress_bar = tqdm(total=num_batches, desc=f\"Epoch {epoch+1}/{self.args.epochs} [Train]\", disable=not self.accelerator.is_main_process)\n",
    "\n",
    "        for step, batch in enumerate(self.train_loader):\n",
    "            with self.accelerator.accumulate(self.model): # Handles gradient accumulation\n",
    "                # Forward pass (model takes care of internal logic)\n",
    "                # Ensure all required inputs from the batch are passed\n",
    "                outputs = self.model(\n",
    "                    pixel_values_a=batch['pixel_values_a'],\n",
    "                    pixel_values_b=batch['pixel_values_b'],\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels'], # Pass labels for caption loss calculation\n",
    "                    mode='train'\n",
    "                )\n",
    "\n",
    "                # ---- Loss Calculation ----\n",
    "                cap_loss = outputs['caption_loss'] # Loss directly from VLM output\n",
    "                seg_logits = outputs['seg_logits']\n",
    "                seg_mask_gt = batch['seg_mask'] # Ground truth seg masks (B, H, W)\n",
    "\n",
    "                # Resize seg_mask_gt if it doesn't match seg_logits size (e.g., due to VLM input size != dataset image size)\n",
    "                if seg_logits.shape[-2:] != seg_mask_gt.shape[-2:]:\n",
    "                     # Ensure seg_mask_gt is float for interpolation if needed, then back to long\n",
    "                     seg_mask_gt_resized = F.interpolate(seg_mask_gt.unsqueeze(1).float(), size=seg_logits.shape[-2:], mode='nearest').squeeze(1).long()\n",
    "                else:\n",
    "                     seg_mask_gt_resized = seg_mask_gt\n",
    "\n",
    "                # Segmentation Loss (ensure GT mask is on the same device)\n",
    "                seg_loss = self.segmentation_criterion(seg_logits, seg_mask_gt_resized)\n",
    "\n",
    "                # Combine Losses\n",
    "                combined_loss = cap_loss + self.args.seg_loss_weight * seg_loss\n",
    "\n",
    "                # --- Accumulate and Log Loss ---\n",
    "                # Average loss across devices and accumulation steps\n",
    "                avg_combined_loss = self.accelerator.gather(combined_loss).mean().item()\n",
    "                avg_cap_loss = self.accelerator.gather(cap_loss).mean().item()\n",
    "                avg_seg_loss = self.accelerator.gather(seg_loss).mean().item()\n",
    "\n",
    "                total_combined_loss += avg_combined_loss\n",
    "                total_cap_loss += avg_cap_loss\n",
    "                total_seg_loss += avg_seg_loss\n",
    "\n",
    "                # Logging (only on main process)\n",
    "                if self.accelerator.is_main_process:\n",
    "                    progress_bar.set_postfix({\n",
    "                        'CapL': f'{avg_cap_loss:.3f}',\n",
    "                        'SegL': f'{avg_seg_loss:.3f}',\n",
    "                        'CombL': f'{avg_combined_loss:.3f}'\n",
    "                    })\n",
    "                    # Log to TensorBoard/WandB\n",
    "                    self.accelerator.log({\n",
    "                        \"train/loss_combined\": avg_combined_loss,\n",
    "                        \"train/loss_caption\": avg_cap_loss,\n",
    "                        \"train/loss_segmentation\": avg_seg_loss,\n",
    "                    }, step=self.global_step)\n",
    "\n",
    "\n",
    "                # ---- Backward Pass & Optimization ----\n",
    "                self.accelerator.backward(combined_loss)\n",
    "\n",
    "                # Gradient Clipping (applied before optimizer step by accelerator if configured)\n",
    "                if self.accelerator.sync_gradients and self.args.grad_clip > 0:\n",
    "                    self.accelerator.clip_grad_norm_(self.model.parameters(), self.args.grad_clip)\n",
    "\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                self.global_step += 1\n",
    "                progress_bar.update(1)\n",
    "\n",
    "\n",
    "        progress_bar.close()\n",
    "        # --- Print Average Loss at End of Epoch ---\n",
    "        avg_epoch_combined_loss = total_combined_loss / num_batches\n",
    "        avg_epoch_cap_loss = total_cap_loss / num_batches\n",
    "        avg_epoch_seg_loss = total_seg_loss / num_batches # Corrected: Should be num_batches from the loader\n",
    "        self.accelerator.print(f\"Epoch {epoch+1} Train Avg Loss -> Combined: {avg_epoch_combined_loss:.4f} (Cap: {avg_epoch_cap_loss:.4f}, Seg: {avg_epoch_seg_loss:.4f})\\n\")\n",
    "        # Log epoch averages\n",
    "        if self.accelerator.is_main_process:\n",
    "             self.accelerator.log({\n",
    "                 \"train/epoch_loss_combined\": avg_epoch_combined_loss,\n",
    "                 \"train/epoch_loss_caption\": avg_epoch_cap_loss,\n",
    "                 \"train/epoch_loss_segmentation\": avg_epoch_seg_loss,\n",
    "             }, step=epoch+1) # Log against epoch number\n",
    "\n",
    "        return avg_epoch_combined_loss\n",
    "\n",
    "\n",
    "    def validate_epoch(self, epoch):\n",
    "        self.model.eval()\n",
    "        references_corpus = [] # List of lists of reference tokens (strings) for BLEU\n",
    "        hypotheses_corpus = [] # List of hypothesis tokens (strings) for BLEU\n",
    "        total_iou = np.zeros(NUM_CLASSES)\n",
    "        num_val_batches = 0\n",
    "\n",
    "        val_progress_bar = tqdm(total=len(self.val_loader), desc=f\"Epoch {epoch+1}/{self.args.epochs} [Validate]\", disable=not self.accelerator.is_main_process)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for step, batch in enumerate(self.val_loader):\n",
    "                # Forward pass for segmentation logits\n",
    "                outputs = self.model(\n",
    "                    pixel_values_a=batch['pixel_values_a'],\n",
    "                    pixel_values_b=batch['pixel_values_b'],\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    # No labels in eval mode\n",
    "                    mode='eval'\n",
    "                )\n",
    "                seg_logits = outputs['seg_logits']\n",
    "                seg_mask_gt = batch['seg_mask']\n",
    "\n",
    "                # --- Perform Generation for Captions ---\n",
    "                # Unwrap model for generation if needed, or use accelerator.unwrap_model\n",
    "                unwrapped_model = self.accelerator.unwrap_model(self.model)\n",
    "                # Prepare inputs for generation (only need prompt, images)\n",
    "                # Use generate method defined in ChangeDetectionVLM\n",
    "                # For generation, we only pass the prompt and image B\n",
    "                prompt_only_inputs = self.processor(\n",
    "                    text=self.args.vqa_prompt,\n",
    "                    images=batch['pixel_values_b'], # Use image B's pixel values\n",
    "                    return_tensors=\"pt\",\n",
    "                    # No padding needed for generation input\n",
    "                ).to(self.accelerator.device) # Move to device\n",
    "\n",
    "                generated_ids = unwrapped_model.generate(\n",
    "                     pixel_values=prompt_only_inputs['pixel_values'],\n",
    "                     input_ids=prompt_only_inputs['input_ids'],\n",
    "                     attention_mask=prompt_only_inputs['attention_mask'],\n",
    "                     # Add specific generation args if needed\n",
    "                     max_length=self.args.max_length, # Use max_length arg for generation\n",
    "                     num_beams=3, # Example beam search\n",
    "                     early_stopping=True,\n",
    "                 ) # Shape (B, generated_seq_len)\n",
    "\n",
    "                # --- Gather tensors across devices ---\n",
    "                # Gather segmentation results\n",
    "                all_seg_logits = self.accelerator.gather_for_metrics(seg_logits)\n",
    "                all_seg_mask_gt = self.accelerator.gather_for_metrics(seg_mask_gt)\n",
    "                # Gather generated captions and references\n",
    "                # Pad generated_ids before gathering to ensure consistent shape\n",
    "                all_generated_ids = self.accelerator.pad_across_processes(generated_ids, dim=1, pad_index=self.processor.tokenizer.pad_token_id)\n",
    "                all_generated_ids = self.accelerator.gather_for_metrics(all_generated_ids)\n",
    "                # References are lists of strings, gather them manually if needed (tricky with DDP)\n",
    "                # For simplicity, calculate BLEU only on the main process using its part of the data,\n",
    "                # OR gather raw reference strings (requires careful handling)\n",
    "                # Let's decode and collect on each process, then gather decoded strings (simpler)\n",
    "\n",
    "                # Decode generated captions\n",
    "                decoded_preds = self.processor.batch_decode(all_generated_ids, skip_special_tokens=True)\n",
    "\n",
    "                # Get reference captions from the batch (list of lists of strings)\n",
    "                # Need to handle gathering this across processes if calculating global BLEU\n",
    "                # For now, let's collect references corresponding to the predictions on this process\n",
    "                raw_references_batch = batch['reference_captions'] # This is on the current process's batch slice\n",
    "\n",
    "                # --- Calculate Final Metrics (on main process after gathering) ---\n",
    "                if self.accelerator.is_main_process:\n",
    "                    # mIoU Calculation\n",
    "                    # Resize GT mask if needed\n",
    "                    if all_seg_logits.shape[-2:] != all_seg_mask_gt.shape[-2:]:\n",
    "                         all_seg_mask_gt_resized = F.interpolate(all_seg_mask_gt.unsqueeze(1).float(), size=all_seg_logits.shape[-2:], mode='nearest').squeeze(1).long()\n",
    "                    else:\n",
    "                         all_seg_mask_gt_resized = all_seg_mask_gt\n",
    "\n",
    "                    batch_iou = calculate_iou(all_seg_logits.cpu(), all_seg_mask_gt_resized.cpu(), NUM_CLASSES)\n",
    "                    total_iou += batch_iou\n",
    "                    num_val_batches += 1 # Count batches on main process\n",
    "\n",
    "                    # Prepare for BLEU Score\n",
    "                    # References need to be list of lists of strings for each hypothesis\n",
    "                    # `raw_references_batch` needs to be structured correctly\n",
    "                    # Assume raw_references_batch is [ [ref1_img1, ref2_img1,...], [ref1_img2, ...], ...]\n",
    "                    # We need [[ref1_img1_tokens, ref2_img1_tokens,...], [ref1_img2_tokens,...], ...]\\\n",
    "\n",
    "                    batch_references_for_bleu = []\n",
    "                    batch_hypotheses_for_bleu = []\n",
    "\n",
    "                    # Iterate through predictions and corresponding references in the gathered batch\n",
    "                    for pred_text, list_of_ref_texts in zip(decoded_preds, raw_references_batch):\n",
    "                        # Tokenize hypothesis (simple split for BLEU)\n",
    "                        hyp_tokens = pred_text.split()\n",
    "                        # Tokenize all references for this item\n",
    "                        ref_tokens_list = [ref.split() for ref in list_of_ref_texts]\n",
    "\n",
    "                        if hyp_tokens and any(ref_tokens_list): # Ensure not empty\n",
    "                             batch_hypotheses_for_bleu.append(hyp_tokens)\n",
    "                             batch_references_for_bleu.append(ref_tokens_list)\n",
    "\n",
    "\n",
    "                    # Extend the main corpus lists\n",
    "                    hypotheses_corpus.extend(batch_hypotheses_for_bleu)\n",
    "                    references_corpus.extend(batch_references_for_bleu)\n",
    "\n",
    "                val_progress_bar.update(1)\n",
    "\n",
    "        val_progress_bar.close()\n",
    "\n",
    "        # ---- Calculate Final Metrics (on main process) ----\n",
    "        mean_iou = 0.0\n",
    "        bleu_score = 0.0\n",
    "        if self.accelerator.is_main_process:\n",
    "            # Final mIoU\n",
    "            mean_iou = np.mean(total_iou / num_val_batches) if num_val_batches > 0 else 0.0\n",
    "\n",
    "            # Final BLEU Score\n",
    "            if not references_corpus or not hypotheses_corpus:\n",
    "                 print(\"Warning: No valid references or hypotheses collected for BLEU score.\\n\")\n",
    "            elif len(references_corpus) != len(hypotheses_corpus):\n",
    "                  print(f\"Warning: Mismatch in reference ({len(references_corpus)}) and hypothesis ({len(hypotheses_corpus)}) counts for BLEU.\\n\")\n",
    "            else:\n",
    "                try:\n",
    "                     bleu_score = corpus_bleu(references_corpus, hypotheses_corpus, smoothing_function=self.smooth_fn)\n",
    "                except Exception as e:\n",
    "                     print(f\"Error calculating BLEU score: {e}\\n\")\n",
    "\n",
    "            self.accelerator.print(f\"Epoch {epoch+1} Validation -> BLEU-4: {bleu_score:.4f}, mIoU: {mean_iou:.4f}\\n\")\n",
    "            # Log validation metrics\n",
    "            self.accelerator.log({\n",
    "                \"eval/bleu4\": bleu_score,\n",
    "                \"eval/mIoU\": mean_iou,\n",
    "            }, step=epoch+1) # Log against epoch number\n",
    "\n",
    "        # Return metrics (gathered on main process)\n",
    "        return bleu_score, mean_iou\n",
    "\n",
    "\n",
    "    def run_training(self):\n",
    "        self.accelerator.print(\"Starting Training...\\n\")\n",
    "        self.accelerator.print(f\"Total training steps: {len(self.train_loader) // self.accelerator.gradient_accumulation_steps * self.args.epochs}\\n\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.args.epochs):\n",
    "            self.accelerator.print(f\"--- Starting Epoch {epoch+1}/{self.args.epochs} ---\\n\")\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            val_bleu, val_miou = self.validate_epoch(epoch)\n",
    "\n",
    "            # ---- Save Checkpoint (using Accelerator) ----\n",
    "            if self.accelerator.is_main_process:\n",
    "                is_best_bleu = val_bleu > self.best_bleu\n",
    "                if is_best_bleu:\n",
    "                    self.best_bleu = val_bleu\n",
    "                    print(f\"*** New best BLEU score: {self.best_bleu:.4f} ***\\n\")\n",
    "\n",
    "                is_best_miou = val_miou > self.best_miou\n",
    "                if is_best_miou:\n",
    "                     self.best_miou = val_miou\n",
    "                     print(f\"*** New best mIoU score: {self.best_miou:.4f} ***\\n\")\n",
    "\n",
    "                # Save latest checkpoint using Accelerator\n",
    "                self.accelerator.save_state(self.latest_checkpoint_path)\n",
    "\n",
    "                # Save custom state separately within the checkpoint folder\n",
    "                custom_state = {\n",
    "                     'epoch': epoch + 1,\n",
    "                     'global_step': self.global_step,\n",
    "                     'args': self.args,\n",
    "                     'best_bleu': self.best_bleu,\n",
    "                     'best_miou': self.best_miou,\n",
    "                 }\n",
    "                custom_state_path = os.path.join(self.latest_checkpoint_path, \"custom_state.pth\")\n",
    "                self.accelerator.save(custom_state, custom_state_path)\n",
    "                print(f\"Saved latest checkpoint state to {self.latest_checkpoint_path}\\n\")\n",
    "\n",
    "\n",
    "                # Save best BLEU checkpoint separately if desired\n",
    "                if is_best_bleu:\n",
    "                    best_bleu_path = os.path.join(self.run_save_dir, 'checkpoint_best_bleu')\n",
    "                    self.accelerator.save_state(best_bleu_path)\n",
    "                    # Also save custom state in best checkpoint dir\n",
    "                    best_custom_state_path = os.path.join(best_bleu_path, \"custom_state.pth\")\n",
    "                    self.accelerator.save(custom_state, best_custom_state_path)\n",
    "                    print(f\"Saved best BLEU checkpoint state to {best_bleu_path}\\n\")\n",
    "\n",
    "            # Wait for all processes to finish saving/loading before next epoch\n",
    "            self.accelerator.wait_for_everyone()\n",
    "\n",
    "\n",
    "        self.accelerator.print(\"Training finished.\\n\")\n",
    "        if self.accelerator.is_main_process:\n",
    "             self.accelerator.print(f\"Best Validation BLEU-4 achieved: {self.best_bleu:.4f}\\n\")\n",
    "             self.accelerator.print(f\"Best Validation mIoU achieved: {self.best_miou:.4f}\\n\")\n",
    "             self.accelerator.end_training() # Clean up trackers\n",
    "\n",
    "# =============================================================================\n",
    "# --- VQA Inference Function ---\n",
    "# =============================================================================\n",
    "@torch.no_grad()\n",
    "def answer_question(model, processor, img_path_a, img_path_b, question, device):\n",
    "    \"\"\"\n",
    "    Performs VQA inference on a pair of images.\n",
    "\n",
    "    Args:\n",
    "        model: The fine-tuned ChangeDetectionVLM model (unwrapped).\n",
    "        processor: The VLM processor.\n",
    "        img_path_a (str): Path to the 'before' image.\n",
    "        img_path_b (str): Path to the 'after' image.\n",
    "        question (str): The natural language question.\n",
    "        device: The torch device to run inference on.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer string.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    try:\n",
    "        imgA_pil = Image.open(img_path_a).convert(\"RGB\")\n",
    "        imgB_pil = Image.open(img_path_b).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        return f\"Error loading images: {e}\"\n",
    "\n",
    "    # Process image B and the question for VQA\n",
    "    # The processor will add the <image> token before the text\n",
    "    try:\n",
    "        # Add the explicit <image> token to the question for inference\n",
    "        vlm_inputs = processor(\n",
    "            text=\"<image>\" + question,\n",
    "            images=imgB_pil, # Use image B for VQA input\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device) # Move to device\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error processing inputs: {e}\"\n",
    "\n",
    "    # Generate answer using the model's generate method\n",
    "    try:\n",
    "        generated_ids = model.generate(\n",
    "            pixel_values=vlm_inputs['pixel_values'],\n",
    "            input_ids=vlm_inputs['input_ids'],\n",
    "            attention_mask=vlm_inputs['attention_mask'],\n",
    "            # Add generation kwargs if needed (e.g., max_length)\n",
    "            max_length=128 # Set a reasonable max length for answers\n",
    "        )\n",
    "        # Decode the generated tokens\n",
    "        answer = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f\"Error during generation: {e}\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- Main Execution Block ---\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set the environment variable for memory management early\n",
    "    import os\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Train or Evaluate VLM for Change Detection VQA')\n",
    "\n",
    "    # --- Paths and Basic Config ---\n",
    "    parser.add_argument('--dataset', type=str, default='LEVIR_MCI', help='Dataset name')\n",
    "    parser.add_argument('--run_name', type=str, default=None, help='Optional run name for logging')\n",
    "    parser.add_argument('--seed', type=int, default=42, help='Random seed')\n",
    "\n",
    "    # --- Model Config ---\n",
    "    parser.add_argument('--model_name_or_path', type=str, default=DEFAULT_VLM, help='Pre-trained VLM name/path')\n",
    "    parser.add_argument('--freeze_encoder', action='store_true', help='Freeze VLM vision tower and base LLM layers')\n",
    "    # Set quantization default to 4bit for better memory usage\n",
    "    parser.add_argument('--quantization', type=str, default='4bit', choices=['no', '4bit', '8bit'], help='Apply quantization (4bit or 8bit)')\n",
    "    # Set gradient checkpointing default to True for better memory usage\n",
    "    parser.add_argument('--gradient_checkpointing', action='store_true', default=True, help='Enable gradient checkpointing to save memory')\n",
    "\n",
    "\n",
    "    # --- Dataset and Preprocessing ---\n",
    "    parser.add_argument('--max_length', type=int, default=64, help='Maximum sequence length for VLM tokenizer')\n",
    "    parser.add_argument('--image_size', type=int, default=224, help='Image size (must match VLM input size)')\n",
    "    parser.add_argument('--vqa_prompt', type=str, default=\"Describe the changes between the two images.\", help='Prompt for captioning/VQA')\n",
    "    # Preprocessing args (less critical now but kept for consistency)\n",
    "    parser.add_argument('--word_count_threshold', default=5, type=int, help='Min word count (less relevant)')\n",
    "    # Add load_segmentation to args parser so it's available in self.args\n",
    "    parser.add_argument('--load_segmentation', action='store_true', default=True, help='Whether to load segmentation masks')\n",
    "\n",
    "\n",
    "    # --- Training Config (using Accelerate) ---\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of training epochs')\n",
    "    # Reduce default batch size and increase accumulation steps for better memory usage\n",
    "    parser.add_argument('--batch_size_per_gpu', type=int, default=1, help='Batch size per GPU (adjust based on memory)')\n",
    "    parser.add_argument('--num_workers', type=int, default=2, help='Number of dataloader workers')\n",
    "    parser.add_argument('--learning_rate', type=float, default=5e-5, help='Learning rate for AdamW optimizer')\n",
    "    parser.add_argument('--grad_clip', type=float, default=1.0, help='Max gradient norm for clipping (0 for no clipping)')\n",
    "    parser.add_argument('--seg_loss_weight', type=float, default=1.0, help='Weight for the segmentation loss term')\n",
    "    # Increase default gradient accumulation steps to compensate for smaller batch size\n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=8, help='Steps for gradient accumulation')\n",
    "    parser.add_argument('--mixed_precision', type=str, default='bf16', choices=['no', 'fp16', 'bf16'], help='Mixed precision type')\n",
    "\n",
    "    # --- Execution Mode ---\n",
    "    parser.add_argument('--run_mode', type=str, default='train', choices=['preprocess', 'train', 'evaluate', 'vqa'],\n",
    "                        help='Pipeline mode: preprocess, train, evaluate (on val set), or vqa (single example)')\n",
    "\n",
    "    # --- VQA Mode Specific Args ---\n",
    "    parser.add_argument('--vqa_img_a', type=str, default=None, help='Path to image A for VQA mode')\n",
    "    parser.add_argument('--vqa_img_b', type=str, default=None, help='Path to image B for VQA mode')\n",
    "    parser.add_argument('--vqa_question', type=str, default=\"What has changed?\", help='Question for VQA mode')\n",
    "    parser.add_argument('--vqa_checkpoint', type=str, default=None, help='Path to checkpoint folder for VQA/evaluate mode (uses latest if not specified)')\n",
    "\n",
    "\n",
    "    # In a notebook, parse default args or provide specific ones\n",
    "    # Example: args = parser.parse_args(['--run_mode', 'preprocess'])\n",
    "    # Example: args = parser.parse_args(['--run_mode', 'train', '--batch_size_per_gpu', '2', '--gradient_accumulation_steps', '4', '--quantization', '4bit', '--gradient_checkpointing'])\\n    # Example: args = parser.parse_args(['--run_mode', 'vqa', '--vqa_img_a', 'path/to/imgA.png', '--vqa_img_b', 'path/to/imgB.png', '--vqa_question', 'How many buildings were added?'])\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        # This block allows running in a notebook without passing command-line args\n",
    "        # Use a minimal set of default args for notebook execution\n",
    "        # You can override these in a cell before this block if needed\n",
    "        args = parser.parse_args([]) # Parse no arguments, use defaults\n",
    "    else:\n",
    "        # Parse command-line arguments if not in a notebook\n",
    "        args = parser.parse_args()\n",
    "\n",
    "\n",
    "    # --- Execute Based on Mode ---\n",
    "    trainer = None # Initialize trainer variable\n",
    "\n",
    "    if args.run_mode == 'preprocess':\n",
    "        print(\"--- Running Preprocessing ---\\n\")\n",
    "        run_preprocessing(args)\n",
    "        print(\"--- Preprocessing Finished ---\\n\")\n",
    "\n",
    "    elif args.run_mode == 'train':\n",
    "        print(\"--- Running Training ---\\n\")\n",
    "        # Ensure preprocessing ran (check for split files)\n",
    "        if not os.path.exists(os.path.join(SAVE_OUTPUT_DIR, 'train.txt')):\n",
    "             print(\"Split files not found. Running preprocessing first.\\n\")\n",
    "             run_preprocessing(args)\n",
    "             print(\"-\" * 20 + \"\\n\") # Separator\n",
    "\n",
    "        trainer = Trainer(args)\n",
    "        trainer.run_training()\n",
    "        print(\"--- Training Finished ---\\n\")\n",
    "\n",
    "    elif args.run_mode == 'evaluate':\n",
    "        print(\"--- Running Evaluation ---\\n\")\n",
    "        # Requires a trained model checkpoint\n",
    "        trainer = Trainer(args) # Initialize trainer to load model and data\n",
    "        checkpoint_to_load = args.vqa_checkpoint or trainer.latest_checkpoint_path\n",
    "        if not os.path.exists(checkpoint_to_load):\n",
    "             print(f\"Error: Checkpoint not found at {checkpoint_to_load}. Cannot evaluate.\\n\")\n",
    "        else:\n",
    "             print(f\"Loading state from: {checkpoint_to_load}\\n\")\n",
    "             trainer.accelerator.load_state(checkpoint_to_load)\n",
    "             print(\"Evaluating on validation set...\\n\")\n",
    "             # Use epoch=-1 or similar to indicate it's a final eval run\n",
    "             val_bleu, val_miou = trainer.validate_epoch(epoch=-1)\n",
    "             print(f\"--- Evaluation Finished ---\\n\")\n",
    "             print(f\"Validation BLEU-4: {val_bleu:.4f}\\n\")\n",
    "             print(f\"Validation mIoU: {val_miou:.4f}\\n\")\n",
    "\n",
    "    elif args.run_mode == 'vqa':\n",
    "        print(\"--- Running VQA Inference ---\\n\")\n",
    "        if not args.vqa_img_a or not args.vqa_img_b:\n",
    "             print(\"Error: Please provide paths to both image A and image B using --vqa_img_a and --vqa_img_b\\n\")\n",
    "        else:\n",
    "            # Initialize minimal components needed for inference\n",
    "            # Need args to initialize the model correctly (e.g., quantization)\n",
    "            # Create a dummy args object or parse minimal args\n",
    "            minimal_parser = argparse.ArgumentParser()\n",
    "            minimal_parser.add_argument('--model_name_or_path', type=str, default=DEFAULT_VLM)\n",
    "            minimal_parser.add_argument('--quantization', type=str, default='no', choices=['no', '4bit', '8bit'])\n",
    "            minimal_parser.add_argument('--freeze_encoder', action='store_true', default=False) # Inference doesn't freeze\n",
    "            minimal_parser.add_argument('--gradient_checkpointing', action='store_true', default=False) # Inference doesn't need checkpointing\n",
    "            minimal_parser.add_argument('--max_length', type=int, default=128) # Add max_length for generation\n",
    "            # Parse only known args to avoid errors with other args present in sys.argv in notebooks\n",
    "            minimal_args, _ = minimal_parser.parse_known_args(sys.argv[1:] if 'ipykernel' not in sys.modules else [])\n",
    "\n",
    "\n",
    "            accelerator = Accelerator(mixed_precision=minimal_args.mixed_precision) # Use mixed precision if available\n",
    "            processor = PaliGemmaProcessor.from_pretrained(minimal_args.model_name_or_path)\n",
    "\n",
    "            # Quantization Config\n",
    "            quantization_config = None\n",
    "            if minimal_args.quantization == '4bit':\n",
    "                 quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "            elif minimal_args.quantization == '8bit':\n",
    "                 quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "            # Load model (without full Trainer setup)\n",
    "            model = ChangeDetectionVLM(\n",
    "                 minimal_args, # Pass minimal args\n",
    "                 minimal_args.model_name_or_path,\n",
    "                 processor,\n",
    "                 quantization_config=quantization_config,\n",
    "                 freeze_vlm_base=False # No need to freeze for inference\n",
    "             )\n",
    "            model = accelerator.prepare(model) # Prepare model for device\n",
    "\n",
    "            # Load checkpoint\n",
    "            checkpoint_to_load = args.vqa_checkpoint or os.path.join(SAVE_OUTPUT_DIR, 'training_output', 'checkpoint_latest')\n",
    "            if not os.path.exists(checkpoint_to_load):\n",
    "                 print(f\"Warning: Checkpoint {checkpoint_to_load} not found. Using pre-trained weights.\\n\")\n",
    "            else:\n",
    "                 print(f\"Loading model state from: {checkpoint_to_load}\\n\")\n",
    "                 # Need to load state dict manually if not using accelerator.load_state\n",
    "                 try:\n",
    "                     # Assumes accelerate saved state_dict in 'pytorch_model.bin' or similar\n",
    "                     # This might need adjustment based on how accelerate saves\n",
    "                     state_dict_path = os.path.join(checkpoint_to_load, \"pytorch_model.bin\") # Common path\n",
    "                     if not os.path.exists(state_dict_path):\n",
    "                          # Try custom state path if main one doesn't exist (less likely for model state)\n",
    "                          # Fallback or specific filename needed here based on saving method.\n",
    "                          raise FileNotFoundError(\"Could not find model state dict file.\")\n",
    "\n",
    "                     # Load state dict, handling potential DDP prefix\n",
    "                     state_dict = torch.load(state_dict_path, map_location='cpu')\n",
    "                     unwrapped_model = accelerator.unwrap_model(model)\n",
    "                     # Remove 'module.' prefix if present\n",
    "                     new_state_dict = {}\n",
    "                     for k, v in state_dict.items():\n",
    "                         if k.startswith('module.'):\n",
    "                             new_state_dict[k[7:]] = v\n",
    "                         else:\n",
    "                             new_state_dict[k] = v\n",
    "                     unwrapped_model.load_state_dict(new_state_dict) # Load into unwrapped model\n",
    "                     print(\"Model state loaded successfully.\\n\")\n",
    "                 except Exception as e:\n",
    "                      print(f\"Error loading model state from checkpoint {checkpoint_to_load}: {e}. Using pre-trained weights.\\n\")\n",
    "\n",
    "\n",
    "            # Perform VQA\n",
    "            print(f\"\\nImage A: {args.vqa_img_a}\\n\")\n",
    "            print(f\"\\nImage B: {args.vqa_img_b}\\n\")\n",
    "            print(f\"\\nQuestion: {args.vqa_question}\\n\")\n",
    "\n",
    "            answer = answer_question(\n",
    "                accelerator.unwrap_model(model), # Pass unwrapped model\n",
    "                processor,\n",
    "                args.vqa_img_a,\n",
    "                args.vqa_img_b,\n",
    "                args.vqa_question,\n",
    "                accelerator.device\n",
    "            )\n",
    "\n",
    "            print(f\"\\nAnswer: {answer}\\n\")\n",
    "            print(\"--- VQA Finished ---\\n\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Unknown run_mode: {args.run_mode}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7240493,
     "sourceId": 11545714,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
